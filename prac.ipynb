{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a3c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://en.wikipedia.org/wiki/Web_crawler\n",
      "Crawling: https://en.wikipedia.org/wiki/Web_crawler#bodyContent\n",
      "Crawling: https://en.wikipedia.org/wiki/Main_Page\n",
      "Crawling: https://en.wikipedia.org/wiki/Wikipedia:Contents\n",
      "Crawling: https://en.wikipedia.org/wiki/Portal:Current_events\n",
      "Crawling: https://en.wikipedia.org/wiki/Special:Random\n",
      "Crawling: https://en.wikipedia.org/wiki/Wikipedia:About\n",
      "Crawling: https://en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "Crawling: https://en.wikipedia.org/wiki/Help:Contents\n",
      "Crawling: https://en.wikipedia.org/wiki/Help:Introduction\n",
      "Crawled 10 pages.\n",
      "\n",
      "Doc 0: https://en.wikipedia.org/wiki/Web_crawler\n",
      "\n",
      "Doc 1: https://en.wikipedia.org/wiki/Web_crawler#bodyContent\n",
      "\n",
      "Doc 2: https://en.wikipedia.org/wiki/Main_Page\n",
      "\n",
      "Doc 3: https://en.wikipedia.org/wiki/Wikipedia:Contents\n",
      "\n",
      "Doc 4: https://en.wikipedia.org/wiki/Portal:Current_events\n",
      "\n",
      "Doc 5: https://en.wikipedia.org/wiki/Special:Random\n",
      "\n",
      "Doc 6: https://en.wikipedia.org/wiki/Wikipedia:About\n",
      "\n",
      "Doc 7: https://en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "\n",
      "Doc 8: https://en.wikipedia.org/wiki/Help:Contents\n",
      "\n",
      "Doc 9: https://en.wikipedia.org/wiki/Help:Introduction\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "\n",
    "class SimpleCrawler:\n",
    "    def __init__(self, base_url, max_pages=50):\n",
    "        self.base_url = base_url\n",
    "        self.visited = set()\n",
    "        self.to_visit = [base_url]\n",
    "        self.max_pages = max_pages\n",
    "        self.documents = {}  # doc_id -> text\n",
    "\n",
    "    def crawl(self):\n",
    "        while self.to_visit and len(self.visited) < self.max_pages:\n",
    "            url = self.to_visit.pop(0)\n",
    "            if url in self.visited:\n",
    "                continue\n",
    "\n",
    "            print(f\"Crawling: {url}\")\n",
    "            try:\n",
    "                response = requests.get(url, timeout=5)\n",
    "                if response.status_code != 200:\n",
    "                    continue\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # Extract text (simple)\n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "                # Store doc\n",
    "                doc_id = len(self.visited)\n",
    "                self.documents[doc_id] = {'url': url, 'text': text}\n",
    "\n",
    "                self.visited.add(url)\n",
    "\n",
    "                # Find new links within the same domain\n",
    "                base_domain = urlparse(self.base_url).netloc\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    abs_link = urljoin(url, link['href'])\n",
    "                    if urlparse(abs_link).netloc == base_domain and abs_link not in self.visited:\n",
    "                        self.to_visit.append(abs_link)\n",
    "\n",
    "                time.sleep(1)  # polite crawling\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to crawl {url}: {e}\")\n",
    "\n",
    "        print(f\"Crawled {len(self.documents)} pages.\")\n",
    "        return self.documents\n",
    "import json\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = 'https://en.wikipedia.org/wiki/Web_crawler'\n",
    "    crawler = SimpleCrawler(base_url, max_pages=10)\n",
    "    docs = crawler.crawl()\n",
    "    filename = \"documents.json\"\n",
    "    with open(filename) as f:\n",
    "        json.dump(docs,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64b90f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = \"documents.json\"\n",
    "with open(filename,'w') as f:\n",
    "    json.dump(docs,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b5814c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index built with 3803 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Bhuvain\n",
      "[nltk_data]     Jhamb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Bhuvain\n",
      "[nltk_data]     Jhamb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Bhuvain\n",
      "[nltk_data]     Jhamb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Download required NLTK data files once\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    return filtered\n",
    "\n",
    "def build_inverted_index(docs):\n",
    "    inverted_index = defaultdict(set)\n",
    "    for doc_id, doc in docs.items():\n",
    "        tokens = preprocess(doc['text'])\n",
    "        for token in set(tokens):  # add once per document\n",
    "            inverted_index[token].add(doc_id)\n",
    "    # Convert sets to lists for JSON serialization\n",
    "    inverted_index = {k: list(v) for k,v in inverted_index.items()}\n",
    "    return inverted_index\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume you loaded docs from previous crawler step\n",
    "    with open(\"documents.json\", \"r\") as f:\n",
    "        docs = json.load(f)\n",
    "\n",
    "    inverted_index = build_inverted_index(docs)\n",
    "\n",
    "    with open(\"inverted_index.json\", \"w\") as f:\n",
    "        json.dump(inverted_index, f)\n",
    "\n",
    "    print(f\"Inverted index built with {len(inverted_index)} unique tokens.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89ff0943",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def build_semantic_index(docs, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    docs: dict of doc_id -> { 'url':..., 'text':... }\n",
    "    returns: dict doc_id -> embedding vector (numpy array)\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = {}\n",
    "    for doc_id, doc in docs.items():\n",
    "        emb = model.encode(doc['text'], convert_to_numpy=True)\n",
    "        embeddings[doc_id] = emb\n",
    "    return embeddings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"documents.json\", \"r\") as f:\n",
    "        docs = json.load(f)\n",
    "\n",
    "    embeddings = build_semantic_index(docs)\n",
    "\n",
    "    # Save embeddings (can't save numpy arrays directly as JSON)\n",
    "    with open(\"semantic_embeddings.pkl\", \"wb\") as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "    print(f\"Built semantic embeddings for {len(embeddings)} documents.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "175198e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['helllodoodood', 'ddsdsdsd']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"helllodoodood ddsdsdsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe08c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt tokenizer is already downloaded within the virtual environment.\n",
      "Stopwords corpus is already downloaded within the virtual environment.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Bhuvain Jhamb/nltk_data'\n    - 'c:\\\\Users\\\\Bhuvain Jhamb\\\\Desktop\\\\Semantic-Search-Engine-with-Summarization-Ranking\\\\myenv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Bhuvain Jhamb\\\\Desktop\\\\Semantic-Search-Engine-with-Summarization-Ranking\\\\myenv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Bhuvain Jhamb\\\\Desktop\\\\Semantic-Search-Engine-with-Summarization-Ranking\\\\myenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bhuvain Jhamb\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Now try tokenizing again\u001b[39;00m\n\u001b[32m     22\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mhelllodoodood ddsdsdsd\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhuvain Jhamb\\Desktop\\Semantic-Search-Engine-with-Summarization-Ranking\\myenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhuvain Jhamb\\Desktop\\Semantic-Search-Engine-with-Summarization-Ranking\\myenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhuvain Jhamb\\Desktop\\Semantic-Search-Engine-with-Summarization-Ranking\\myenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhuvain Jhamb\\Desktop\\Semantic-Search-Engine-with-Summarization-Ranking\\myenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhuvain Jhamb\\Desktop\\Semantic-Search-Engine-with-Summarization-Ranking\\myenv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Bhuvain Jhamb\\Desktop\\Semantic-Search-Engine-with-Summarization-Ranking\\myenv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Bhuvain Jhamb/nltk_data'\n    - 'c:\\\\Users\\\\Bhuvain Jhamb\\\\Desktop\\\\Semantic-Search-Engine-with-Summarization-Ranking\\\\myenv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Bhuvain Jhamb\\\\Desktop\\\\Semantic-Search-Engine-with-Summarization-Ranking\\\\myenv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Bhuvain Jhamb\\\\Desktop\\\\Semantic-Search-Engine-with-Summarization-Ranking\\\\myenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Bhuvain Jhamb\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"Punkt tokenizer is already downloaded within the virtual environment.\")\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading the 'punkt' tokenizer within the virtual environment...\")\n",
    "    nltk.download('punkt')\n",
    "    print(\"'punkt' tokenizer downloaded successfully within the virtual environment.\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"Stopwords corpus is already downloaded within the virtual environment.\")\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading the 'stopwords' corpus within the virtual environment...\")\n",
    "    nltk.download('stopwords')\n",
    "    print(\"'stopwords' corpus downloaded successfully within the virtual environment.\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Now try tokenizing again\n",
    "text = \"helllodoodood ddsdsdsd\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bfc50ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Bhuvain\n",
      "[nltk_data]     Jhamb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc742d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
