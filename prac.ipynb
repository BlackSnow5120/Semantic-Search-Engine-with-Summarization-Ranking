{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a3c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://en.wikipedia.org/wiki/Web_crawler\n",
      "Crawling: https://en.wikipedia.org/wiki/Web_crawler#bodyContent\n",
      "Crawling: https://en.wikipedia.org/wiki/Main_Page\n",
      "Crawling: https://en.wikipedia.org/wiki/Wikipedia:Contents\n",
      "Crawling: https://en.wikipedia.org/wiki/Portal:Current_events\n",
      "Crawling: https://en.wikipedia.org/wiki/Special:Random\n",
      "Crawling: https://en.wikipedia.org/wiki/Wikipedia:About\n",
      "Crawling: https://en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "Crawling: https://en.wikipedia.org/wiki/Help:Contents\n",
      "Crawling: https://en.wikipedia.org/wiki/Help:Introduction\n",
      "Crawled 10 pages.\n",
      "\n",
      "Doc 0: https://en.wikipedia.org/wiki/Web_crawler\n",
      "\n",
      "Doc 1: https://en.wikipedia.org/wiki/Web_crawler#bodyContent\n",
      "\n",
      "Doc 2: https://en.wikipedia.org/wiki/Main_Page\n",
      "\n",
      "Doc 3: https://en.wikipedia.org/wiki/Wikipedia:Contents\n",
      "\n",
      "Doc 4: https://en.wikipedia.org/wiki/Portal:Current_events\n",
      "\n",
      "Doc 5: https://en.wikipedia.org/wiki/Special:Random\n",
      "\n",
      "Doc 6: https://en.wikipedia.org/wiki/Wikipedia:About\n",
      "\n",
      "Doc 7: https://en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "\n",
      "Doc 8: https://en.wikipedia.org/wiki/Help:Contents\n",
      "\n",
      "Doc 9: https://en.wikipedia.org/wiki/Help:Introduction\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "\n",
    "class SimpleCrawler:\n",
    "    def __init__(self, base_url, max_pages=50):\n",
    "        self.base_url = base_url\n",
    "        self.visited = set()\n",
    "        self.to_visit = [base_url]\n",
    "        self.max_pages = max_pages\n",
    "        self.documents = {}  # doc_id -> text\n",
    "\n",
    "    def crawl(self):\n",
    "        while self.to_visit and len(self.visited) < self.max_pages:\n",
    "            url = self.to_visit.pop(0)\n",
    "            if url in self.visited:\n",
    "                continue\n",
    "\n",
    "            print(f\"Crawling: {url}\")\n",
    "            try:\n",
    "                response = requests.get(url, timeout=5)\n",
    "                if response.status_code != 200:\n",
    "                    continue\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # Extract text (simple)\n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "                # Store doc\n",
    "                doc_id = len(self.visited)\n",
    "                self.documents[doc_id] = {'url': url, 'text': text}\n",
    "\n",
    "                self.visited.add(url)\n",
    "\n",
    "                # Find new links within the same domain\n",
    "                base_domain = urlparse(self.base_url).netloc\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    abs_link = urljoin(url, link['href'])\n",
    "                    if urlparse(abs_link).netloc == base_domain and abs_link not in self.visited:\n",
    "                        self.to_visit.append(abs_link)\n",
    "\n",
    "                time.sleep(1)  # polite crawling\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to crawl {url}: {e}\")\n",
    "\n",
    "        print(f\"Crawled {len(self.documents)} pages.\")\n",
    "        return self.documents\n",
    "import json\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = 'https://en.wikipedia.org/wiki/Web_crawler'\n",
    "    crawler = SimpleCrawler(base_url, max_pages=10)\n",
    "    docs = crawler.crawl()\n",
    "    filename = \"documents.json\"\n",
    "    with open(filename) as f:\n",
    "        json.dump(docs,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64b90f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "filename = \"documents.json\"\n",
    "with open(filename,'w') as f:\n",
    "    json.dump(docs,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b5814c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index built with 3803 unique tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Bhuvain\n",
      "[nltk_data]     Jhamb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Bhuvain\n",
      "[nltk_data]     Jhamb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Bhuvain\n",
      "[nltk_data]     Jhamb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Download required NLTK data files once\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    return filtered\n",
    "\n",
    "def build_inverted_index(docs):\n",
    "    inverted_index = defaultdict(set)\n",
    "    for doc_id, doc in docs.items():\n",
    "        tokens = preprocess(doc['text'])\n",
    "        for token in set(tokens):  # add once per document\n",
    "            inverted_index[token].add(doc_id)\n",
    "    # Convert sets to lists for JSON serialization\n",
    "    inverted_index = {k: list(v) for k,v in inverted_index.items()}\n",
    "    return inverted_index\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume you loaded docs from previous crawler step\n",
    "    with open(\"documents.json\", \"r\") as f:\n",
    "        docs = json.load(f)\n",
    "\n",
    "    inverted_index = build_inverted_index(docs)\n",
    "\n",
    "    with open(\"inverted_index.json\", \"w\") as f:\n",
    "        json.dump(inverted_index, f)\n",
    "\n",
    "    print(f\"Inverted index built with {len(inverted_index)} unique tokens.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89ff0943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bhuvain Jhamb\\Desktop\\Semantic-Search-Engine-with-Summarization-Ranking\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Bhuvain Jhamb\\Desktop\\Semantic-Search-Engine-with-Summarization-Ranking\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Bhuvain Jhamb\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built semantic embeddings for 10 documents.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def build_semantic_index(docs, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    docs: dict of doc_id -> { 'url':..., 'text':... }\n",
    "    returns: dict doc_id -> embedding vector (numpy array)\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = {}\n",
    "    for doc_id, doc in docs.items():\n",
    "        emb = model.encode(doc['text'], convert_to_numpy=True)\n",
    "        embeddings[doc_id] = emb\n",
    "    return embeddings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"documents.json\", \"r\") as f:\n",
    "        docs = json.load(f)\n",
    "\n",
    "    embeddings = build_semantic_index(docs)\n",
    "\n",
    "    # Save embeddings (can't save numpy arrays directly as JSON)\n",
    "    with open(\"semantic_embeddings.pkl\", \"wb\") as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "\n",
    "    print(f\"Built semantic embeddings for {len(embeddings)} documents.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175198e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0 | Score: 1.000 | URL: https://en.wikipedia.org/wiki/Web_crawler\n",
      "Snippet: Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Co...\n",
      "\n",
      "Doc 1 | Score: 1.000 | URL: https://en.wikipedia.org/wiki/Web_crawler#bodyContent\n",
      "Snippet: Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Co...\n",
      "\n",
      "Doc 2 | Score: 0.418 | URL: https://en.wikipedia.org/wiki/Main_Page\n",
      "Snippet: Wikipedia, the free encyclopedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn t...\n",
      "\n",
      "Doc 6 | Score: 0.199 | URL: https://en.wikipedia.org/wiki/Wikipedia:About\n",
      "Snippet: Wikipedia:About - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edi...\n",
      "\n",
      "Doc 8 | Score: 0.183 | URL: https://en.wikipedia.org/wiki/Help:Contents\n",
      "Snippet: Help:Contents - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1)*np.linalg.norm(vec2) + 1e-10)\n",
    "with open(\"inverted_index.json\", \"r\") as f:\n",
    "    inverted_index = json.load(f)\n",
    "\n",
    "with open(\"semantic_embeddings.pkl\", \"rb\") as f:\n",
    "    semantic_embeddings = pickle.load(f)\n",
    "\n",
    "# For demo, assume docs dictionary is available here too\n",
    "with open(\"documents.json\", \"r\") as f:\n",
    "    docs = json.load(f)\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "def keyword_search(query_tokens, inverted_index):\n",
    "    \"\"\"\n",
    "    Returns dict doc_id -> keyword score (e.g. term frequency in query)\n",
    "    \"\"\"\n",
    "    doc_scores = Counter()\n",
    "    for token in query_tokens:\n",
    "        if token in inverted_index:\n",
    "            for doc_id in inverted_index[token]:\n",
    "                doc_scores[doc_id] += 1\n",
    "    return doc_scores\n",
    "def semantic_search(query, semantic_embeddings, model, top_k=10):\n",
    "    query_emb = model.encode(query, convert_to_numpy=True)\n",
    "    scores = {}\n",
    "    for doc_id, emb in semantic_embeddings.items():\n",
    "        scores[doc_id] = cosine_similarity(query_emb, emb)\n",
    "    top_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return dict(top_docs)\n",
    "def combine_scores(kw_scores, sem_scores, alpha=0.5):\n",
    "    \"\"\"\n",
    "    alpha: weight for keyword score (0-1), (1-alpha) for semantic score\n",
    "    Normalize scores before combining\n",
    "    \"\"\"\n",
    "    all_doc_ids = set(kw_scores.keys()).union(sem_scores.keys())\n",
    "\n",
    "    # Normalize\n",
    "    max_kw = max(kw_scores.values()) if kw_scores else 1\n",
    "    max_sem = max(sem_scores.values()) if sem_scores else 1\n",
    "\n",
    "    combined_scores = {}\n",
    "    for doc_id in all_doc_ids:\n",
    "        kw_score = kw_scores.get(doc_id, 0) / max_kw\n",
    "        sem_score = sem_scores.get(doc_id, 0) / max_sem\n",
    "        combined_scores[doc_id] = alpha * kw_score + (1 - alpha) * sem_score\n",
    "\n",
    "    # Sort descending\n",
    "    ranked = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked\n",
    "def search(query, inverted_index, semantic_embeddings, model, docs, alpha=0.5, top_k=5):\n",
    "    query_tokens = preprocess(query)\n",
    "    kw_scores = keyword_search(query_tokens, inverted_index)\n",
    "    sem_scores = semantic_search(query, semantic_embeddings, model, top_k=top_k*3)  # get more sem results to combine\n",
    "\n",
    "    combined = combine_scores(kw_scores, sem_scores, alpha=alpha)\n",
    "    top_results = combined[:top_k]\n",
    "\n",
    "    results = []\n",
    "    for doc_id, score in top_results:\n",
    "        doc = docs[str(doc_id)]\n",
    "        snippet = doc['text'][:200].replace('\\n',' ') + '...'  # simple snippet\n",
    "        results.append({'doc_id': doc_id, 'url': doc['url'], 'score': score, 'snippet': snippet})\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"machine learning web crawler\"\n",
    "    results = search(query, inverted_index, semantic_embeddings, model, docs)\n",
    "    for r in results:\n",
    "        print(f\"Doc {r['doc_id']} | Score: {r['score']:.3f} | URL: {r['url']}\\nSnippet: {r['snippet']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc50ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bhuvain Jhamb\\Desktop\\Semantic-Search-Engine-with-Summarization-Ranking\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Bhuvain Jhamb\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n",
      "Your max_length is set to 100, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Your max_length is set to 100, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Your max_length is set to 100, but your input_length is only 85. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0 | Score: 1.000 | URL: https://en.wikipedia.org/wiki/Web_crawler\n",
      "Summary: Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Search Appearance Donate Create account Log in Personal tools Log in Log in Pages for logged out editors learn more Contributions Talk\n",
      "\n",
      "Doc 1 | Score: 1.000 | URL: https://en.wikipedia.org/wiki/Web_crawler#bodyContent\n",
      "Summary: Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Search Appearance Donate Create account Log in Personal tools Log in Log in Pages for logged out editors learn more Contributions Talk\n",
      "\n",
      "Doc 2 | Score: 0.418 | URL: https://en.wikipedia.org/wiki/Main_Page\n",
      "Summary:  Wikipedia, the free encyclopedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools\n",
      "\n",
      "Doc 6 | Score: 0.199 | URL: https://en.wikipedia.org/wiki/Wikipedia:About\n",
      "Summary:  Wikipedia:About - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate create account Login\n",
      "\n",
      "Doc 8 | Score: 0.183 | URL: https://en.wikipedia.org/wiki/Help:Contents\n",
      "Summary:  Wikipedia is a Wikipediaedia. Use the weekly Newsquiz to test your knowledge of stories you saw on Wikipedia. Visit the community portal to help editors with reading comprehension and vocabulary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "with open(\"inverted_index.json\", \"r\") as f:\n",
    "    inverted_index = json.load(f)\n",
    "\n",
    "with open(\"semantic_embeddings.pkl\", \"rb\") as f:\n",
    "    semantic_embeddings = pickle.load(f)\n",
    "\n",
    "with open(\"documents.json\", \"r\") as f:\n",
    "    docs = json.load(f)\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "def generate_summary(text, max_length=60, min_length=30):\n",
    "    input_text = text if len(text) < 1000 else text[:1000]\n",
    "    summary = summarizer(input_text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "def search_with_summary(query, inverted_index, semantic_embeddings, model, docs, alpha=0.5, top_k=5):\n",
    "    query_tokens = preprocess(query)\n",
    "    kw_scores = keyword_search(query_tokens, inverted_index)\n",
    "    sem_scores = semantic_search(query, semantic_embeddings, model, top_k=top_k*3)\n",
    "\n",
    "    combined = combine_scores(kw_scores, sem_scores, alpha=alpha)\n",
    "    top_results = combined[:top_k]\n",
    "\n",
    "    results = []\n",
    "    for doc_id, score in top_results:\n",
    "        doc = docs[str(doc_id)]\n",
    "        snippet = doc['text'][:500].replace('\\n',' ') + '...' \n",
    "        summary = generate_summary(snippet)\n",
    "        results.append({\n",
    "            'doc_id': doc_id,\n",
    "            'url': doc['url'],\n",
    "            'score': score,\n",
    "            'snippet': snippet,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"machine learning web crawler\"\n",
    "    results = search_with_summary(query, inverted_index, semantic_embeddings, model, docs)\n",
    "    for r in results:\n",
    "        print(f\"Doc {r['doc_id']} | Score: {r['score']:.3f} | URL: {r['url']}\\nSummary: {r['summary']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc742d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
