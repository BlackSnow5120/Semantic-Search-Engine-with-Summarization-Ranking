{"0": {"url": "https://en.wikipedia.org/wiki/Web_crawler", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "1": {"url": "https://en.wikipedia.org/wiki/Web_crawler#bodyContent", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "2": {"url": "https://en.wikipedia.org/wiki/Main_Page", "text": "Wikipedia, the free encyclopedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Main Page Main Page Talk English Read View source View history Tools Tools move to sidebar hide Actions Read View source View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikimedia Foundation MediaWiki Meta-Wiki Wikimedia Outreach Multilingual Wikisource Wikispecies Wikibooks Wikidata Wikifunctions Wikimania Wikinews Wikiquote Wikisource Wikiversity Wikivoyage Wiktionary Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Welcome to Wikipedia , the free encyclopedia that anyone can edit . 118,797 active editors 6,994,823 articles in English From today's featured article Habitable zones of TRAPPIST-1 and the Solar System TRAPPIST-1 is a cool red dwarf star with seven known exoplanets . It lies in the constellation Aquarius about 40.66 light-years away from Earth. It has a mass of about 9% of the Sun and is estimated to be 7.6\u00a0billion years old, making it older than the Solar System . The discovery of the star was first published in 2000. Observations in 2016 from the Transiting Planets and Planetesimals Small Telescope (TRAPPIST) led to the discovery of two orbiting terrestrial planets , and five more were found in 2017. It takes the seven planets between about 1.5 and 19 days to orbit around the star. They are likely tidally locked to TRAPPIST-1 , leading to permanent day on one side and night on the other. Their masses are comparable to that of Earth. Up to four of the planets orbit at distances where temperatures are suitable for the existence of liquid water (diagram pictured) , and are thus potentially hospitable to life. This has drawn interest from both researchers and popular culture. ( Full\u00a0article... ) Recently featured: iMac\u00a0G4 Merchant's House Museum Marie Sophie Hingst Archive By email More featured articles About Did you know\u00a0... Bertoncelj at the 2015 European Games ... that Sa\u0161o Bertoncelj (pictured) competed on MasterChef Slovenia during his gymnastics career? ... that the Knox Mountain Hillclimb is considered very challenging due to every corner being off camber ? ... that the works of Yaelokre have been compared to The Lord of the Rings , Gorillaz , and the illustrations of Where the Wild Things Are ? ... that Camp Growden was envisioned to become a winter sports complex with toboggan runs? ... that Paddy Higson was known as the \"mother of Scottish film\"? ... that the robotic costumes used in the Doctor Who episode \" The Robot Revolution \" were 3D printed in thirty-four different pieces before being assembled? ... that Google searches for the word \"fortnight\" in the US increased by 868% on the release day of the song \" Fortnight \" by Taylor Swift ? ... that the Green Bay Packers signed Tom Birney as their new kicker in 1979, even though he had not kicked a field goal in more than five years? ... that a 2015 issue of the Strawberry Newspaper features Hello Kitty discussing military conflicts in Afghanistan, Somalia, and Ukraine? Archive Start a new article Nominate an article In the news Jos\u00e9 Mujica Former president of Uruguay Jos\u00e9 Mujica (pictured) dies at the age of 89 . The Kurdistan Workers' Party announces its dissolution, ending its insurgency against Turkey. Robert Francis Prevost is elected as Pope Leo XIV , becoming the first Catholic pope born in the United States. Friedrich Merz is elected Chancellor of Germany and sworn in alongside his coalition government . Zhao Xintong defeats Mark Williams to win the World Snooker Championship . In horse racing, Sovereignty , ridden by Junior Alvarado , wins the Kentucky Derby . Ongoing : Gaza war M23 campaign Russian invasion of Ukraine timeline Sudanese civil war timeline Recent deaths : Rich Rollins Sharpe James Matthew Best Simon Mann Douglas Gibson Chet Lemon Nominate an article On this day May 17 : International Day Against Homophobia, Biphobia and Transphobia Anne of Denmark 1590 \u2013 Anne of Denmark (pictured) was crowned the queen consort of Scotland in a ceremony at Holyrood Abbey in Edinburgh. 1863 \u2013 American Civil War : At the Battle of Big Black River Bridge in Mississippi, Union forces under John A. McClernand defeated a Confederate rearguard and captured around 1,700 men. 1900 \u2013 The first copies of the children's novel The Wonderful Wizard of Oz by L.\u00a0Frank Baum were printed. 1954 \u2013 The U.S. Supreme Court ruled in the landmark case Brown v. Board of Education , outlawing racial segregation in public schools because \"separate educational facilities are inherently unequal\" and therefore unconstitutional. 1987 \u2013 An Iraqi jet fired two Exocet missiles at the American frigate USS Stark , killing 37 personnel and injuring 21 others. Caroline of Brunswick ( b. 1768) Erik Satie ( b. 1866) Little Gerhard ( b. 1934) Maggie Laubser ( d. 1973) More anniversaries: May 16 May 17 May 18 Archive By email List of days of the year About Today's featured picture The short-beaked echidna ( Tachyglossus aculeatus ) is one of four living species of echidna . It is covered in fur and spines , has a distinctive snout to help detect its surroundings, and uses a specialized tongue to catch insects. Its extremely strong front limbs and claws allow it to burrow quickly. It repels predators by curling into a ball and deters them with its spines. During the Australian winter, it goes into deep torpor and hibernation . As the temperature increases, it emerges to mate . Female echidnas lay one egg a year and the mating period is the only time the solitary animals meet. A newborn echidna grows rapidly on mother's milk and is expelled into the mother's burrow when it grows too large for the pouch. It leaves the burrow when it is around six months old. The species is found throughout Australia and in coastal and highland regions of eastern New Guinea . It is not threatened with extinction, but human activities have reduced its distribution in Australia. This photograph shows a Tasmanian short-beaked echidna ( T.\u00a0a.\u00a0setosus ), a subspecies of the short-beaked echidna, near Scottsdale, Tasmania . Photograph credit: Charles J. Sharp Recently featured: Margaret Hamilton Kiwifruit Lysander Spooner Archive More featured pictures Other areas of Wikipedia Community portal \u2013 The central hub for editors, with resources, links, tasks, and announcements. Village pump \u2013 Forum for discussions about Wikipedia itself, including policies and technical issues. Site news \u2013 Sources of news about Wikipedia and the broader Wikimedia movement. Teahouse \u2013 Ask basic questions about using or editing Wikipedia. Help desk \u2013 Ask questions about using or editing Wikipedia. Reference desk \u2013 Ask research questions about encyclopedic topics. Content portals \u2013 A unique way to navigate the encyclopedia. Wikipedia's sister projects Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation , a non-profit organization that also hosts a range of other volunteer projects : Commons Free media repository MediaWiki Wiki software development Meta-Wiki Wikimedia project coordination Wikibooks Free textbooks and manuals Wikidata Free knowledge base Wikinews Free-content news Wikiquote Collection of quotations Wikisource Free-content library Wikispecies Directory of species Wikiversity Free learning tools Wikivoyage Free travel guide Wiktionary Dictionary and thesaurus Wikipedia languages This Wikipedia is written in English . Many other Wikipedias are available ; some of the largest are listed below. 1,000,000+ articles \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Deutsch Espa\u00f1ol \u0641\u0627\u0631\u0633\u06cc \u200e Fran\u00e7ais Italiano Nederlands \u65e5\u672c\u8a9e Polski Portugu\u00eas \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Svenska \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 Ti\u1ebfng Vi\u1ec7t \u4e2d\u6587 250,000+ articles Bahasa Indonesia Bahasa Melayu B\u00e2n-l\u00e2m-g\u00fa \u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438 Catal\u00e0 \u010ce\u0161tina Dansk Eesti Esperanto Euskara \u05e2\u05d1\u05e8\u05d9\u05ea \u0540\u0561\u0575\u0565\u0580\u0565\u0576 \ud55c\uad6d\uc5b4 Magyar Norsk bokm\u00e5l Rom\u00e2n\u0103 Simple English Sloven\u010dina Srpski Srpskohrvatski Suomi T\u00fcrk\u00e7e O\u02bbzbekcha 50,000+ articles Asturianu Az\u0259rbaycanca \u09ac\u09be\u0982\u09b2\u09be Bosanski \u06a9\u0648\u0631\u062f\u06cc \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Frysk Gaeilge Galego Hrvatski \u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8 Kurd\u00ee Latvie\u0161u Lietuvi\u0173 \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02 \u041c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438 \u1019\u103c\u1014\u103a\u1019\u102c\u1018\u102c\u101e\u102c Norsk nynorsk \u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40 Shqip Sloven\u0161\u010dina \u0e44\u0e17\u0e22 \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41 \u0627\u0631\u062f\u0648 Retrieved from \" https://en.wikipedia.org/w/index.php?title=Main_Page&oldid=1276485694 \" 49 languages \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca \u09ac\u09be\u0982\u09b2\u09be \u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438 Bosanski Catal\u00e0 \u010ce\u0161tina Dansk Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Esperanto Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais Galego \ud55c\uad6d\uc5b4 Hrvatski Bahasa Indonesia Italiano \u05e2\u05d1\u05e8\u05d9\u05ea \u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8 Latvie\u0161u Lietuvi\u0173 Magyar \u041c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438 Bahasa Melayu Nederlands \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk Polski Portugu\u00eas Rom\u00e2n\u0103 \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English Sloven\u010dina Sloven\u0161\u010dina \u06a9\u0648\u0631\u062f\u06cc \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Srpskohrvatski / \u0441\u0440\u043f\u0441\u043a\u043e\u0445\u0440\u0432\u0430\u0442\u0441\u043a\u0438 Suomi Svenska \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 Ti\u1ebfng Vi\u1ec7t \u4e2d\u6587 This page was last edited on 19 February 2025, at 03:32 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Main Page 49 languages Add topic"}, "3": {"url": "https://en.wikipedia.org/wiki/Wikipedia:Contents", "text": "Wikipedia:Contents - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Navigating Wikipedia's subjects Toggle Navigating Wikipedia's subjects subsection 1.1 Culture 1.2 Geography 1.3 Health 1.4 History 1.4.1 Timelines 1.4.2 Current history entries 1.5 Human activities 1.6 Mathematics and formal sciences 1.7 Natural sciences 1.8 People 1.9 Philosophy 1.10 Reference works 1.10.1 Third-party classification systems 1.10.2 Bibliographies 1.11 Religion 1.12 Society and social sciences 1.13 Technology and applied sciences 2 Wikipedia's main content systems Toggle Wikipedia's main content systems subsection 2.1 Overview articles 2.2 Outline pages 2.3 List pages 2.4 Portals 2.5 Glossaries 2.6 Category system 3 Articles by importance or quality Toggle Articles by importance or quality subsection 3.1 Vital articles 3.2 List of articles every Wikipedia should have 3.3 Featured content 3.4 Good content 4 Spoken articles 5 Alphabetical lists of articles Toggle the table of contents Wikipedia : Contents 47 languages \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u0985\u09b8\u09ae\u09c0\u09af\u09bc\u09be \u062a\u06c6\u0631\u06a9\u062c\u0647 \u09ac\u09be\u0982\u09b2\u09be \u092d\u094b\u091c\u092a\u0941\u0930\u0940 \u0427\u04d1\u0432\u0430\u0448\u043b\u0430 Cebuano Espa\u00f1ol \u0641\u0627\u0631\u0633\u06cc \u0413\u04c0\u0430\u043b\u0433\u04c0\u0430\u0439 \ud55c\uad6d\uc5b4 \u0939\u093f\u0928\u094d\u0926\u0940 Hrvatski Bahasa Indonesia \u049a\u0430\u0437\u0430\u049b\u0448\u0430 \u041a\u043e\u043c\u0438 Kurd\u00ee Magyar \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02 Bahasa Melayu \u1019\u103c\u1014\u103a\u1019\u102c\u1018\u102c\u101e\u102c Nederlands \u0928\u0947\u092a\u093e\u0932\u0940 \u65e5\u672c\u8a9e O\u02bbzbekcha / \u045e\u0437\u0431\u0435\u043a\u0447\u0430 \u067e\u069a\u062a\u0648 \u1797\u17b6\u179f\u17b6\u1781\u17d2\u1798\u17c2\u179a Polski Portugu\u00eas \u0420\u0443\u0441\u0441\u043a\u0438\u0439 \u1c65\u1c5f\u1c71\u1c5b\u1c5f\u1c72\u1c64 \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0633\u0646\u068c\u064a \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Sunda Suomi Tagalog \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0422\u0430\u0442\u0430\u0440\u0447\u0430 / tatar\u00e7a \u1010\u1086\u1038 \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41 \u0e44\u0e17\u0e22 \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 Xitsonga \u05d9\u05d9\u05b4\u05d3\u05d9\u05e9 \u7cb5\u8a9e \u4e2d\u6587 Edit links Project page Talk English Read View source View history Tools Tools move to sidebar hide Actions Read View source View history General What links here Related changes Upload file Permanent link Page information Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikiversity Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia List of Wikipedia's tables of contents Overviews Outlines Lists Portals Glossaries Categories Vital articles Featured content Good articles Indices A\u2013Z index Reference Culture Geography Health History Human activities Mathematics Nature People Philosophy Religion Society Technology \"WP:START\" redirects here. For Wikipedia's criteria for Start-class articles, see WP:STARTCLASS . This page lists Wikipedia's tables of contents. For Wikipedia's community directory, see Wikipedia:Directories and indexes . Shortcuts WP:START WP:EXPLORE Explore the vast knowledge of Wikipedia through these helpful resources. If you have a specific topic in mind, use Wikipedia's search box. If you don't know exactly what you are looking for or wish to explore broad areas, click on a link in the header menu at the top of this page, or begin your browsing below: Navigating Wikipedia's subjects Wikipedia organizes its content into distinct subject classifications, each with further subdivisions. Culture Explore the diverse cultures, arts, beliefs, and customs of human societies. Culture Geography Discover the wonders of Earth's lands, features, inhabitants, and planetary phenomena. Geography Health Learn about physical, mental, and social health. Health History Dive into the past through written records and scholarly exploration. History Timelines Explore chronological events through our comprehensive timelines. List of timelines Current history entries Stay up-to-date with encyclopedia entries covering ongoing events. 2025 \u2013 Major events this year 2025 in science \u2013 Ongoing science findings and technology advancements Portal:Current events \u2013 Featured current events and related project activities Human activities Delve into diverse human actions, from leisure and entertainment to industry and warfare. Human activities Mathematics and formal sciences Explore the study of quantity, structure, space, and change. Mathematics and logic Natural sciences Understand natural phenomena through empirical evidence, observations, and experiments. Natural and physical sciences People Learn about collective entities, ethnic groups, and nations. People Philosophy Dive deep into fundamental questions about existence, knowledge, values, and more. Philosophy Reference works Access comprehensive information collections compiled for easy retrieval. Reference works Third-party classification systems Refer to various third-party classification systems linked to Wikipedia articles. Library of Congress Classification List of Dewey Decimal classes Figurative system of human knowledge ( Encyclop\u00e9die ) Prop\u00e6dia ( Encyclop\u00e6dia Britannica ) Tree of knowledge system Universal Decimal Classification Bibliographies Access sources on specific topics for further reading or verification. Wikipedia:List of bibliographies \u2013 Compilation of bibliographies Category:Wikipedia bibliographies \u2013 Multi-page catalog of Wikipedia's bibliographies Religion Explore social-cultural systems, beliefs, ethics, and more. Religion Society and social sciences Understand collectives, social interactions, political authority, and cultural norms. Society Technology and applied sciences Learn about techniques, skills, methods, and processes in technology and science. Technology Wikipedia's main content systems Overview articles Get summaries of broad topics with links to subtopics, biographies, and related articles. Overview articles Outline pages Explore topics in outline format, linking to more detailed articles. Outline pages List pages Find enumerations of specific types, such as lists of countries and people. List pages Portals Access featured articles, images, news, and more through thematic portals. Portals Glossaries Access lists of terms with definitions through alphabetical glossaries. Glossaries Category system Browse Wikipedia's category pages, which index articles by subject. Category:Main topic classifications \u2013 Arts, History, Technology, and more Wikipedia:Contents/Categories \u2013 Hand-crafted list of topic categories Category:People \u2013 Biographies Articles by importance or quality Vital articles Explore subjects that demand high-quality articles, grouped by importance. Vital articles Level 1 \u2013 10 most important articles Level 2 \u2013 100 most important articles Level 3 \u2013 1,000 most important articles Level 4 \u2013 10,000 most important articles Level 5 \u2013 50,000 most important articles List of articles every Wikipedia should have Explore topics where Wikipedias of all languages should have articles. List of articles every Wikipedia should have List of articles every Wikipedia should have/Expanded Featured content Discover Wikipedia's best, reviewed and vetted for quality. Featured content Featured articles Featured lists Featured pictures Featured topics Good content Explore well-written, factually accurate articles that meet editorial standards. Good articles Good topics Spoken articles Listen to Wikipedia articles as spoken word recordings. Category:Spoken articles Wikipedia:Spoken articles Alphabetical lists of articles Browse Wikipedia's articles alphabetically. Special:Allpages \u2013 List of all current pages Wikipedia:Contents/A\u2013Z index \u2013 Alphabetical index Category:Wikipedia indexes \u2013 Alphabetical list of topic indexes Wikipedia:Contents/Indices \u2013 Indexes sorted by topic area Content listings Topics Current events Reference Culture Geography Health History Mathematics Nature People Philosophy Religion Society Technology Types Vital articles Featured content Good articles Spoken articles Overviews Outlines Lists Portals Glossaries Categories Indices Places, people and times Academic disciplines Anniversaries (days of the year) today Sovereign states and dependent territories Timelines decades, centuries, and millennia Indices A\u2013Z index Categories Dewey Decimal classes Library of Congress Classification v t e Wikipedia directories and indexes Administration pages Protocols Policies Guidelines Manual of Style Assistance Help directory Menu FAQs Interactive help Reader's index Tips Styletips Tools The community Portal Discussions Noticeboards Essays Editor's index Departments Maintenance WikiProjects MediaWiki Wikitext HTML Templates Locutions Abbreviations Edit summaries Glossary Shortcuts Encyclopedia proper Types Overviews Outlines Lists Portals Glossaries Categories Indices Featured , good Featured articles Good articles Featured lists Featured pictures Featured topics Good topics Topics Current events Reference Culture Geography Health History Math Nature People Philosophy Religion Society Technology LOC, bios, times Academic disciplines Anniversaries Today Sovereign states and dependent territories Deaths this year Timelines Decades, centuries, and millennia Indexes A\u2013Z index Categories Dewey Decimal classes Library of Congress Classification Spoken articles Searching Retrieved from \" https://en.wikipedia.org/w/index.php?title=Wikipedia:Contents&oldid=1289583872 \" Categories : Wikipedia directories Wikipedia contents Wikipedia navigation Hidden categories: Wikipedia semi-protected project pages Wikipedia move-protected project pages This page was last edited on 9 May 2025, at 14:37 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Wikipedia : Contents 47 languages Add topic"}, "4": {"url": "https://en.wikipedia.org/wiki/Portal:Current_events", "text": "Portal:Current events - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Portal : Current events 118 languages \u00c6nglisc \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u0985\u09b8\u09ae\u09c0\u09af\u09bc\u09be \u0905\u0935\u0927\u0940 \u0410\u0432\u0430\u0440 \u062a\u06c6\u0631\u06a9\u062c\u0647 Banjar \u95a9\u5357\u8a9e / B\u00e2n-l\u00e2m-g\u00fa \u0411\u0430\u0448\u04a1\u043e\u0440\u0442\u0441\u0430 \u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f \u092d\u094b\u091c\u092a\u0941\u0930\u0940 Bikol Central Catal\u00e0 \u0427\u04d1\u0432\u0430\u0448\u043b\u0430 Cebuano \u010ce\u0161tina ChiTumbuka Dagbanli Dansk Deutsch \u078b\u07a8\u0788\u07ac\u0780\u07a8\u0784\u07a6\u0790\u07b0 \u0921\u094b\u091f\u0947\u0932\u0940 \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac \u042d\u0440\u0437\u044f\u043d\u044c Espa\u00f1ol Esperanto Estreme\u00f1u Euskara E\u028begbe \u0641\u0627\u0631\u0633\u06cc F\u00f8royskt Fran\u00e7ais Gaeilge Gaelg \u8d1b\u8a9e \u06af\u06cc\u0644\u06a9\u06cc \u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0 \u5ba2\u5bb6\u8a9e / Hak-k\u00e2-ng\u00ee \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 \u0939\u093f\u0928\u094d\u0926\u0940 Hrvatski Bahasa Indonesia Interlingua \u0418\u0440\u043e\u043d IsiZulu \u00cdslenska \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 Kapampangan \u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8 \u049a\u0430\u0437\u0430\u049b\u0448\u0430 Kurd\u00ee Ladino Latina L\u00ebtzebuergesch Lietuvi\u0173 Li Niha Limburgs Magyar \u092e\u0948\u0925\u093f\u0932\u0940 \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02 \u092e\u0930\u093e\u0920\u0940 \u0645\u0635\u0631\u0649 \u0645\u0627\u0632\u0650\u0631\u0648\u0646\u06cc Bahasa Melayu Minangkabau \u041c\u043e\u043d\u0433\u043e\u043b \u1019\u103c\u1014\u103a\u1019\u102c\u1018\u102c\u101e\u102c N\u0101huatl Nederlands \u0928\u0947\u092a\u093e\u0932\u0940 \u0928\u0947\u092a\u093e\u0932 \u092d\u093e\u0937\u093e \u65e5\u672c\u8a9e Napulitano \u041d\u043e\u0445\u0447\u0438\u0439\u043d Occitan O\u02bbzbekcha / \u045e\u0437\u0431\u0435\u043a\u0447\u0430 \u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40 \u067e\u0646\u062c\u0627\u0628\u06cc \u067e\u069a\u062a\u0648 Piemont\u00e8is Polski Portugu\u00eas Qaraqalpaqsha Rom\u00e2n\u0103 \u0420\u0443\u0441\u0438\u043d\u044c\u0441\u043a\u044b\u0439 \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Scots Sicilianu \u0dc3\u0dd2\u0d82\u0dc4\u0dbd \u0633\u0646\u068c\u064a Sloven\u0161\u010dina Soomaaliga \u06a9\u0648\u0631\u062f\u06cc \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Sunda Suomi Svenska Tagalog \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0422\u0430\u0442\u0430\u0440\u0447\u0430 / tatar\u00e7a \u0e44\u0e17\u0e22 \u0422\u043e\u04b7\u0438\u043a\u04e3 \u13e3\u13b3\u13a9 Tyap \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u0627\u0631\u062f\u0648 Veps\u00e4n kel\u2019 Ti\u1ebfng Vi\u1ec7t Volap\u00fck Walon \u6587\u8a00 \u5434\u8bed \u05d9\u05d9\u05b4\u05d3\u05d9\u05e9 Yor\u00f9b\u00e1 \u7cb5\u8a9e Ze\u00eauws \u4e2d\u6587 Edit links Portal Talk English Read View source View history Tools Tools move to sidebar hide Actions Read View source View history General What links here Related changes Upload file Permanent link Page information Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide Edit instructions From Wikipedia, the free encyclopedia Portal maintenance status: (October 2020) This portal's subpages have been checked by an editor, and are needed. Please take care when editing, especially if using automated editing software . Learn how to update the maintenance information here . Wikipedia portal for content related to Current events Wikimedia portal Worldwide current events Sports events Recent deaths Entry views by week list Wikimedia portal Topics in the news Jos\u00e9 Mujica Former president of Uruguay Jos\u00e9 Mujica (pictured) dies at the age of 89 . The Kurdistan Workers' Party announces its dissolution, ending its insurgency against Turkey. Robert Francis Prevost is elected as Pope Leo XIV , becoming the first Catholic pope born in the United States. Friedrich Merz is elected Chancellor of Germany and sworn in alongside his coalition government . Zhao Xintong defeats Mark Williams to win the World Snooker Championship . In horse racing, Sovereignty , ridden by Junior Alvarado , wins the Kentucky Derby . Ongoing : Gaza war M23 campaign Russian invasion of Ukraine timeline Sudanese civil war timeline Recent deaths : Rich Rollins Sharpe James Matthew Best Simon Mann Douglas Gibson Chet Lemon Nominate an article May\u00a017,\u00a02025 ( 2025-05-17 ) (Saturday) edit history watch Armed conflicts and attacks Russian invasion of Ukraine Attacks on civilians in the Russian invasion of Ukraine A bus evacuating civilians is attacked by a Russian drone near Bilopillia , Sumy Oblast , Ukraine , killing 9 people and injuring five others. (Ukrainska Pravda) Politics and elections South Korean former President Yoon Suk Yeol leaves the People Power Party , a month after the Constitutional Court of Korea removed him from office due to short-lived martial law . (Reuters) May\u00a016,\u00a02025 ( 2025-05-16 ) (Friday) edit history watch Armed conflicts and attacks Gaza war Israeli bombing of the Gaza Strip More than 74 people have been killed in Israeli strikes on the Gaza Strip since midnight. (RT\u00c9 News) The Israeli Air Force launches airstrikes on the ports of Hudaydah and Salif in Houthi-controlled Yemen , as prime minister Benjamin Netanyahu and defense minister Israel Katz warn that if the Houthis persist in attacking Israel , their leader Abdul-Malik al-Houthi will be directly targeted. (Al Arabiya) Business and economy United States federal government credit-rating downgrades Moody's Ratings lowers the United States ' credit rating from Aaa to Aa1, citing the government's rising debt, widening deficits, and increased interest payments. ( USA Today ) Disasters and accidents Tornado outbreak sequence of May 15\u201316, 2025 A tornado strikes St. Louis, Missouri . At least five people are killed, 35-45 others are injured and more than 100,000 are without power in the St. Louis metropolitan area following severe weather in the city and surrounding areas. (Associated Press) (KSDK) Five people are killed and three others are injured by a landslide at a hydropower plant construction site in Phong Th\u1ed5 , Lai Ch\u00e2u province , Vietnam . (News.az) International relations Foreign relations of Angola , 2023 Gabonese coup d'\u00e9tat The Government of Angola says that former President of Gabon Ali Bongo , along with former First Lady Sylvia Bongo Ondimba and their son Noureddin Bongo Valentin arrived last night in Angola , where they received asylum \"for humanitarian reasons\", days after their release from prison, where they had been since the 2023 coup. (Reuters) Law and crime Stabbing of Salman Rushdie A court in New York , United States sentences 27-year old Hadi Matar to 25 years in prison for the August 2022 stabbing attack on Indian-British-American author Salman Rushdie , which left Rushdie severely injured while he was giving a talk in Chautauqua . (AP) 2025 Las Vegas gym shooting At least two people are killed, including the perpetrator and four others are injured, including one critically, in a mass shooting and shootout when a man opens fire inside of a gym in Las Vegas , Nevada , United States. (KTNV) Ten inmates take part in a mass escape from the Orleans Parish jail in New Orleans , Louisiana , United States. Three are recaptured; the other seven remain fugitives . ( The Times-Picayune/The New Orleans Advocate ) International Criminal Court Prosecutor Karim Ahmad Khan temporarily steps aside from his role while the United Nations investigates the sexual misconduct allegations against him. ( The Washington Post ) May\u00a015,\u00a02025 ( 2025-05-15 ) (Thursday) edit history watch Armed conflicts and attacks Islamist insurgency in Burkina Faso Islamist militant group Jama'at Nasr al-Islam wal-Muslimin claims it killed 200 soldiers in an attack on a military base on Sunday in Djibo , Sahel , Burkina Faso . They also claimed responsibility for the Diapaga attack this week at a military post in Loroum Province that killed 60 soldiers. (Al Jazeera) Papua conflict The Indonesian military confirms that 18 Papuan separatists and three civilians were killed in yesterday's military operation in Intan Jaya Regency , Central Papua , Indonesia . (Reuters) (detikcom) Arts and culture A long-lost version of the British Magna Carta , dating to 1300 in the reign of King Edward I , is believed to have been found at Harvard University in the United States . (BBC) ( The Guardian ) Disasters and accidents 2025 Canadian wildfires Two people are killed and more than 1,000 others evacuate after wildfires spread across central Manitoba , Canada . (Euronews) Sinking of the Bayesian British investigators say the cause of the sinking of the Bayesian superyacht that killed seven people, including billionaire tech magnate Mike Lynch in Italy in 2024, is from being knocked over by extreme wind and could not recover. (CTV News) Five people are killed and three others are injured after a van drives into the back of a truck on the national road 2 in Konotopa , Masovian Voivodeship , Poland . (TVN24) Three people, including two firefighters, are killed and two other firefighters are seriously injured in a fire at a former Royal Air Force base in Oxfordshire , England . (BBC News) International relations Iran\u2013United States relations 2025 US\u2013Iran negotiations U.S. president Donald Trump says that the U.S. and Iran have \"sort of\" agreed on the terms of a deal on Iran's nuclear program , which reportedly includes Iran agreeing to give up highly enriched uranium while keeping lower-grade uranium needed for civilian nuclear power , in exchange for the lifting of sanctions . (AP) Myanmar\u2013Vietnam relations According to the Vietnamese foreign ministry , more than 650 Vietnamese nationals are detained for immigration violations in Myanmar . The Vietnamese public security ministry has repatriated 450 people, with 200 still in Myanmar. (Vietnam+) ( Vi\u1ec7t Nam News ) Visa policy of mainland China At the China-CELAC Forum in Beijing , the Chinese foreign ministry announces it will allow nationals from Argentina , Brazil , Chile , Peru , and Uruguay to enter the country without a travel visa starting June 1. (AP) Law and crime Capital punishment in Florida The U.S. state of Florida executes serial killer Glen Edward Rogers for the 1995 murder of a woman at a motel in Tampa . ( The Washington Post ) LGBT rights in Russia , Russian anti-LGBTQ law Russian media reports that police raided the publisher office of Eksmo Publishing House yesterday and arrested several of its employees, accusing them of spreading \" LGBTQ + extremism\". ( The Moscow Times ) Politics and elections 2025 Australian Greens leadership election Larissa Waters is elected leader of the Australian Greens , following the resignation of Adam Bandt after the party's poor performance at the 2025 Australian federal election . (ABC News) Sports 2024\u201325 La Liga In association football , FC Barcelona win their 28th title in the 94th season of La Liga after defeating city rivals RCD Espanyol 2\u20130 with two matches to spare. (ESPN) ( The Independent ) May\u00a014,\u00a02025 ( 2025-05-14 ) (Wednesday) edit history watch Armed conflicts and attacks Israeli\u2013Palestinian conflict Gaza war Israeli bombing of the Gaza Strip Israeli airstrikes kill at least 80 Palestinians across the Gaza Strip . (Reuters) Palestinian political violence Two Israelis , including a pregnant woman and her child, are critically wounded in a shooting near Brukhin in the West Bank . ( The Jerusalem Post ) (YNet) Hezbollah\u2013Israel conflict Israeli invasion of Lebanon The Israel Defense Forces claim that a drone strike in Qaaqaait al-Jisr , Nabatieh Governorate , Lebanon , killed a Hezbollah commander. ( The Times of Israel ) Naxalite\u2013Maoist insurgency Operation Black Forest Indian authorities declare a \"major success\" in a military operation against Naxal insurgents in the border between the states of Chhattisgarh and Telangana , killing 31 militants, including top leaders of the Maoist group. ( The Hindustan Times ) Disasters and accidents Two hikers are found dead and seven others are rescued after they were swept away by a river while hiking near Didier Waterfall on the island of Martinique . (AP) International relations China\u2013Colombia relations Colombia joins China 's Belt and Road Initiative during the China-CELAC Forum in Beijing . (AFP via RFI) Syria\u2013United States relations U.S. president Donald Trump meets with Syrian interim president Ahmed al-Sharaa during a trip to Saudi Arabia prior to departing for Qatar , marking the first meeting between a U.S. and Syrian president since 2000. (Al Jazeera) Law and crime Immigration to Argentina Argentine president Javier Milei orders restrictions on immigration to Argentina , saying that immigrants are bringing \"chaos and abuse\" to Argentina. (AP) Tajik president Emomali Rahmon signs a law decriminalizing the \" liking \" of posts on social media publicly calling for terrorism or other serious crimes. (Reuters) Russian election observer Grigory Melkonyants [ ru ] is convicted on charges of working with an \"undesirable organization\" and sentenced to five years in prison. ( The New York Times ) Politics and elections Deportation of undocumented Afghans from Pakistan Pakistani interior minister Mohsin Naqvi says that more than 1 million Afghans have been deported back to Afghanistan since November 2023 as Pakistani authorities intensify a crackdown on illegal immigration . (AMU TV) May\u00a013,\u00a02025 ( 2025-05-13 ) (Tuesday) edit history watch Armed conflicts and attacks Gaza war 2025 Gaza European Hospital strikes At least 28 people are killed in an Israeli air strike that struck the Gaza European Hospital , which the Israeli military claims had a Hamas command and control centre beneath the building. (BBC News) Puntland counter-terrorism operations Three Al-Shabaab fighters, including a senior commander, are killed in a coordinated operation by the Somali National Army in Jowle , Mudug , Puntland . (Garowe Online) Business and economy Microsoft Inc. abruptly lays off 6,000 people, 3% of its global workforce, citing workforce productivity . (AP) (GeekWire) Disasters and accidents 2025 Russian wildfires A state of emergency is declared in Buryatia , Far Eastern Federal District , Russia , as firefighters fight wildfires currently burning more than 53,000 hectares. ( The Moscow Times ) Fourteen people are killed and four others are hospitalized after drinking toxic liquor in Amritsar , Punjab , India . (AP) International relations Syria\u2013United States relations US President Donald Trump announces that the United States will lift all sanctions imposed on Syria under the previous regime of President Bashar al-Assad , and expresses willingness to work with the Syrian transitional government . (Al Jazeera) Law and crime Armed conflict for control of the favelas Brazilian Military Police kill the main leader of the Terceiro Comando Puro , a drug trafficking criminal organization , and two other suspected members, in a shootout in the Mar\u00e9 favela in Rio de Janeiro . (AP) ( Seattle Post-Intelligencer ) Politics and elections Islamic State insurgency in Puntland Puntland counter-terrorism operations A spokesperson for the Puntland Security Force confirms that Puntland has acquired four military helicopters to support ongoing military operations against Islamic State militants in the region. (Garowe Online) 2025 Liberal Party of Australia leadership election Sussan Ley is elected as the new leader of the Liberal Party of Australia following the party's defeat at Saturday's federal election . Ley is the first woman to hold the post. (Reuters) 2025 Malian protests , 2020 Malian coup d'\u00e9tat Malian interim president Assimi Go\u00efta signs a decree dissolving all political parties and organizations and bans them from holding meetings following ongoing pro- democracy protests in the capital city Bamako . (AP) (Al Jazeera) Prime Minister of Peru Gustavo Adrianz\u00e9n resigns a day before a vote of censure against him over rising crime in Peru . (Reuters) Sports Major League Baseball Commissioner of Baseball Rob Manfred announces that all deceased individuals on the permanent ineligible list will be reinstated, allowing players like Pete Rose and Shoeless Joe Jackson to be elected into the National Baseball Hall of Fame and Museum . (ESPN) 2024\u201325 Serie B Multiple European and domestic title winners UC Sampdoria are relegated to the third tier of the Italian football league system for the first time in their 78 year history. ( ESPN ), ( BBC Sport ), ( The Guardian ) May\u00a012,\u00a02025 ( 2025-05-12 ) (Monday) edit history watch Armed conflicts and attacks Gaza war Gaza war hostage crisis Israeli-American hostage Edan Alexander is released by Hamas after being kept in captivity in Gaza for 19 months since the October 7 attack . (Reuters) Libyan crisis 2025 Tripoli clashes Clashes erupt between rival militia gunmen from the 444th Brigade and the Stability Support Apparatus in Tripoli , Libya , after the assassination of Commander Abdel Ghani al-Kikli . Six people are injured, and a state of emergency is declared by authorities. ( The Jerusalem Post ) (Al Jazeera English) Kurdistan Workers' Party insurgency At its 12th Congress, the Kurdistan Workers' Party announces an immediate halt to its insurgent activities in Turkey and plans for its formal dissolution. (Al Jazeera) (Reuters) Myanmar civil war The National Unity Government of Myanmar says that an airstrike by junta forces killed 17 students and injured 20 others at a school in Tabayin , Sagaing Region . (Reuters) Syrian civil war The remains of 30 people believed to have been killed by Islamic State militants are found in a search by Qatar and the U.S. Federal Bureau of Investigation in the remote town of Dabiq , Aleppo Governorate , Syria . (CTV News) Unknown gunmen attack a Chinese artisanal gold mine in Narena , Koulikoro Region , Mali , killing one Malian and two others from Ghana , and abducting two Chinese nationals. (AP) Disasters and accidents Thirteen people are killed by an expired ammunition explosion in Garut Regency , West Java , Indonesia . ( Jakarta Globe ) International relations China\u2013United States trade war The U.S. and China agree to a trade deal in which, for 90 days, U.S. tariffs on most Chinese goods will drop from 145% to 30%, while China's retaliatory tariffs on U.S. goods will drop from 125% to 10%. (BBC News) Immigration policy of the second Donald Trump administration , South Africa\u2013United States relations The first group of Afrikaners arrive in the United States after U.S. president Donald Trump grants refugee status to the white minority group, who Trump says face a \"genocide\" in the South African farm attacks . The government of South Africa rejects the claims. (AP) Politics and elections 2025 Philippine general election Filipinos vote to elect all 317 members of the House of Representatives and 12 members of the Senate , in addition to local elections . ( Asian Journal ) (Reuters) May\u00a011,\u00a02025 ( 2025-05-11 ) (Sunday) edit history watch Armed conflicts and attacks Gaza war Israeli bombing of the Gaza Strip Fifteen Palestinians , mostly women and children, are killed in overnight Israeli airstrikes across the Gaza Strip . (CTV News) Gaza war hostage crisis Hamas agrees to release Edan Alexander , a dual American-Israeli citizen hostage, tomorrow. (Reuters) Russian invasion of Ukraine Peace negotiations in the Russian invasion of Ukraine Russian President Vladimir Putin proposes holding direct negotiations in Istanbul , Turkey , on May 15 after the United Kingdom , France , Germany and Poland threatened further sanctions if it does not agree to an unconditional 30-day ceasefire beginning on May 12. (Reuters) Ukrainian President Volodymyr Zelenskyy calls on Russia to confirm an unconditional ceasefire before any direct negotiations between the two nations. (Reuters) Insurgency in Khyber Pakhtunkhwa At least two police officers are killed and three others are injured in a roadside bombing targeting a police vehicle in Khyber Pakhtunkhwa , Pakistan . (CTV News) Israeli MIAs The body of Israel Defense Forces soldier Zvi Feldman, missing since the 1982 Lebanon War , is repatriated to Israel from \"deep inside Syria \" in a joint Mossad \u2013IDF operation. ( The Times of Israel ) (AP) Disasters and accidents Mediterranean Sea migrant smuggling Three people are found dead, including two children, on a dinghy crossing the Mediterranean Sea from Libya to Italy . German NGO RESQSHIP intercepts the boat and brings the remaining 59 survivors to Lampedusa . (DW) A bus carrying Buddhist pilgrims in Kotmale , Sri Lanka , swerves off a cliff , killing 21 people and injuring 35 others. (CTV News) Five people are killed, dozens are injured including three critically, and hundreds are displaced in a fire at a four-story apartment building in Milwaukee , Wisconsin , United States. (WTMJ-TV) (CNN) A runner collapses and later dies near the finish line during running event Loop Leeuwarden in Leeuwarden , Netherlands , causing the organizers to cancel the event. (NOS) International relations Poland\u2013Russia relations Polish authorities confirm evidences of Russian sabotage behind the arson attack that destroyed most of the Marywilska 44 marketplace in Warsaw in 2024. As a reaction, the Russian consulate general in Krak\u00f3w is ordered to close down. ( The Guardian ) (AP) In his first Sunday address, Pope Leo XIV calls for peace in Ukraine , the release of all war prisoners and the return of abducted Ukrainian children to their families, an immediate ceasefire , the release of all hostages and the allowance of humanitarian aid in Gaza , as well as a permanent ceasefire to the India\u2013Pakistan conflict . ( The Guardian ) Law and crime The Taliban bans chess in Afghanistan , with a Taliban spokesman saying that chess is considered a form of gambling under Sharia law. (BBC News) Politics and elections 2025 Albanian parliamentary election Albanians vote to elect the 140 members of the Parliament , as Prime Minister Edi Rama seeks reelection for a fourth term. (Reuters) More May 2025 events... Time : 08:31 UTC | Day : 17 May \u25c0 May 2025 \u25b6 S M T W T F S 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 More May 2025 events... Wikimedia portal About this page \u2022 News about Wikipedia Ongoing events Disasters Avian flu outbreak United States opioid epidemic Economics Chinese property sector crisis German economic crisis United Kingdom cost-of-living crisis United States tariff policy United States\u2013China trade war Politics and diplomacy Gaza war hostage crisis India\u2013Pakistan standoff Protests and strikes Gaza war protests Georgian protests and political crisis Indonesian anti-government protests Serbian anti-corruption protests Turkish anti-government protests edit section Elections and referendums Recent May 0 3: Australia , Senate, House of Representatives 0 3: Singapore , Parliament 0 3: Togo , President (indirect) 0 4: Romania , President (1st) 7\u20138: Vatican City , Pope (indirect) 11: Albania , Parliament 12: Philippines , Senate , House of Representatives Upcoming 18: Poland , President 18: Portugal , Assembly of the Republic 18: Romania , President (2nd) 25: Suriname , National Assembly 25: Venezuela , National Assembly edit section Trials Recently concluded South Korea: 0 Yoon Suk Yeol Ongoing Canada: 0 Alex Formenton, Cal Foote, Carter Hart, Dillon Dub\u00e9, Michael McLeod United States: 0 Sean Combs edit section Sports Association football 2024\u201325 UEFA Champions League 2024\u201325 UEFA Europa League 2024\u201325 UEFA Conference League 2024\u201325 UEFA Women's Champions League 2024\u201325 Premier League 2024\u201325 Serie A 2024\u201325 La Liga 2024\u201325 Bundesliga 2024\u201325 Ligue 1 2025 Major League Soccer season 2025 National Women's Soccer League season Australian rules football 2025 AFL season Baseball 2025 MLB season Basketball 2025 NBA playoffs 2024\u201325 EuroLeague 2025 WNBA season International cricket 2023\u201325 ICC World Test Championship 2025 IPL season Cycling 2025 UCI World Tour Golf 2025 PGA Tour 2025 PGA Championship 2025 LIV Golf League 2025 LPGA Tour Ice hockey 2025 Stanley Cup playoffs 2025 PWHL playoffs Motorsport 2025 Formula One World Championship 2025 MotoGP World Championship 2025 NASCAR Cup Series 2025 IndyCar Series Rugby league 2025 NRL season 2025 Super League season Rugby union 2024\u201325 European Rugby Champions Cup 2024\u201325 Premiership Rugby 2024\u201325 United Rugby Championship 2025 Super Rugby Pacific season Tennis 2025 ATP Tour 2025 WTA Tour More details \u2013 current sports events edit section Recent deaths May 0 / 0 April 15: Charles Strouse 13: Kit Bond 13: Jos\u00e9 Mujica 11: Robert Benton 11: Sabu 0 9: Greg Cannom 0 9: Margot Friedl\u00e4nder 0 9: Johnny Rodriguez 0 8: David Souter 0 8: Chet Lemon 0 7: Joe Don Baker 0 7: Cleopa Msuya 0 6: James Foley 0 6: Joseph Nye 0 5: James Baker 0 4: Jochen Mass 0 3: S\u0131rr\u0131 S\u00fcreyya \u00d6nder 0 2: George Ryan 0 1: Ruth Buzzi 0 1: Jill Sobule 29: David Horowitz 29: Mike Peters 28: Priscilla Pointer 28: Shaji N. Karun 27: Dick Barnett 27: Cora Sue Collins 27: Jiggly Caliente 27: Stan Love 25: Virginia Giuffre 25: Alexis Herman 25: Walt Jocketty 25: Philip Lowrie 25: Yaroslav Moskalik 24: Rob Holland 23: Steve McMichael 23: David Thomas 22: Lar Park Lincoln 22: Zurab Tsereteli 22: Puan Noor Aishah 22: Keith Stackpole 21: Pope Francis 21: Will Hutchins 21: Hajji Alejandro 20: Mike Patrick 19: Jay Sigel 19: Guy Ullens 18: Clodagh Rodgers 17: Joe Thompson edit section Ongoing conflicts Global War against the Islamic State Africa Cameroon Anglophone Crisis Central African Republic Civil war Democratic Republic of the Congo and Rwanda Kivu conflict Sahel insurgency Mali War Chad Basin Burkina Faso Niger Somalia Civil war Sudan Civil war Americas Haiti Gang war Mexico Mexican drug war Asia\u2212Pacific Afghanistan Islamic State\u2013Taliban conflict India Naxalite\u2013Maoist insurgency India and Pakistan Kashmir conflict Myanmar Civil war Pakistan Insurgency in Balochistan Insurgency in Khyber Pakhtunkhwa Philippines NPA rebellion Europe Russia and Ukraine Russian invasion of Ukraine Middle East Israel Gaza war Syria Israeli invasion of Syria Yemen Red Sea crisis See also \u2013 List of ongoing proxy wars edit section 2025 events and developments by topic Arts Animation ( Anime ) Architecture Comics Film ( Horror , Science fiction ) Literature ( Poetry ) Music ( Classical , Country , Hip hop , Jazz , Latin , Metal , Rock , UK , US , Korea ) Radio Television ( Australia , Canada , Ireland , UK , Scotland , US ) Video games Politics and government Elections International leaders Sovereign states Sovereign state leaders Territorial governors Science and technology Archaeology Biotechnology Computing Palaeontology Quantum computing and communication Senescence research Space/Astronomy Spaceflight Sustainable energy research Environment and environmental sciences Climate change Weather ( Heat waves Tornadoes Wildfires ) Transportation Aviation Rail transport Transportation technology By place Afghanistan Albania Algeria Andorra Angola Antarctica Antigua and Barbuda Argentina Armenia Australia Austria Azerbaijan Bangladesh The Bahamas Bahrain Barbados Belarus Belgium Belize Benin Bhutan Bolivia Bosnia and Herzegovina Botswana Brazil Brunei Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Central African Republic Chad Chile China Colombia Costa Rica Comoros Congo DR Congo Croatia Cuba Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Eritrea Estonia Ethiopia Eswatini Equatorial Guinea Fiji Finland France Gabon The Gambia Georgia Germany Ghana Greece Grenada Guatemala Guinea Guinea-Bissau Guyana Haiti Honduras Hong Kong Hungary Iceland India Indonesia Iran Iraq Ireland Israel Italy Ivory Coast Jamaica Japan Jordan Kazakhstan Kenya Kiribati Kosovo Kuwait Kyrgyzstan Laos Latvia Lebanon Lesotho Liberia Liechtenstein Libya Lithuania Luxembourg Macau Madagascar Marshall Islands Malawi Malaysia Maldives Mali Malta Mauritania Mauritius Mexico Micronesia Moldova Monaco Mongolia Montenegro Morocco Mozambique Myanmar Nauru Namibia Nepal Netherlands New Zealand Nicaragua Niger Nigeria North Korea North Macedonia Norway Oman Pakistan Palau Palestine Panama Papua New Guinea Paraguay Peru Philippines Poland Portugal Qatar Romania Russia Rwanda Saint Kitts and Nevis Saint Lucia Saint Vincent and the Grenadines Samoa San Marino S\u00e3o Tom\u00e9 and Pr\u00edncipe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Somalia South Africa Solomon Islands South Korea South Sudan Spain Sri Lanka Sudan Suriname Sweden Switzerland Syria Taiwan Tajikistan Tanzania Thailand Timor-Leste Togo Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States Uruguay Uzbekistan Vanuatu Vatican City Venezuela Vietnam Yemen Zambia Zimbabwe Establishments and disestablishments categories Establishments Disestablishments Works and introductions categories Works Introductions Works entering the public domain 2025 at Wikipedia's sister projects Media from Commons News from Wikinews Quotations from Wikiquote Texts from Wikisource Taxa from Wikispecies Discussions from Meta-Wiki ...more edit section v t e Current events by month 2025 January February March April May 2024 January February March April May June July August September October November December 2023 January February March April May June July August September October November December 2022 January February March April May June July August September October November December 2021 January February March April May June July August September October November December 2020 January February March April May June July August September October November December 2019 January February March April May June July August September October November December 2018 January February March April May June July August September October November December 2017 January February March April May June July August September October November December 2016 January February March April May June July August September October November December 2015 January February March April May June July August September October November December 2014 January February March April May June July August September October November December 2013 January February March April May June July August September October November December 2012 January February March April May June July August September October November December 2011 January February March April May June July August September October November December 2010 January February March April May June July August September October November December 2009 January February March April May June July August September October November December 2008 January February March April May June July August September October November December 2007 January February March April May June July August September October November December 2006 January February March April May June July August September October November December 2005 January February March April May June July August September October November December 2004 January February March April May June July August September October November December 2003 January February March April May June July August September October November December 2002 January February March April May June July August September October November December Portal:Current events Calendars Discover Wikipedia using portals List of all portals The arts portal Biography portal Current events portal Geography portal History portal Mathematics portal Science portal Society portal Technology portal Random portal WikiProject Portals Retrieved from \" https://en.wikipedia.org/w/index.php?title=Portal:Current_events&oldid=1234748158 \" Categories : All portals 2025 by day Current events portal 2025 Current events WikiProject Current events History portals Hidden categories: Wikipedia pages protected against vandalism Portals with triaged subpages from October 2020 All portals with triaged subpages Portals with no named maintainer This page was last edited on 15 July 2024, at 23:17 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Portal : Current events 118 languages Add topic"}, "5": {"url": "https://en.wikipedia.org/wiki/Special:Random", "text": "L\u1ee5c S\u01a1n - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 References Toggle the table of contents L\u1ee5c S\u01a1n 4 languages Catal\u00e0 Cebuano Nederlands Ti\u1ebfng Vi\u1ec7t Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide Coordinates : 21\u00b013\u203242\u2033N 106\u00b036\u203252\u2033E \ufeff / \ufeff 21.22833\u00b0N 106.61444\u00b0E \ufeff / 21.22833; 106.61444 From Wikipedia, the free encyclopedia Commune and village in B\u1eafc Giang Province, Vietnam L\u1ee5c S\u01a1n Commune and village Country Vietnam Province B\u1eafc Giang Province District L\u1ee5c Nam Time zone UTC+07:00 L\u1ee5c S\u01a1n is a commune ( x\u00e3 ) and village in L\u1ee5c Nam District , B\u1eafc Giang Province , in northeastern Vietnam . [ 1 ] References [ edit ] ^ Ministry of Public Information in Vietnam Archived 2012-08-20 at the Wayback Machine v t e B\u1eafc Giang province Capital: B\u1eafc Giang B\u1eafc Giang \u0110a Mai D\u0129nh K\u1ebf Ho\u00e0ng V\u0103n Th\u1ee5 L\u00ea L\u1ee3i M\u1ef9 \u0110\u1ed9 Ng\u00f4 Quy\u1ec1n Th\u1ecd X\u01b0\u01a1ng Tr\u1ea7n Nguy\u00ean H\u00e3n Tr\u1ea7n Ph\u00fa X\u01b0\u01a1ng Giang D\u0129nh Tr\u00ec \u0110\u1ed3ng S\u01a1n Song Kh\u00ea Song Mai T\u00e2n M\u1ef9 T\u00e2n Ti\u1ebfn Ch\u0169 Ch\u0169 H\u1ed3ng Giang Ph\u01b0\u1ee3ng S\u01a1n Thanh H\u1ea3i Tr\u00f9 H\u1ef1u Ki\u00ean Lao Ki\u00ean Th\u00e0nh M\u1ef9 An Nam D\u01b0\u01a1ng Qu\u00fd S\u01a1n Vi\u1ec7t Y\u00ean B\u00edch \u0110\u1ed9ng H\u1ed3ng Th\u00e1i N\u1ebfnh Ninh S\u01a1n Quang Ch\u00e2u Qu\u1ea3ng Minh T\u0103ng Ti\u1ebfn T\u1ef1 L\u1ea1n V\u00e2n Trung H\u01b0\u01a1ng Mai Minh \u0110\u1ee9c Ngh\u0129a Trung Th\u01b0\u1ee3ng Lan Ti\u00ean S\u01a1n Trung S\u01a1n V\u00e2n H\u00e0 Vi\u1ec7t Ti\u1ebfn Hi\u1ec7p H\u00f2a District Th\u1eafng B\u1eafc L\u00fd Ch\u00e2u Minh \u0110\u1ea1i Th\u00e0nh Danh Th\u1eafng \u0110oan B\u00e1i \u0110\u00f4ng L\u1ed7 \u0110\u1ed3ng T\u00e2n H\u00f2a S\u01a1n Ho\u00e0ng An Ho\u00e0ng L\u01b0\u01a1ng Ho\u00e0ng Thanh Ho\u00e0ng V\u00e2n H\u1ee3p Th\u1ecbnh H\u00f9ng S\u01a1n H\u01b0\u01a1ng L\u00e2m L\u01b0\u01a1ng Phong Mai \u0110\u00ecnh Mai Trung Ng\u1ecdc S\u01a1n Quang Minh Th\u00e1i S\u01a1n Thanh V\u00e2n Th\u01b0\u1eddng Th\u1eafng Xu\u00e2n C\u1ea9m L\u1ea1ng Giang District V\u00f4i K\u00e9p Th\u00e1i \u0110\u00e0o \u0110\u1ea1i L\u00e2m T\u00e2n D\u0129nh X\u01b0\u01a1ng L\u00e2m T\u00e2n H\u01b0ng H\u01b0\u01a1ng S\u01a1n Xu\u00e2n H\u01b0\u01a1ng M\u1ef9 Th\u00e1i T\u00e2n Thanh M\u1ef9 H\u00e0 Ti\u00ean L\u1ee5c \u0110\u00e0o M\u1ef9 An H\u00e0 H\u01b0\u01a1ng L\u1ea1c Ngh\u0129a H\u01b0ng Ngh\u0129a H\u00f2a Quang Th\u1ecbnh D\u01b0\u01a1ng \u0110\u1ee9c Y\u00ean M\u1ef9 L\u1ee5c Nam District \u0110\u1ed3i Ng\u00f4 L\u1ee5c S\u01a1n B\u00ecnh S\u01a1n Tr\u01b0\u1eddng S\u01a1n V\u00f4 Tranh Tr\u01b0\u1eddng Giang Ngh\u0129a Ph\u01b0\u01a1ng C\u01b0\u01a1ng S\u01a1n Huy\u1ec1n S\u01a1n B\u1eafc L\u0169ng C\u1ea9m L\u00fd V\u0169 X\u00e1 \u0110an H\u1ed9i Y\u00ean S\u01a1n Lan M\u1eabu Ph\u01b0\u01a1ng S\u01a1n Thanh L\u00e2m Chu \u0110i\u1ec7n B\u1ea3o \u0110\u00e0i B\u1ea3o S\u01a1n Tam D\u1ecb \u0110\u00f4ng Ph\u00fa \u0110\u00f4ng H\u01b0ng Ti\u00ean Nha Kh\u00e1m L\u1ea1ng L\u1ee5c Ng\u1ea1n District Ph\u1ec9 \u0110i\u1ec1n Bi\u1ec3n \u0110\u1ed9ng Bi\u00ean S\u01a1n C\u1ea5m S\u01a1n \u0110\u00e8o Gia \u0110\u1ed3ng C\u1ed1c Gi\u00e1p S\u01a1n H\u1ed9 \u0110\u00e1p Kim S\u01a1n Phong Minh Phong V\u00e2n Ph\u00fa Nhu\u1eadn Sa L\u00fd S\u01a1n H\u1ea3i T\u00e2n Hoa T\u00e2n L\u1eadp T\u00e2n M\u1ed9c T\u00e2n Quang T\u00e2n S\u01a1n S\u01a1n \u0110\u1ed9ng District An Ch\u00e2u T\u00e2y Y\u00ean T\u1eed H\u1eefu S\u1ea3n An L\u1ea1c V\u00e2n S\u01a1n L\u1ec7 Vi\u1ec5n V\u0129nh An D\u01b0\u01a1ng H\u01b0u Long S\u01a1n An B\u00e1 Y\u00ean \u0110\u1ecbnh Tu\u1ea5n \u0110\u1ea1o Thanh Lu\u1eadn C\u1ea9m \u0110\u00e0n Gi\u00e1o Li\u00eam \u0110\u1ea1i S\u01a1n Ph\u00fac S\u01a1n T\u00e2n Y\u00ean District Cao Th\u01b0\u1ee3ng Nh\u00e3 Nam Qu\u1ebf Nham Vi\u1ec7t L\u1eadp Li\u00ean Chung Cao X\u00e1 Ng\u1ecdc L\u00fd Ng\u1ecdc Thi\u1ec7n Ng\u1ecdc Ch\u00e2u Ng\u1ecdc V\u00e2n H\u1ee3p \u0110\u1ee9c Ph\u00fac H\u00f2a T\u00e2n Trung An D\u01b0\u01a1ng Lan Gi\u1edbi \u0110\u1ea1i H\u00f3a Quang Ti\u1ebfn Ph\u00fac S\u01a1n Lam C\u1ed1t Vi\u1ec7t Ng\u1ecdc Song V\u00e2n Li\u00ean S\u01a1n Y\u00ean D\u0169ng District Nham Bi\u1ec1n T\u00e2n An \u0110\u1ed3ng Ph\u00fac \u0110\u1ed3ng Vi\u1ec7t T\u01b0 M\u1ea1i \u0110\u1ee9c Giang Ti\u1ebfn D\u0169ng C\u1ea3nh Th\u1ee5y L\u00e3ng S\u01a1n Tr\u00ed Y\u00ean L\u00e3o H\u1ed9 Xu\u00e2n Ph\u00fa T\u00e2n Li\u1ec5u Ti\u1ec1n Phong Y\u00ean L\u01b0 H\u01b0\u01a1ng Gi\u00e1n Qu\u1ef3nh S\u01a1n N\u1ed9i Ho\u00e0ng Y\u00ean Th\u1ebf District Ph\u1ed3n X\u01b0\u01a1ng B\u1ed1 H\u1ea1 \u0110\u1ed3ng V\u01b0\u01a1ng Canh N\u1eadu \u0110\u1ed3ng K\u1ef3 H\u01b0\u01a1ng V\u0129 \u0110\u00f4ng S\u01a1n Xu\u00e2n L\u01b0\u01a1ng Tam Ti\u1ebfn Ti\u1ebfn Th\u1eafng T\u00e2n Hi\u1ec7p Tam Hi\u1ec7p An Th\u01b0\u1ee3ng \u0110\u1ed3ng L\u1ea1c H\u1ed3ng K\u1ef3 \u0110\u1ed3ng H\u01b0u T\u00e2n S\u1ecfi \u0110\u1ed3ng Ti\u1ebfn \u0110\u1ed3ng T\u00e2m 21\u00b013\u203242\u2033N 106\u00b036\u203252\u2033E \ufeff / \ufeff 21.22833\u00b0N 106.61444\u00b0E \ufeff / 21.22833; 106.61444 This article about a location in B\u1eafc Giang province , Vietnam is a stub . You can help Wikipedia by expanding it . v t e Retrieved from \" https://en.wikipedia.org/w/index.php?title=L\u1ee5c_S\u01a1n&oldid=1063532109 \" Categories : Populated places in B\u1eafc Giang province Communes of B\u1eafc Giang province B\u1eafc Giang province geography stubs Hidden categories: Webarchive template wayback links Pages using gadget WikiMiniAtlas Articles with short description Short description is different from Wikidata Infobox mapframe without OSM relation ID on Wikidata Pages using infobox settlement with no map Pages using infobox settlement with no coordinates Coordinates on Wikidata All stub articles Pages using the Kartographer extension This page was last edited on 3 January 2022, at 15:27 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents L\u1ee5c S\u01a1n 4 languages Add topic"}, "6": {"url": "https://en.wikipedia.org/wiki/Wikipedia:About", "text": "Wikipedia:About - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Wikipedia : About 138 languages Afrikaans Alemannisch \u0410\u043b\u0442\u0430\u0439 \u0442\u0438\u043b \u0905\u0902\u0917\u093f\u0915\u093e \u0410\u0525\u0441\u0448\u04d9\u0430 \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Aragon\u00e9s \u0985\u09b8\u09ae\u09c0\u09af\u09bc\u09be Asturianu Ava\u00f1e'\u1ebd Az\u0259rbaycanca \u062a\u06c6\u0631\u06a9\u062c\u0647 \u09ac\u09be\u0982\u09b2\u09be Banjar \u0411\u0430\u0448\u04a1\u043e\u0440\u0442\u0441\u0430 \u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f \u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f (\u0442\u0430\u0440\u0430\u0448\u043a\u0435\u0432\u0456\u0446\u0430) \u092d\u094b\u091c\u092a\u0941\u0930\u0940 Bikol Central \u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438 Boarisch \u0f56\u0f7c\u0f51\u0f0b\u0f61\u0f72\u0f42 Catal\u00e0 \u0427\u04d1\u0432\u0430\u0448\u043b\u0430 Cebuano Chi-Chewa Cymraeg Dansk \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Davvis\u00e1megiella Deutsch \u078b\u07a8\u0788\u07ac\u0780\u07a8\u0784\u07a6\u0790\u07b0 Din\u00e9 bizaad \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc F\u00f8royskt Fran\u00e7ais Frysk Galego \u0413\u04c0\u0430\u043b\u0433\u04c0\u0430\u0439 \u8d1b\u8a9e \u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0 \ud800\udf32\ud800\udf3f\ud800\udf44\ud800\udf39\ud800\udf43\ud800\udf3a \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 \u0939\u093f\u0928\u094d\u0926\u0940 Bahasa Hulontalo Ilokano Bahasa Indonesia Interlingua \u0418\u0440\u043e\u043d \u00cdslenska Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Jawa \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 Kapampangan \u0915\u0949\u0936\u0941\u0930 / \u06a9\u0672\u0634\u064f\u0631 \u049a\u0430\u0437\u0430\u049b\u0448\u0430 Kiswahili Kurd\u00ee \u041a\u044b\u0440\u0433\u044b\u0437\u0447\u0430 Latvie\u0161u L\u00ebtzebuergesch Lietuvi\u0173 Magyar \u041c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438 Malti M\u0101ori \u10db\u10d0\u10e0\u10d2\u10d0\u10da\u10e3\u10e0\u10d8 \u0645\u0635\u0631\u0649 \u0645\u0627\u0632\u0650\u0631\u0648\u0646\u06cc Bahasa Melayu \u041c\u043e\u043d\u0433\u043e\u043b \u1019\u103c\u1014\u103a\u1019\u102c\u1018\u102c\u101e\u102c Nederlands \u0928\u0947\u092a\u093e\u0932\u0940 \u65e5\u672c\u8a9e \u041d\u043e\u0445\u0447\u0438\u0439\u043d Nordfriisk Norsk bokm\u00e5l Norsk nynorsk \ua187\ua259 \u0b13\u0b21\u0b3c\u0b3f\u0b06 O\u02bbzbekcha / \u045e\u0437\u0431\u0435\u043a\u0447\u0430 \u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40 \u1015\u1021\u102d\u102f\u101d\u103a\u108f\u1018\u102c\u108f\u101e\u102c\u108f \u1797\u17b6\u179f\u17b6\u1781\u17d2\u1798\u17c2\u179a Polski Portugu\u00eas Qaraqalpaqsha Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 \u0421\u0430\u0445\u0430 \u0442\u044b\u043b\u0430 S\u00e4ng\u00f6 Scots Shqip \u0dc3\u0dd2\u0d82\u0dc4\u0dbd Simple English \u0633\u0646\u068c\u064a Sloven\u010dina Sloven\u0161\u010dina \u0421\u043b\u043e\u0432\u0463\u043d\u044c\u0441\u043a\u044a / \u2c14\u2c0e\u2c11\u2c02\u2c21\u2c10\u2c20\u2c14\u2c0d\u2c1f Soomaaliga \u06a9\u0648\u0631\u062f\u06cc \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Srpskohrvatski / \u0441\u0440\u043f\u0441\u043a\u043e\u0445\u0440\u0432\u0430\u0442\u0441\u043a\u0438 Sunda Suomi Svenska Tagalog \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0422\u0430\u0442\u0430\u0440\u0447\u0430 / tatar\u00e7a \u1010\u1086\u1038 \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41 \u0e44\u0e17\u0e22 \u0422\u043e\u04b7\u0438\u043a\u04e3 T\u00fcrk\u00e7e T\u00fcrkmen\u00e7e Tyap \u0423\u0434\u043c\u0443\u0440\u0442 \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u0627\u0631\u062f\u0648 \u0626\u06c7\u064a\u063a\u06c7\u0631\u0686\u06d5 / Uyghurche Ti\u1ebfng Vi\u1ec7t Walon Winaray Wolof \u5434\u8bed \u05d9\u05d9\u05b4\u05d3\u05d9\u05e9 Yor\u00f9b\u00e1 \u7cb5\u8a9e Zazaki \u4e2d\u6587 \ua80d\ua824\ua81f\ua810\ua824 Edit links Project page Talk English Read View source View history Tools Tools move to sidebar hide Actions Read View source View history General What links here Related changes Upload file Permanent link Page information Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikimedia Foundation MediaWiki Meta-Wiki Multilingual Wikisource Wikispecies Wikidata Wikifunctions Wikimania Wikinews Wikiquote Wikisource Wikivoyage Wiktionary Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Introduction to English language Wikipedia Shortcuts WP:ABT WP:ABOUT WP:WIKIPEDIA This is a general introduction for visitors to Wikipedia. For aspiring contributors, also see this guide and tutorial . For other uses, see Wikipedia:Wikipedia (disambiguation) . \"Imagine a world in which every single person on the planet is given free access to the sum of all human knowledge. That's what we're doing.\" \u2014 Jimmy Wales Wikipedia is a free online encyclopedia that anyone can edit, and millions already have . Wikipedia's purpose is to benefit readers by presenting information on all branches of knowledge . Hosted by the Wikimedia Foundation , Wikipedia consists of freely editable content, with articles that usually contain numerous links guiding readers to more information. Written collaboratively by volunteers known as Wikipedians , Wikipedia articles can be edited by anyone with Internet access , except in limited cases in which editing is restricted to prevent disruption or vandalism . Since its creation on January 15, 2001 , it has grown into the world's largest reference website , attracting over a billion visitors each month . Wikipedia currently has more than sixty-four million articles in more than 300 languages , including 6,994,824 articles in English , with 118,797 active contributors in the past month. Wikipedia's fundamental principles are summarized in its five pillars . While the Wikipedia community has developed many policies and guidelines , new editors do not need to be familiar with them before they start contributing. Anyone can edit Wikipedia's text, data, references, and images. The quality of content is more important than the expertise of who contributes it. Wikipedia's content must conform with its policies , including being verifiable by published reliable sources . Contributions based on personal opinions , beliefs, or personal experiences, unreviewed research , libellous material, and copyright violations are not allowed, and will not remain. Wikipedia's software makes it easy to reverse errors, and experienced editors watch and patrol bad edits. Wikipedia differs from printed references in important ways . Anyone can instantly improve it, add quality information, remove misinformation, and fix errors and vandalism . Since Wikipedia is continually updated, encyclopedic articles on major news events appear within minutes. For over 24 years, editors have volunteered their time and talents to create history's most comprehensive encyclopedia while providing references and other resources to researchers worldwide (see Researching with Wikipedia ). In summary, Wikipedia has tested the wisdom of the crowd since 2001 and has found that it succeeds. To start editing simply click the Edit or Edit\u00a0source button, or the pencil icon , at the top of any non-protected Wikipedia page or section. v t e Basic information on Wikipedia - ( Search ) Help directory menu Teahouse (interactive help for new editors) About Wikipedia Readers' index to Wikipedia Statistics Administration FAQs Purpose Who writes Wikipedia? Organization Censorship In brief General disclaimer Readers' FAQ Student help Navigation Searching Viewing media Help Mobile access Parental advice Other languages Researching with Wikipedia Citing Wikipedia Copyright Introductions to contributing Main introduction List of tutorials and introductions The answer Dos and don'ts Learning the ropes Common mistakes Newcomer primer Simplified ruleset The \"Missing Manual\" Your first article Wizard Young Wikipedians The Wikipedia Adventure Accounts Why create an account? Logging in Email confirmation Editing Toolbar Conflict VisualEditor User guide Pillars, policies and guidelines Five pillars Manual of Style Simplified Etiquette Expectations Oversight Principles Ignore all rules The rules are principles Core content policies Policies and guidelines Vandalism Appealing blocks What Wikipedia is not Getting help Help menu Help desk Reference Desk Category Requests for help Disputes resolution requests IRC live chat Tutorial Contact us Wikipedia community Departments Meetups WikiProjects Overview FAQ Village pump policy technical proposals idea lab wikimedia misc Newsletters : Signpost Library Newsletter GLAM Newsletter Informational: Community portal Dashboard Noticeboards Maintenance Task Center Essays Open letters Sourcing and referencing Research Wikipedia Library Request Resources Free resources Request access to major databases WikiProject Resource Exchange Shared resources Guidelines Finding sources Combining sources Referencing Citations Citation Style 1 Citation templates Footnotes Page numbers Cite errors How-to guides Category Tools Infoboxes Writing articles Development Better writing Files Images Media files Uploading Deletion User contribution pages Editing pages Diffs Namespaces Deletion Merging Renaming Requests Redirecting Reverting Vandalism cleanup Talk pages Archiving Wiki markup Wiki markup Cheatsheet Columns HTML Lists Magic words For beginners Sections Sounds Special Characters Tables Templates Documentation Index Substitution Transclusion Image and video markup Tutorial Linking Directories and glossaries Category Abbreviations Contents Edit summaries Essays Glossary Index Shortcuts Tips Retrieved from \" https://en.wikipedia.org/w/index.php?title=Wikipedia:About&oldid=1289828186 \" Category : Wikipedia basic information Hidden categories: Wikipedia semi-protected project pages Wikipedia move-protected project pages This page was last edited on 11 May 2025, at 03:21 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Wikipedia : About 138 languages Add topic"}, "7": {"url": "https://en.wikipedia.org/wiki/Wikipedia:Contact_us", "text": "Wikipedia:Contact us - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Wikipedia : Contact us 79 languages Alemannisch \u0905\u0902\u0917\u093f\u0915\u093e \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca \u062a\u06c6\u0631\u06a9\u062c\u0647 \u09ac\u09be\u0982\u09b2\u09be Banjar \u95a9\u5357\u8a9e / B\u00e2n-l\u00e2m-g\u00fa Basa Banyumasan \u092d\u094b\u091c\u092a\u0941\u0930\u0940 \u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438 Catal\u00e0 \u010ce\u0161tina Cymraeg Dansk \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Davvis\u00e1megiella Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Esperanto \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0939\u093f\u0928\u094d\u0926\u0940 Hrvatski Bahasa Indonesia Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Jawa \u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8 \u041a\u044b\u0440\u0433\u044b\u0437\u0447\u0430 Latvie\u0161u L\u00ebtzebuergesch Magyar Malti Bahasa Melayu \u1019\u103c\u1014\u103a\u1019\u102c\u1018\u102c\u101e\u102c Nederlands \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk Occitan \u0b13\u0b21\u0b3c\u0b3f\u0b06 O\u02bbzbekcha / \u045e\u0437\u0431\u0435\u043a\u0447\u0430 \u067e\u069a\u062a\u0648 Plattd\u00fc\u00fctsch Polski Portugu\u00eas Rom\u00e2n\u0103 \u0420\u0443\u0441\u0438\u043d\u044c\u0441\u043a\u044b\u0439 \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Scots Shqip \u0dc3\u0dd2\u0d82\u0dc4\u0dbd Simple English \u0633\u0646\u068c\u064a Sloven\u010dina Sloven\u0161\u010dina \u06a9\u0648\u0631\u062f\u06cc \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Srpskohrvatski / \u0441\u0440\u043f\u0441\u043a\u043e\u0445\u0440\u0432\u0430\u0442\u0441\u043a\u0438 Sunda Suomi Svenska Tagalog \u0422\u0430\u0442\u0430\u0440\u0447\u0430 / tatar\u00e7a \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41 \u0e44\u0e17\u0e22 \u0422\u043e\u04b7\u0438\u043a\u04e3 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u0627\u0631\u062f\u0648 V\u00e8neto Ti\u1ebfng Vi\u1ec7t \u05d9\u05d9\u05b4\u05d3\u05d9\u05e9 \u7cb5\u8a9e \u4e2d\u6587 Edit links Project page Talk English Read View source View history Tools Tools move to sidebar hide Actions Read View source View history General What links here Related changes Upload file Permanent link Page information Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Meta-Wiki Wikibooks Wikinews Wikiquote Wikiversity Wikivoyage Wiktionary Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Introduction Readers How to report a problem with an article, or find out more information. Article subjects Problems with articles about you, your company, or somebody you represent. Licensing How to copy Wikipedia's information, donate your own, or report unlicensed use of your information. Donors Find out about the process, how to donate, and information about how your money is spent. Press and partnerships If you're a member of the press looking to contact Wikipedia, or have a business proposal for us. Back to main page Thank you for your interest in contacting Wikipedia. Before proceeding, some important disclaimers: Wikipedia has no central editorial board. Contributions are made by a large number of volunteers at their own discretion. Edits are neither the responsibility of the Wikimedia Foundation (the organisation that hosts the site) nor of its staff and edits will not generally be made in response to an email request. Although Wikipedia was founded by Jimmy Wales , he is not personally responsible for our content. If you have questions about the concept of Wikipedia rather than a specific problem, the About Wikipedia page may help. If you want to ask other users for help with editing or using Wikipedia, stop by the Teahouse , Wikipedia's live help channel , or the help desk to ask someone for assistance. If you disagree with an article's content, or are involved in a content dispute, see Dispute resolution . The links on the left should direct you to how to contact us or resolve problems. If you cannot find your issue listed there, you can email helpful, experienced volunteers at info-en wikimedia.org . Please refrain from emailing about disagreements with content; they will not be resolved via email. Retrieved from \" https://en.wikipedia.org/w/index.php?title=Wikipedia:Contact_us&oldid=967537943 \" Categories : Wikipedia tutorials Wikipedia quick introductions This page was last edited on 13 July 2020, at 20:45 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Wikipedia : Contact us 79 languages Add topic"}, "8": {"url": "https://en.wikipedia.org/wiki/Help:Contents", "text": "Help:Contents - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Help : Contents 188 languages Ac\u00e8h Afrikaans Alemannisch \u12a0\u121b\u122d\u129b \u0905\u0902\u0917\u093f\u0915\u093e \u00c6nglisc \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Aragon\u00e9s \u0710\u072a\u0721\u071d\u0710 Asturianu Ava\u00f1e'\u1ebd \u0410\u0432\u0430\u0440 Aymar aru Az\u0259rbaycanca \u062a\u06c6\u0631\u06a9\u062c\u0647 \u09ac\u09be\u0982\u09b2\u09be Banjar \u95a9\u5357\u8a9e / B\u00e2n-l\u00e2m-g\u00fa Basa Banyumasan \u0411\u0430\u0448\u04a1\u043e\u0440\u0442\u0441\u0430 \u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f \u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f (\u0442\u0430\u0440\u0430\u0448\u043a\u0435\u0432\u0456\u0446\u0430) \u092d\u094b\u091c\u092a\u0941\u0930\u0940 Bislama Boarisch \u0f56\u0f7c\u0f51\u0f0b\u0f61\u0f72\u0f42 Bosanski Brezhoneg Catal\u00e0 \u0427\u04d1\u0432\u0430\u0448\u043b\u0430 Cebuano \u010ce\u0161tina Chavacano de Zamboanga Cymraeg Dansk \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Davvis\u00e1megiella Deutsch Din\u00e9 bizaad Dolnoserbski \u0921\u094b\u091f\u0947\u0932\u0940 \u0f47\u0f7c\u0f44\u0f0b\u0f41 Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Esperanto Euskara \u0641\u0627\u0631\u0633\u06cc F\u00f8royskt Fran\u00e7ais Frysk Gaeilge G\u00e0idhlig Galego \u0413\u04c0\u0430\u043b\u0433\u04c0\u0430\u0439 \u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0 \u5ba2\u5bb6\u8a9e / Hak-k\u00e2-ng\u00ee \ud55c\uad6d\uc5b4 Hausa \u0540\u0561\u0575\u0565\u0580\u0565\u0576 \u0939\u093f\u0928\u094d\u0926\u0940 Hornjoserbsce Hrvatski Bahasa Hulontalo Ido Ilokano Bahasa Indonesia Interlingua Interlingue \u0418\u0440\u043e\u043d \u00cdslenska Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Jawa Kalaallisut \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8 \u0915\u0949\u0936\u0941\u0930 / \u06a9\u0672\u0634\u064f\u0631 \u049a\u0430\u0437\u0430\u049b\u0448\u0430 Kernowek Ikirundi Kiswahili Krey\u00f2l ayisyen Kurd\u00ee \u041a\u044b\u0440\u0433\u044b\u0437\u0447\u0430 Ladin Ladino \u0ea5\u0eb2\u0ea7 Latina Latvie\u0161u L\u00ebtzebuergesch Lietuvi\u0173 Li Niha Limburgs Ling\u00e1la Lombard Magyar \u041c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438 \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02 Malti M\u0101ori \u092e\u0930\u093e\u0920\u0940 \u0645\u0635\u0631\u0649 Bahasa Melayu Minangkabau \u95a9\u6771\u8a9e / M\u00ecng-d\u0115\u0324ng-ng\u1e73\u0304 Mirand\u00e9s \u041c\u043e\u043d\u0433\u043e\u043b \u1019\u103c\u1014\u103a\u1019\u102c\u1018\u102c\u101e\u102c Nederlands Nedersaksies \u0928\u0947\u092a\u093e\u0932\u0940 \u0928\u0947\u092a\u093e\u0932 \u092d\u093e\u0937\u093e \u65e5\u672c\u8a9e Napulitano \u041d\u043e\u0445\u0447\u0438\u0439\u043d Norsk bokm\u00e5l Norsk nynorsk Occitan \u0b13\u0b21\u0b3c\u0b3f\u0b06 O\u02bbzbekcha / \u045e\u0437\u0431\u0435\u043a\u0447\u0430 \u1015\u1021\u102d\u102f\u101d\u103a\u108f\u1018\u102c\u108f\u101e\u102c\u108f Papiamentu \u067e\u069a\u062a\u0648 \u1797\u17b6\u179f\u17b6\u1781\u17d2\u1798\u17c2\u179a Piemont\u00e8is Plattd\u00fc\u00fctsch Polski Portugu\u00eas Q\u0131r\u0131mtatarca Ripoarisch Rom\u00e2n\u0103 Rumantsch Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 \u0421\u0430\u0445\u0430 \u0442\u044b\u043b\u0430 \u0938\u0902\u0938\u094d\u0915\u0943\u0924\u092e\u094d Sardu Scots Seeltersk Shqip Sicilianu \u0dc3\u0dd2\u0d82\u0dc4\u0dbd Simple English \u0633\u0646\u068c\u064a Sloven\u010dina Sloven\u0161\u010dina \u0421\u043b\u043e\u0432\u0463\u043d\u044c\u0441\u043a\u044a / \u2c14\u2c0e\u2c11\u2c02\u2c21\u2c10\u2c20\u2c14\u2c0d\u2c1f \u015al\u016fnski Soomaaliga \u06a9\u0648\u0631\u062f\u06cc \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Srpskohrvatski / \u0441\u0440\u043f\u0441\u043a\u043e\u0445\u0440\u0432\u0430\u0442\u0441\u043a\u0438 Sunda Suomi Svenska Tagalog \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0422\u0430\u0442\u0430\u0440\u0447\u0430 / tatar\u00e7a \u1010\u1086\u1038 \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41 \u0e44\u0e17\u0e22 \u0422\u043e\u04b7\u0438\u043a\u04e3 Lea faka-Tonga Tsets\u00eahest\u00e2hese \u0ca4\u0cc1\u0cb3\u0cc1 T\u00fcrk\u00e7e T\u00fcrkmen\u00e7e Twi \u0422\u044b\u0432\u0430 \u0434\u044b\u043b \u0423\u0434\u043c\u0443\u0440\u0442 Basa Ugi \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u0627\u0631\u062f\u0648 \u0626\u06c7\u064a\u063a\u06c7\u0631\u0686\u06d5 / Uyghurche V\u00e8neto Veps\u00e4n kel\u2019 Ti\u1ebfng Vi\u1ec7t Volap\u00fck V\u00f5ro Walon West-Vlams \u5434\u8bed \u05d9\u05d9\u05b4\u05d3\u05d9\u05e9 \u7cb5\u8a9e Zazaki \u017demait\u0117\u0161ka \u4e2d\u6587 Edit links Help page Talk English Read View source View history Tools Tools move to sidebar hide Actions Read View source View history General What links here Related changes Upload file Permanent link Page information Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons MediaWiki Meta-Wiki Multilingual Wikisource Wikispecies Wikibooks Wikidata Wikifunctions Wikinews Wikiquote Wikisource Wikiversity Wikivoyage Wiktionary Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Main page of the help namespace This page provides help with the most common questions about Wikipedia . You can also search Wikipedia's help pages using the search box below, or browse the Help menu or the Help directory . Help Wikipedia Read or find an article The Readers' FAQ and our about page contain the most commonly sought information about Wikipedia. For simple searches, there is a search bar at the top of every page. Type what you are looking for in the box. Suggested matches will appear in a dropdown list. Select any page in the list to go to that page. Or, select the \"Search\" button, or press \u21b5 Enter , to go to a full search result. For advanced searches, see Help:Searching . There are other ways to browse and explore Wikipedia articles; many can be found at Wikipedia:Contents . See our disclaimer for cautions about Wikipedia's limitations. For mobile access , press the mobile view link at the very bottom of every desktop view page. Edit an article Contributing is easy: see how to edit a page . For a quick summary on participating, see contributing to Wikipedia , and for a friendly tutorial, see our introduction . For a listing of introductions and tutorials by topic, see getting started . The Simplified Manual of Style and Cheatsheet can remind you of basic wiki markup . Be\u00a0bold in improving articles! When adding facts, please provide references so others may verify them. If you are affiliated with the article subject, please see our conflict of interest guideline . The simple guide to vandalism cleanup can help you undo malicious edits. If you're looking for places you can help out, the Task Center is the place to go, or check out what else is happening at the community portal . You can practice editing and experiment in a sandbox your sandbox . Report a problem with an article If there is a problem with an article about yourself, a family member, a friend or a colleague, please read Biographies of living persons/Help . If you spot a problem with an article, you can fix it directly, by clicking on the \"Edit\" link at the top of that page. See the \"edit an article\" section of this page for more information. If you don't feel ready to fix the article yourself, post a message on the article's talk page . This will bring the matter to the attention of others who work on that article. There is a \"Talk\" link at the beginning of every article page. You can contact us . If it's an article about you or your organization, see Contact us \u2013 Subjects . Create a new article or upload media Check Your first article to see if your topic is appropriate, then the Article wizard will walk you through creating the article. Once you have created an article, see Writing better articles for guidance on how to improve it and what to include (like reference citations ). For contributing images, audio or video files, see the Introduction to uploading images . Then the Upload wizard will guide you through that process. FAQ and interactive help Answers to common problems can be found at frequently asked questions . Or check out where to ask questions or make comments . New users should seek help at the Teahouse if they're having problems while editing Wikipedia. More complex questions can be posed at the Help desk . Volunteers will respond as soon as they're able. Or ask for help on your talk page and a volunteer will visit you there! You can get live help with editing in the help chatroom . For help with technical issues, ask at the Village pump . Factual questions If searching Wikipedia has not answered your question ( for example, questions like \"Which country has the world's largest fishing fleet?\" ), try the Reference Desk . Volunteers there will attempt to answer your questions on any topic, or point you toward the information you need. Directories Directory : main list of directories and indexes. Help related Help directory : for informative, instructional and consultation pages. Request directory : for services and assistance that can be requested on Wikipedia. Protocols and conventions Policy directory : official policies for \"English Wikipedia\". Guideline directory : official guidelines for \"English Wikipedia\". Manual of Style directory : pages related to the style manual of Wikipedia articles. Community related Departments : for the different divisions of Wikipedia. Editor's index : for everything an editor needs to know to work on Wikipedia. Essay directory : for Wikipedia namespace essays. Dashboard : for current discussions taking place throughout Wikipedia. WikiProjects : for people who want to work together as a team to improve Wikipedia. MediaWiki software Wikitext : for the syntax used by Wikipedia to format a page. HTML : for HTML5 elements, or tags and their attribute. Templates : for templates used within Wikipedia. Help by topic Help Menu : for searching by subject matter. Navigating Wikipedia : for searching and browsing the encyclopedia. Joining Wikipedia : how to get involved. Editing Wikipedia : has general help for editors. Links and references : has help for creating links, or dealing with references Images and media : how to use images, videos and sound files. Keeping track of changes : how to track the evolution of a page, or follow a user. Policies and guidelines : for community standards. Asking questions : volunteers will attempt to answer. The Wikipedia community : how to submit or debate a proposal. Resources and lists : has resources for editors. Account settings : has tips and tools for registered users. Technical information : has tools for advanced users, and troubleshooting. Site map : is the above thirteen pages on one single page. Tip of the day Citation footnotes Use the <ref> tag to add references to your articles presented as footnotes.  This tag is easy and convenient because it allows you to cite your sources within your text and have them automatically numbered and added to your References section at the end of the article.  To cite a source, simply type the <ref> tag after the statement the reference is for. For example: Haliburton park is the largest park in the world.<ref>Bill Harton (2005).  http://www.linkhere.com. Retrieved March 3, 2005.</ref> Then, at the end of your article, add the following template to include all of the citations in your article: {{Reflist}} directly under the References or Notes section title. Prior tip \u2013 Tips library \u2013 Next tip Read more: Help:Footnotes Become a Wikipedia tipster \u2013 Tips library, by subject To add this auto-updating daily tip to your user page, use {{ totd }} Additional searches Search Frequently Asked Questions Search the help desk archives v t e Basic information on Wikipedia - ( Search ) Help directory menu Teahouse (interactive help for new editors) About Wikipedia Readers' index to Wikipedia Statistics Administration FAQs Purpose Who writes Wikipedia? Organization Censorship In brief General disclaimer Readers' FAQ Student help Navigation Searching Viewing media Help Mobile access Parental advice Other languages Researching with Wikipedia Citing Wikipedia Copyright Introductions to contributing Main introduction List of tutorials and introductions The answer Dos and don'ts Learning the ropes Common mistakes Newcomer primer Simplified ruleset The \"Missing Manual\" Your first article Wizard Young Wikipedians The Wikipedia Adventure Accounts Why create an account? Logging in Email confirmation Editing Toolbar Conflict VisualEditor User guide Pillars, policies and guidelines Five pillars Manual of Style Simplified Etiquette Expectations Oversight Principles Ignore all rules The rules are principles Core content policies Policies and guidelines Vandalism Appealing blocks What Wikipedia is not Getting help Help menu Help desk Reference Desk Category Requests for help Disputes resolution requests IRC live chat Tutorial Contact us Wikipedia community Departments Meetups WikiProjects Overview FAQ Village pump policy technical proposals idea lab wikimedia misc Newsletters : Signpost Library Newsletter GLAM Newsletter Informational: Community portal Dashboard Noticeboards Maintenance Task Center Essays Open letters Sourcing and referencing Research Wikipedia Library Request Resources Free resources Request access to major databases WikiProject Resource Exchange Shared resources Guidelines Finding sources Combining sources Referencing Citations Citation Style 1 Citation templates Footnotes Page numbers Cite errors How-to guides Category Tools Infoboxes Writing articles Development Better writing Files Images Media files Uploading Deletion User contribution pages Editing pages Diffs Namespaces Deletion Merging Renaming Requests Redirecting Reverting Vandalism cleanup Talk pages Archiving Wiki markup Wiki markup Cheatsheet Columns HTML Lists Magic words For beginners Sections Sounds Special Characters Tables Templates Documentation Index Substitution Transclusion Image and video markup Tutorial Linking Directories and glossaries Category Abbreviations Contents Edit summaries Essays Glossary Index Shortcuts Tips Shortcuts to this page: WP:HELP WP:H Retrieved from \" https://en.wikipedia.org/w/index.php?title=Help:Contents&oldid=1283050621 \" Categories : Help Wikipedia help contents Hidden categories: Wikipedia semi-protected project pages Wikipedia move-protected project pages This page was last edited on 30 March 2025, at 03:57 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Help : Contents 188 languages Add topic"}, "9": {"url": "https://en.wikipedia.org/wiki/Help:Introduction", "text": "Help:Introduction - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Help : Introduction 117 languages Afrikaans Alemannisch \u00c6nglisc \u0410\u0525\u0441\u0448\u04d9\u0430 \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Arpetan \u0985\u09b8\u09ae\u09c0\u09af\u09bc\u09be \u0410\u0432\u0430\u0440 \u062a\u06c6\u0631\u06a9\u062c\u0647 Basa Bali \u09ac\u09be\u0982\u09b2\u09be \u95a9\u5357\u8a9e / B\u00e2n-l\u00e2m-g\u00fa Basa Banyumasan \u0411\u0430\u0448\u04a1\u043e\u0440\u0442\u0441\u0430 \u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438 Boarisch Bosanski \u0411\u0443\u0440\u044f\u0430\u0434 Catal\u00e0 Cebuano \u010ce\u0161tina Cymraeg Davvis\u00e1megiella Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Esperanto Euskara \u0641\u0627\u0631\u0633\u06cc F\u00f8royskt Fran\u00e7ais Gaeilge Gaelg G\u00e0idhlig Galego \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Hulontalo Bahasa Indonesia Interlingua \u00cdslenska Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Jawa Kapampangan \u041a\u044a\u0430\u0440\u0430\u0447\u0430\u0439-\u043c\u0430\u043b\u043a\u044a\u0430\u0440 \u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8 \u0915\u0949\u0936\u0941\u0930 / \u06a9\u0672\u0634\u064f\u0631 \u049a\u0430\u0437\u0430\u049b\u0448\u0430 Krey\u00f2l ayisyen Kurd\u00ee \u041a\u044b\u0440\u0433\u044b\u0437\u0447\u0430 Ladin Ladino \u0ea5\u0eb2\u0ea7 Latina Latvie\u0161u L\u00ebtzebuergesch Lietuvi\u0173 Limburgs Magyar \u092e\u0948\u0925\u093f\u0932\u0940 \u041c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438 \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02 Malti \u0645\u0635\u0631\u0649 Bahasa Melayu \uabc3\uabe4\uabc7\uabe9 \uabc2\uabe3\uabdf \u041c\u043e\u043d\u0433\u043e\u043b \u1019\u103c\u1014\u103a\u1019\u102c\u1018\u102c\u101e\u102c Nederlands \u65e5\u672c\u8a9e Napulitano Norsk bokm\u00e5l Norsk nynorsk \u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40 \u1015\u1021\u102d\u102f\u101d\u103a\u108f\u1018\u102c\u108f\u101e\u102c\u108f \u067e\u069a\u062a\u0648 \u1797\u17b6\u179f\u17b6\u1781\u17d2\u1798\u17c2\u179a Plattd\u00fc\u00fctsch Polski Rom\u00e2n\u0103 Romani \u010dhib Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 \u0938\u0902\u0938\u094d\u0915\u0943\u0924\u092e\u094d Sardu Scots Sicilianu \u0dc3\u0dd2\u0d82\u0dc4\u0dbd Simple English \u0633\u0646\u068c\u064a Sloven\u010dina Sloven\u0161\u010dina \u06a9\u0648\u0631\u062f\u06cc \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Sunda Suomi Svenska Tagalog Tarand\u00edne \u0422\u0430\u0442\u0430\u0440\u0447\u0430 / tatar\u00e7a \u1010\u1086\u1038 \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41 \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u0434\u043c\u0443\u0440\u0442 \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u0627\u0631\u062f\u0648 \u0626\u06c7\u064a\u063a\u06c7\u0631\u0686\u06d5 / Uyghurche Vahcuengh Ti\u1ebfng Vi\u1ec7t \u05d9\u05d9\u05b4\u05d3\u05d9\u05e9 \u4e2d\u6587 \u1956\u196d\u1970 \u1956\u196c\u1972 \u1951\u1968\u1952\u1970 Edit links Help page Talk English Read View source View history Tools Tools move to sidebar hide Actions Read View source View history General What links here Related changes Upload file Permanent link Page information Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Multilingual Wikisource Wikifunctions Wikisource Wikiversity Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Tutorial for new editors Introduction to Wikipedia Wikipedia is made by people like you . This page takes you through a set of tutorials aimed at complete newcomers who wish to contribute. It covers all the basics, and each tutorial takes only a few minutes, so you can become a proficient Wikipedian quickly. Get started Policies and Guidelines The wiki markup source editor shows the underlying page source code. It works like a plain text file, indicating links and other items using simple code like this: [[Earth]] . Editing Referencing Images Tables VisualEditor is the easier way of editing that works more like a word processor and hides the underlying source code. Links and other items are edited using toolbar and pop-up interfaces. Editing Referencing Images Tables Talk pages Navigating Wikipedia Manual of Style Conclusion View all as single page Help Wikipedia Shortcuts WP:I H:I H:INTRO For more training information, see also: Full help contents page Training for students A single-page guide to contributing A training adventure game Resources for new editors Please consider leaving us some feedback. Retrieved from \" https://en.wikipedia.org/w/index.php?title=Help:Introduction&oldid=1286675512 \" Category : Wikipedia quick introductions Hidden category: Wikipedia semi-protected project pages This page was last edited on 21 April 2025, at 09:39 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Help : Introduction 117 languages Add topic"}, "10": {"url": "https://en.wikipedia.org/wiki/Wikipedia:Community_portal", "text": "Wikipedia:Community portal - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Wikipedia : Community portal 218 languages \u0410\u0434\u044b\u0433\u0430\u0431\u0437\u044d Afrikaans Alemannisch \u0410\u043b\u0442\u0430\u0439 \u0442\u0438\u043b \u0905\u0902\u0917\u093f\u0915\u093e \u00c6nglisc \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Aragon\u00e9s Arm\u00e3neashti \u0985\u09b8\u09ae\u09c0\u09af\u09bc\u09be Asturianu Atikamekw Ava\u00f1e'\u1ebd \u0410\u0432\u0430\u0440 Az\u0259rbaycanca \u062a\u06c6\u0631\u06a9\u062c\u0647 Basa Bali \u09ac\u09be\u0982\u09b2\u09be Banjar \u95a9\u5357\u8a9e / B\u00e2n-l\u00e2m-g\u00fa Basa Banyumasan \u0411\u0430\u0448\u04a1\u043e\u0440\u0442\u0441\u0430 \u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f \u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f (\u0442\u0430\u0440\u0430\u0448\u043a\u0435\u0432\u0456\u0446\u0430) \u092d\u094b\u091c\u092a\u0941\u0930\u0940 \u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438 Boarisch Bosanski Brezhoneg Catal\u00e0 \u0427\u04d1\u0432\u0430\u0448\u043b\u0430 \u010ce\u0161tina Chamoru Chavacano de Zamboanga Chi-Chewa ChiTumbuka Chahta anumpa Cymraeg Dansk \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Davvis\u00e1megiella Deitsch Deutsch \u078b\u07a8\u0788\u07ac\u0780\u07a8\u0784\u07a6\u0790\u07b0 \u0f47\u0f7c\u0f44\u0f0b\u0f41 Ebon Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac \u042d\u0440\u0437\u044f\u043d\u044c Espa\u00f1ol Esperanto Estreme\u00f1u Euskara \u0641\u0627\u0631\u0633\u06cc F\u00f8royskt Fran\u00e7ais Frysk Furlan Gaeilge Gaelg G\u00e0idhlig Galego \u0413\u04c0\u0430\u043b\u0433\u04c0\u0430\u0439 \u8d1b\u8a9e \u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0 \u5ba2\u5bb6\u8a9e / Hak-k\u00e2-ng\u00ee \u0425\u0430\u043b\u044c\u043c\u0433 \ud55c\uad6d\uc5b4 Hausa \u0939\u093f\u0928\u094d\u0926\u0940 Hiri Motu Bahasa Hulontalo Ido Igbo Ilokano Bahasa Indonesia Interlingua Interlingue I\u00f1upiatun IsiXhosa \u00cdslenska Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Jawa Kalaallisut \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 \u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8 Kasz\u00ebbsczi \u049a\u0430\u0437\u0430\u049b\u0448\u0430 Kernowek Ikinyarwanda \u041a\u043e\u043c\u0438 Kongo Kurd\u00ee Ladino \u041b\u0430\u043a\u043a\u0443 \u0ea5\u0eb2\u0ea7 Latina L\u00ebtzebuergesch \u041b\u0435\u0437\u0433\u0438 Lietuvi\u0173 Li Niha Ligure Lingua Franca Nova Livvinkarjala Lombard Magyar Madhur\u00e2 \u041c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438 Malagasy \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02 M\u0101ori \u092e\u0930\u093e\u0920\u0940 \u1018\u102c\u101e\u102c\u1019\u1014\u103a Bahasa Melayu Minangkabau Mirand\u00e9s \u041c\u043e\u043a\u0448\u0435\u043d\u044c \u1019\u103c\u1014\u103a\u1019\u102c\u1018\u102c\u101e\u102c Dorerin Naoero Na Vosa Vakaviti Nederlands Nedersaksies \u0928\u0947\u092a\u093e\u0932 \u092d\u093e\u0937\u093e \u65e5\u672c\u8a9e Napulitano \u041d\u043e\u0445\u0447\u0438\u0439\u043d Nordfriisk Norsk bokm\u00e5l Norsk nynorsk \ua187\ua259 Occitan \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 \u0b13\u0b21\u0b3c\u0b3f\u0b06 Oshiwambo Otsiherero O\u02bbzbekcha / \u045e\u0437\u0431\u0435\u043a\u0447\u0430 P\u00e4lzisch Pangasinan Pangcah \u067e\u069a\u062a\u0648 Patois \u1797\u17b6\u179f\u17b6\u1781\u17d2\u1798\u17c2\u179a Piemont\u00e8is Pinayuanan Plattd\u00fc\u00fctsch Polski Portugu\u00eas Qaf\u00e1r af Qaraqalpaqsha Ripoarisch Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0438\u043d\u044c\u0441\u043a\u044b\u0439 \u0420\u0443\u0441\u0441\u043a\u0438\u0439 \u0421\u0430\u0445\u0430 \u0442\u044b\u043b\u0430 Sakizaya S\u00e4ng\u00f6 \u1c65\u1c5f\u1c71\u1c5b\u1c5f\u1c72\u1c64 Sardu Scots Seeltersk Setswana Sicilianu \u0dc3\u0dd2\u0d82\u0dc4\u0dbd Simple English \u0633\u0646\u068c\u064a Sloven\u010dina Sloven\u0161\u010dina Soomaaliga \u06a9\u0648\u0631\u062f\u06cc \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Srpskohrvatski / \u0441\u0440\u043f\u0441\u043a\u043e\u0445\u0440\u0432\u0430\u0442\u0441\u043a\u0438 Sunda Suomi Svenska Tagalog \u0ba4\u0bae\u0bbf\u0bb4\u0bcd Tarand\u00edne \u0422\u0430\u0442\u0430\u0440\u0447\u0430 / tatar\u00e7a \u1010\u1086\u1038 \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41 \u0e44\u0e17\u0e22 \u1275\u130d\u122d\u129b \u0422\u043e\u04b7\u0438\u043a\u04e3 Tshivenda \u0ca4\u0cc1\u0cb3\u0cc1 T\u00fcrk\u00e7e T\u00fcrkmen\u00e7e Tyap \u0422\u044b\u0432\u0430 \u0434\u044b\u043b \u0423\u0434\u043c\u0443\u0440\u0442 Basa Ugi \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u0627\u0631\u062f\u0648 Vahcuengh V\u00e8neto Veps\u00e4n kel\u2019 Ti\u1ebfng Vi\u1ec7t Volap\u00fck \u6587\u8a00 West-Vlams Winaray \u5434\u8bed Xitsonga \u05d9\u05d9\u05b4\u05d3\u05d9\u05e9 Yor\u00f9b\u00e1 \u7cb5\u8a9e Zazaki \u017demait\u0117\u0161ka \u4e2d\u6587 Obolo Betawi Ghanaian Pidgin Jaku Iban Yerwa Kanuri \ua80d\ua824\ua81f\ua810\ua824 Tol\u0131\u015fi Edit links Project page Talk English Read View source View history Tools Tools move to sidebar hide Actions Read View source View history General What links here Related changes Upload file Permanent link Page information Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons MediaWiki Multilingual Wikisource Wikispecies Wikibooks Wikidata Wikifunctions Wikimania Wikinews Wikiquote Wikisource Wikiversity Wikivoyage Wiktionary Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Collaborations, tasks and news about English Wikipedia Community portal Dashboard Departments Maintenance Tasks Discord IRC Meetups Directories ( Protocols Essays How-to Index Noticeboards WikiProjects ) Welcome to the community portal! Shortcuts P:WP WP:COM This page provides a listing of current collaborations, tasks, and news about English Wikipedia. New to Wikipedia ? See the contributing to Wikipedia page or our tutorial for everything you need to know to get started. For a listing of internal project pages of interest, see the department directory . Interact more For a listing of ongoing discussions and current requests, see the Dashboard . Help desk Ask questions about how to use or edit Wikipedia The Teahouse Friendly help for newcomers Reference desk Ask research questions about any topic except Wikipedia itself WikiProjects Work with other editors on a shared area of interest Dispute resolution Get help resolving disputes Village pump sections ( view all ) Policy Discuss existing and proposed policies Technical Discuss technical issues about Wikipedia Proposals Discuss new proposals that are not policy-related Idea lab Incubate new ideas before formally proposing them WMF Discuss issues involving the Wikimedia Foundation Miscellaneous Post messages that do not fit into any other category Community bulletin board This section is transcluded from Wikipedia:Community bulletin board . ( edit | history ) How to add to the community bulletin board The community bulletin board has 2 sections that can be used by Wikipedians for announcements: \"Events and projects\" and \"WikiProject notices\". In general, keep it concise (under 2 lines), refrain from fancy formatting , and new entries should be placed at the top of their section. Before editing, make sure you are on Wikipedia:Community bulletin board , not Wikipedia:Community portal , where the board is transcluded. Events and projects : In this section, only organized events, projects, and/or competitions should be listed. These are organized by how often they occur: The Yearly section is for uncommon events, like events that only occur every year, once, or irregularly. The Monthly section is for events that occur each month, or are always ongoing. WikiProject notices : In this section, any announcement, request for help or other notice from a WikiProject should be listed here. Entries should be signed, and ordered from newest to oldest. Entries are to be removed after a period of 6 months . Welcome to the community bulletin board , which is a page used for announcements from WikiProjects and other groups. Included here are coordinated efforts, events, projects, and other general announcements. Dashboard News Task Center Goings-on Meetups In the media Mailing lists Events and projects [add] Yearly or infrequent events WikiProject Unreferenced articles is holding a month-long Unreferenced articles backlog drive beginning 1 June 2025, 00:00 (UTC). The Core Contest . April 15 - May 31, an contest to improve broad and important articles, together with others or alone. Sign-ups are open until the end of the competition. Wikidata and Sister Projects . May 29 - June 1, an online event exploring the different ways Wikidata is connected and used in the other Wikimedia projects, including Wikipedia. We are currently accepting session proposals for the event program, you can submit them here , or ask any questions you may have! Monthly or continuous events Monthly contest, WikiProject Military history . The contest department of the Military history WikiProject aims to motivate increased quality in military history articles by offering a form of friendly competition for project members making improvements to them. The primary contest available is a simple rolling competition that awards points for improving articles. The contest runs from the first to last day of each month. Guild of Copy Editors' editing drive. The May 2025 backlog drive is a one-month-long effort by the Guild of Copy Editors to reduce the backlog of articles that require copy editing: those carrying the {{ copy edit }} tag (also {{ awkward }} , {{ copy edit section }} , {{ inappropriate person }} , and {{ copy edit inline }} , and their redirects) or are listed on the GOCE Requests page . It began on 1 May, 00:00 (UTC), and will end on 31 May, 23:59 (UTC). Wikipedia:WikiProject Women in Red 2025 Events : Recently completed: Alphabet run: G & H Business New this month: Revolutionary women Alphabet run: I & J Ongoing initiatives: Music #1day1woman Upcoming events: Ideas Meetups for May 2025 +/- Exeter 3 May\u00a03,\u00a02025 ( 2025-05-03 ) San Diego 121 May\u00a03,\u00a02025 ( 2025-05-03 ) San Francisco May\u00a08,\u00a02025 ( 2025-05-08 ) San Diego 122 May\u00a010,\u00a02025 ( 2025-05-10 ) London 216 May\u00a011,\u00a02025 ( 2025-05-11 ) Los Angeles May\u00a011,\u00a02025 ( 2025-05-11 ) US Mountain West online May\u00a013,\u00a02025 ( 2025-05-13 ) WikiCon New Zealand ( 2025-05-16 ) ( 2025-05-18 ) May 16\u201318, 2025 Singapore 21 May\u00a017,\u00a02025 ( 2025-05-17 ) Johannesburg May\u00a017,\u00a02025 ( 2025-05-17 ) Cape Town May\u00a017,\u00a02025 ( 2025-05-17 ) Oxford 111 May\u00a018,\u00a02025 ( 2025-05-18 ) Chicago May 2025 May\u00a024,\u00a02025 ( 2025-05-24 ) Aberdeen 2 May\u00a026,\u00a02025 ( 2025-05-26 ) Edinburgh 21 May\u00a031,\u00a02025 ( 2025-05-31 ) Meetups for June 2025 +/- Brighton 5 June\u00a014,\u00a02025 ( 2025-06-14 ) San Diego 123 June\u00a021,\u00a02025 ( 2025-06-21 ) Exeter 4 June\u00a028,\u00a02025 ( 2025-06-28 ) WikiProject notices [add] Also consider posting WikiProject, Task Force, and Collaboration news at The Signpost ' s WikiProject Report page. Please include your signature when adding a listing here. WikiProject Christianity is back and running! We are looking for new and interested editors to join . We look forward to working with you! She riff U3 12:51, 5 April 2025 (UTC) WikiProject Outlines news: New outlines: Outline of San Diego Outline of Leeds Outline of Glasgow Another outline has been nominated for Featured List status: Outline of caves The following outline covers an ongoing situation, please help keep it updated: Outline of the Russo-Ukrainian War The Outline of the week is Outline of Earth science . Please help improve them. \u2014 The Transhumanist 11:59, 29 March 2025 (UTC) WP Unreferenced articles is trying to clear the decades-long backlog of unreferenced articles . The backlog recently reached under 67,000 -- down from 90,000 half a year ago. Come contribute! Mrfoogles ( talk ) 21:44, 12 March 2025 (UTC) Technical news This section is transcluded from Wikipedia:Tech news . ( edit | history ) Latest tech news from the Wikimedia technical community. Please tell other users about these changes. Not all changes will affect you. Translations are available. Weekly highlight The \"Get shortened URL\" link on the sidebar now includes a QR code . Wikimedia site users can now use it by scanning or downloading it to quickly share and access shared content from Wikimedia sites, conveniently. Updates for editors The Wikimedia Foundation is working on a system called Edge Uniques , which will enable A/B testing , help protect against distributed denial-of-service attacks (DDoS attacks), and make it easier to understand how many visitors the Wikimedia sites have. This is to help more efficiently build tools which help readers, and make it easier for readers to find what they are looking for. Tech News has previously written about this . The deployment will be gradual. Some might see the Edge Uniques cookie the week of 19 May. You can discuss this on the talk page . Starting May 19, 2025, Event organisers in wikis with the CampaignEvents extension enabled can use Event Registration in the project namespace (e.g., Wikipedia namespace, Wikidata namespace). With this change, communities don't need admins to use the feature. However, wikis that don't want this change can remove and add the permitted namespaces at Special:CommunityConfiguration/CampaignEvents . The Wikipedia project now has a Wikipedia in Nupe ( w:nup: ). This is a language primarily spoken in the North Central region of Nigeria. Speakers of this language are invited to contribute to new Wikipedia . View all 27 community-submitted tasks that were resolved last week . Updates for technical contributors Developers can now access pre-parsed Dutch Wikipedia, amongst others (English, German, French, Spanish, Italian, and Portuguese) through the Structured Contents snapshots (beta) . The content includes parsed Wikipedia abstracts, descriptions, main images, infoboxes, article sections, and references. The /page/data-parsoid REST API endpoint is no longer in use and will be deprecated. It is scheduled to be turned off on June 7, 2025. Detailed code updates later this week: MediaWiki In depth The IPv6 support is a newly introduced Cloud virtual network that significantly boosts Wikimedia platforms' scalability, security, and readiness for the future. If you are a technical contributor eager to learn more, check out this blog post for an in-depth look at the journey to IPv6. Meetings and events The 2nd edition of 2025 of Afrika Baraza , a virtual platform for African Wikimedians to connect, will take place on May 15 at 17:00 UTC . This edition will focus on discussions regarding Wikimedia Annual planning and progress . The MENA Connect Community Call , a virtual meeting for MENA Wikimedians to connect, will take place on May 17 at 17:00 UTC . You can register now to attend. Tech news prepared by Tech News writers and posted by bot \u2022 Contribute \u2022 Translate \u2022 Get help \u2022 Give feedback \u2022 Subscribe or unsubscribe . Requests for comment Discussions in the following areas have requested wider attention via Requests for comment : Biographies Economy and trade History and geography Language and linguistics Media, the arts, and architecture Politics, government, and law Religion and philosophy Science and mathematics Society, sports, and culture Wikipedia policies and guidelines Wikipedia style and naming WikiProjects and collaborations Wikipedia technical issues and templates Wikipedia proposals Unsorted General notices Stay up to date with what's happening on Wikipedia. The newest issue of The Signpost is out now. Gaza genocide in wikivoice and opening sentence Post-nominal letters in the first sentence of biographical articles Community sanctions for \"Assyrian\" topics Speedy keep for no-BEFORE mass nominations Capitalization of sources in citations 14 May 2025 News and notes: WMF to kick off new-CEO quest as Iskandar preps to move on \u2014 Supreme Court nixes gag of Wiki page for other India court row on ANI \u2014 code-heads give fix-up date for Charts in lieu of long-dead Graph gizmo In the media: Wikimedia Foundation sues over UK government decision that might require identity verification of editors worldwide Disinformation report: What does Jay-Z know about Wikipedia? In focus: On the hunt for sources: Swedish AfD discussions Technology report: WMF introduces unique but privacy-preserving browser cookie Debriefing: Goldsztajn's RfA debriefing Obituary: Max Lum (User:ICOHBuzz) Community view: A Deep Dive Into Wikimedia (part 2) Comix: Collection From the archives: Humor from the Archives Single page Front page About Subscribe Suggestions Archives Help out You can help improve the articles listed below! This list updates frequently, so check back here for more tasks to try. (See Wikipedia:Maintenance or the Task Center for further information.) Fix spelling and grammar None More... \u2022 Learn how Fix wikilinks Korey Koot Jacob O. Wobbrock Norman McCall Tulloh Little Heart Records Montelupich Prison More... \u2022 Learn how Update with new information Bank rate Federal takeover of Fannie Mae and Freddie Mac Islands District Bangladesh Davis Cup team Hartpury University R.F.C. More... \u2022 Learn how Expand short articles Masie Niewiarowo Laskowiec, Podlaskie Voivodeship Lewonie, Gmina Mo\u0144ki Krynica, Mo\u0144ki County More... \u2022 Learn how Check and add references Philosophical Investigations List of Punjabi films of 2021 Telecommunications in India Proton Iriz Heart of Atlanta Motel, Inc. v. United States More... \u2022 Learn how Fix original research issues Epicenity Dapol Carburetor FERT Centre Right Alliance More... \u2022 Learn how Improve lead sections Takeru (company) Brumer Islands 2008 AIK Fotboll season 2020 New York Red Bulls II season Worood Zuhair More... \u2022 Learn how Add an image Rhode Island Route 146 Rosario Sports Complex Rwamagana Solar Power Station Ribberull Residence, Abbott Street More... \u2022 Learn how Translate and clean up Indian political society Fights Break Spheres Mahoroba (train) Civic Initiative Valentin A. Bazhanov More... \u2022 Learn how Help counter systemic bias by creating new articles on important women . Help improve popular pages , especially those of low quality . This section is transcluded from Wikipedia:Community portal/Open tasks . ( edit | history ) This week's article for improvement is: Pneumatics Previous selections: Motherboard \u00b7 Internet research \u00b7 Stuffed toy This week's backlog of the week is: Category:Articles that need to differentiate between fact and fiction Things to review Copyvios New pages New files Recent edits New editors IP edits Discussions Newest featured content Articles St Scholastica Day riot Roon-class cruiser Not in Love (Crystal Castles song) Spyridon Marinatos Weird Faith Llullaillaco Heartburn Hume MRT station Daily News Building Orphic Hymns Gaetano Bresci Grace Coolidge 1924\u201325 Gillingham F.C. season Zeng Laishun Constans II (son of Constantine III) Topics Line of Duty Overview of SZA Green Bay Packers players History of the National Hockey League Peter Capaldi Detroit Lions draft history Jake Gyllenhaal Green Bay Packers draft history Overview of Rachelle Ann Go Bruno Mars Overview of Ben&Ben Primates 30 (album) Jupiter Overview of Angeline Quinto Lists Small Faces discography Hot Adult Contemporary number ones of 1990 Ghost towns in Oklahoma Baseball Academic All-America Team Members of the Year Members of the 12th Central Committee of the Communist Party of Vietnam North Korean propaganda slogans World Figure Skating Championships Performances by Josette Simon Mormoopids Minnesota Golden Gophers women's ice hockey seasons Governors of Rivers State Chiropterans The Sarah Jane Adventures serials Accolades received by Little Women (2019 film) Los Angeles Rams starting quarterbacks Pictures Dua Lipa Mountains at Collioure Saint-Gaudens double eagle Second illustration to \"The Bottle Imp\" Toxopneustes pileolus Callistocypraea aurantium George Metesky A Sunday Afternoon on the Island of La Grande Jatte The Cameraman American woodcock Azure Window First illustration of \"The Bottle Imp\" Thousand-yard stare Garcinia humilis Tufted titmouse Tip of the day Tip of the day... How to add hidden editor notes in an article Have you ever needed to post an important message to all editors about an article, on the article itself, but thought it would stick out like a sore thumb and ruin the article if you did?  Are you reverting many edits on an article because editors just aren't seeing the important message or special instructions on the talk page? The solution is that you can insert hidden text in the article! That way, only the people editing the page will see your message! Here is how to insert a hidden comment: First, begin the comment by typing <!-- Once you have done that, type what you need the editors to read Then, end the comment by typing --> Once you have completed those 3 easy steps, you won't be reverting as many mistakes! For example, the following hidden comment has been used in the Meaning of life article, in the Popular views section: <!--Please do not add 42 in this section. It is covered under the section titled \"Popular culture treatments\". Thank you.--> Some more examples of pages that have hidden messages include: Main page Community bulletin board English Mastiff Prior tip \u2013 Tips library \u2013 Next tip Read more: Help:Hidden text Become a Wikipedia tipster To add this auto-updating template to your user page, use {{ totd }} Wikipedia:Community portal at Wikipedia's sister projects : Definitions from Wiktionary Media from Commons News from Wikinews Quotations from Wikiquote Texts from Wikisource Textbooks from Wikibooks Resources from Wikiversity Travel guides from Wikivoyage Taxa from Wikispecies Data from Wikidata v t e Basic information on Wikipedia - ( Search ) Help directory menu Teahouse (interactive help for new editors) About Wikipedia Readers' index to Wikipedia Statistics Administration FAQs Purpose Who writes Wikipedia? Organization Censorship In brief General disclaimer Readers' FAQ Student help Navigation Searching Viewing media Help Mobile access Parental advice Other languages Researching with Wikipedia Citing Wikipedia Copyright Introductions to contributing Main introduction List of tutorials and introductions The answer Dos and don'ts Learning the ropes Common mistakes Newcomer primer Simplified ruleset The \"Missing Manual\" Your first article Wizard Young Wikipedians The Wikipedia Adventure Accounts Why create an account? Logging in Email confirmation Editing Toolbar Conflict VisualEditor User guide Pillars, policies and guidelines Five pillars Manual of Style Simplified Etiquette Expectations Oversight Principles Ignore all rules The rules are principles Core content policies Policies and guidelines Vandalism Appealing blocks What Wikipedia is not Getting help Help menu Help desk Reference Desk Category Requests for help Disputes resolution requests IRC live chat Tutorial Contact us Wikipedia community Departments Meetups WikiProjects Overview FAQ Village pump policy technical proposals idea lab wikimedia misc Newsletters : Signpost Library Newsletter GLAM Newsletter Informational: Community portal Dashboard Noticeboards Maintenance Task Center Essays Open letters Sourcing and referencing Research Wikipedia Library Request Resources Free resources Request access to major databases WikiProject Resource Exchange Shared resources Guidelines Finding sources Combining sources Referencing Citations Citation Style 1 Citation templates Footnotes Page numbers Cite errors How-to guides Category Tools Infoboxes Writing articles Development Better writing Files Images Media files Uploading Deletion User contribution pages Editing pages Diffs Namespaces Deletion Merging Renaming Requests Redirecting Reverting Vandalism cleanup Talk pages Archiving Wiki markup Wiki markup Cheatsheet Columns HTML Lists Magic words For beginners Sections Sounds Special Characters Tables Templates Documentation Index Substitution Transclusion Image and video markup Tutorial Linking Directories and glossaries Category Abbreviations Contents Edit summaries Essays Glossary Index Shortcuts Tips v t e Wikipedia editor navigation - ( Search ) v t e Wikipedia key policies and guidelines (?) Five pillars Ignore all rules Content (?) P Verifiability No original research Neutral point of view What Wikipedia is not Biographies of living persons Copyright ( Copyright violations ) Image use Article titles G Notability Autobiography Citing sources Reliable sources Medicine Do not include copies of lengthy primary sources Plagiarism Don't create hoaxes Fringe theories Patent nonsense External links Conduct (?) P Civility Consensus Harassment Vandalism Ignore all rules No personal attacks Ownership of content Edit warring Dispute resolution Sockpuppetry No legal threats Child protection Paid-contribution disclosure G Assume good faith Conflict of interest Disruptive editing Do not disrupt Wikipedia to illustrate a point Etiquette Gaming the system Please do not bite the newcomers Courtesy vanishing Responding to threats of harm Talk page guidelines Signatures Deletion (?) P Deletion policy Proposed deletion Biographies Speedy deletion Attack page Oversight Revision deletion Enforcement (?) P Administrators Banning Blocking Page protection Editing (?) P Editing policy G Article size Summary style Be bold Disambiguation Hatnotes Broad-concept article Understandability Style Manual of Style Contents Accessibility Dates and numbers Images Layout Lead section Linking Lists Classification Categories, lists, and navigation templates Categorization Template namespace Project content (?) G Project namespace WikiProjects User pages User boxes Shortcuts Subpages WMF (?) P Universal Code of Conduct Terms of Use List of policies Friendly space policy Licensing and copyright Privacy policy List of all policies and guidelines P : List of policies G : List of guidelines Summaries of values and principles v t e Manual of Style Overview Contents Tips Content Accessibility Biography Disambiguation pages Organizing by subject area Gender identity Hidden text Infoboxes Linking Self-references Words to watch Formatting Abbreviations Capitalization Dates and numbers Pronunciation Spelling Text formatting Titles of works Images Captions Image placement Icons Images Layout Layout Lead section Tables Trivia sections Lists Lists Lists of works Road junctions Stand-alone lists By topic area Arts Anime and manga Comics Film Lyrics and poetry Novels Television Video games Visual arts Writing about fiction WikiProject style advice Music Music Music samples Record charts Stringed instruments WikiProject style advice History Blazons Military history WikiProject style advice Legal and cultural Legal Trademarks WikiProject style advice Regional Specific naming conventions Canada China (and Chinese) France (and French) Egypt Hawaii India Indonesia Ireland Japan Korea Malaysia Pakistan Philippines Poland Singapore WikiProject style advice Religion and education Islam Latter Day Saints WikiProject style advice Science and technology Mathematics Medicine Chemistry Compound classes Chemicals References and external links Safety Structure drawing Computer science Taxonomy WikiProject style advice Sports Cue sports Snooker WikiProject style advice Related guidelines Article size Article titles Categories, lists, and navigation templates Categorization Hatnotes Subpages Understandability Search Category v t e Wikipedia accounts and governance Unregistered (IP) users Why create an account? Create an account Request an account IPs are human too IP addresses are not people IP hopper Registered users New account Logging in Reset passwords Username policy Changing username Usernames for administrator attention Unified login or SUL Alternate account Account security Password strength requirements User account security Personal security practices Two-factor authentication 2FA for AWB Committed identity On privacy, confidentiality and discretion Compromised accounts How to not get outed Blocks, bans, sanctions, global actions Blocking policy FAQ Admin's guide Tools Autoblock Appealing a block Guide to appealing blocks UTRS Unblock Ticket Request System Blocking IP addresses Range blocks IPv6 Open proxies Banning policy ArbCom appeals Sanctions Personal sanctions General sanctions Contentious topics and Log Essay Indef \u2260 infinite Long-term abuse Standard offer Global actions Related to accounts Sockpuppetry Single-purpose account Sleeper account Vandalism-only account Wikibreak Enforcer Retiring Courtesy vanishing Clean start Quiet return User groups and global user groups Requests for permissions Admin instructions Admin guide Account creator PERM (Auto) confirmed PERM Autopatrolled PERM AutoWikiBrowser PERM Bot Request Edit filter helper Request Event coordinator PERM Extended confirmed PERM File mover PERM IP block exempt Request Mass message sender PERM New page reviewer PERM Page mover PERM Pending changes reviewer PERM Rollback PERM Template editor PERM Global rights policy Volunteer Response Team Advanced user groups Administrator RfA Bureaucrat RfB CheckUser and Oversight Request Edit filter manager Request Interface administrator Request Founder Importer Researcher Committees and related Arbitration Committee Bot approvals group Functionaries Clerks SPI clerks ArbCom clerks Governance Administration FAQ Formal organization Editorial oversight and control Quality control Wikimedia Foundation Board Founder's seat Meta-Wiki Proposals WikiProjects Elections Policies and guidelines Petitions Noticeboards Consensus Dispute resolution Reforms v t e Wikipedia community For a listing of current collaborations, tasks, and news, see the Community portal . For a listing of ongoing discussions and current requests, see the Dashboard . General community topics Administration News The Signpost Goings-on In the media Meetups Mailing lists Wikipedians Statistics The Wikipedia Library Centralized discussion Village pump Idea lab Policy Proposals Technical Miscellaneous WMF Holidays Bots Contents and grading Requested articles Most-wanted articles Images needing articles Articles needing images Articles for creation WP:AFC/R WP:AFC/C Creating articles Help Vital articles Articles for improvement Peer review Good article nominations Featured article candidates Lists Pictures Topics Article translation Pages Main Page Errors WikiProjects and collaborations Directory Culture and the arts Geographical History and society Science, technology and engineering Wikipedia assistance and tasks Patrols Recent changes Counter-Vandalism Unit Accessibility Organizations category Awards and feedback Reward board Contests A nice cup of tea and a sit down Charitableness WikiLove Compliment before criticism Kindness Campaign Thanks! Maintenance tasks Task Center Open tasks Backlog Category Admin category Edit requests Category Database reports Category tracker Dusty articles Special pages New pages Recent changes Controversial issues Administrators and noticeboards Administrators' noticeboard Incidents Edit warring Vandalism Admin dashboard Admin requests Closure Page protection User permissions Sockpuppets Open proxies Revision deletion Oversight Request Usernames Changing Title blacklist OTRS Bureaucrats' Requests for adminship and bureaucratship Arbitration Committee Requests Enforcement Content dispute resolution Requests for comment Third opinion Dispute resolution noticeboard Biographies of living persons Conflict of interest External links Fringe theories Neutral point of view No original research Reliable sources Other noticeboards and assistance Regional notice boards Requests for help Category Asking\u00a0questions Teahouse Help\u00a0desk Reference\u00a0desk Adopt-a-user Copyright assistance Copyright investigations Text problems Media questions Resource requests Mergers History mergers Moves Page importation Spam Blacklist Whitelist Bots Education General sanctions Editor sanctions Long-term abuse Deletion discussions Guide Admin Today Articles Templates Files Categories Redirects Miscellany Speedy Proposed BLP Review Undeletion Arguments to avoid Arguments to make Article Rescue Elections and voting Requests for comment ( meta ) Wikimedia Foundation elections WP Democracy Milestones Directories, indexes, and summaries Departments Edit summary legend Editor's index Essays FAQs Glossary Abbreviations Help Manual of Style Simplified Rules Five pillars Policies Guidelines Shortcuts Templates Citation templates Tips Today Tools Wiki markup Media Category Templates v t e Wikipedia essays (?) Essays on building, editing, and deleting content Philosophy Article content Articles must be written All Five Pillars are equally important Avoid vague introductions Be a reliable source Civil POV pushing Cohesion Competence is required Concede lost arguments Dissent is not disloyalty Don't lie Don't search for objections Duty to comply Editing Wikipedia is like visiting a foreign country Editors will sometimes be wrong Eight simple rules for editing our encyclopedia Explanationism External criticism of Wikipedia Here to build an encyclopedia Large language models Leave it to the experienced Levels of competence Levels of consensus Most ideas are bad Need Neutrality of sources Not broken is ugly Not editing because of Wikipedia restriction Not every article can be a Featured Article The one question Oversimplification Paradoxes Paraphrasing POV and OR from editors, sources, and fields Process is important Product, process, policy Purpose Reasonability rule Systemic bias There is no seniority Ten Simple Rules for Editing Wikipedia Tendentious editing The role of policies in collaborative anarchy The rules are principles Trifecta We are absolutely here to right great wrongs Wikipedia in brief Wikipedia is an encyclopedia Wikipedia is a community Wikipedia is not RationalWiki Article construction 100K featured articles Abandoned stubs Acronym overkill Adding images improves the encyclopedia Advanced text formatting Akin's Laws of Article Writing Alternatives to the \"Expand\" template Amnesia test A navbox on every page An unfinished house is a real problem Article revisions Articles have a half-life Autosizing images Avoid mission statements Be neutral in form Beef up that first revision Blind men and an elephant BOLD, revert, discuss cycle Build content to endure Cherrypicking Chesterton's fence Children's lit, adult new readers, & large-print books Citation overkill Citation underkill Common-style fallacy Concept cloud Creating controversial content Criticisms of society may be consistent with NPOV and reliability Dictionaries as sources Don't cite Wikipedia on Wikipedia Don't demolish the house while it's still being built Don't get hung up on minor details Don't hope the house will build itself Don't panic Don't \"teach the controversy\" Editing on mobile devices Editors are not mindreaders Encourage the newcomers Endorsements (commercial) Featured articles may have problems Formatting bilateral relations articles Formatting bilateral relations templates Fruit of the poisonous tree Give an article a chance How to write a featured article Identifying and using independent sources History sources Law sources Primary sources Science sources Style guides Tertiary sources Ignore STRONGNAT for date formats Inaccuracy Introduction to structurism Mine a source Merge Test Minors and persons judged incompetent \"Murder of\" articles Not every story/event/disaster needs a biography Not everything needs a navbox Not everything needs a template Nothing is in stone Obtain peer review comments Organizing disambiguation pages by subject area Permastub Potential, not just current state Presentism Principle of Some Astonishment The problem with elegant variation Pro and con lists Printability Publicists Put a little effort into it Restoring part of a reverted edit Robotic editing Sham consensus Source your plot summaries Specialized-style fallacy Stub Makers Run an edit-a-thon Temporary versions of articles Tertiary-source fallacy There are no shortcuts to neutrality There is no deadline There is a deadline The deadline is now Try not to leave it a stub What is a reliable source Understanding Wikipedia's content standards Walled garden What an article should not include Wikipedia is a work in progress Wikipedia is not being written in an organized fashion The world will not end tomorrow Write the article first Writing better articles Writing article content Avoid thread mode Copyediting reception sections Coup Don't throw more litter onto the pile Gender-neutral language Myth vs fiction Proseline Turning biology research into a Wikipedia article Use our own words We shouldn't be able to figure out your opinions Write the article first Writing about women Writing better articles Removing or deleting content Adjectives in your recommendations AfD is not a war zone Arguments to avoid in deletion discussions Arguments to avoid in deletion reviews Arguments to avoid in image deletion discussions Arguments to make in deletion discussions Avoid repeated arguments Before commenting in a deletion discussion But there must be sources! Confusing arguments mean nothing Content removal Counting and sorting are not original research Delete or merge Delete the junk Deletion is not cleanup Does deletion help? Don't attack the nominator Don't confuse stub status with non-notability Don't overuse shortcuts to policy and guidelines to win your argument Emptying categories out of proccess Follow the leader How the presumption of notability works How to save an article proposed for deletion I just don't like it Identifying blatant advertising Identifying test edits Immunity Keep it concise Liar liar pants on fire Nothing Nothing is clear Overzealous deletion Relisting can be abusive Relist bias The Heymann Standard Unopposed AFD discussion Wikipedia is not Whack-A-Mole Why was the page I created deleted? What to do if your article gets tagged for speedy deletion When in doubt, hide it in the woodwork No Encyclopedic Use Essays on civility The basics Accepting other users Apology Being right isn't enough Contributing to complicated discussions Divisiveness Don't retaliate Editors' pronouns Edit at your own pace Encouraging the newcomers Enjoy yourself Expect no thanks High-functioning autism and Asperger's editors How to be civil Maintaining a friendly space Negotiation Obsessive\u2013compulsive disorder editors Please say please Relationships with academic editors Thank you Too long; didn't read Truce Unblock perspectives We are all Wikipedians here You have a right to remain silent Philosophy A thank you never hurts A weak personal attack is still wrong Advice for hotheads An uncivil environment is a poor environment Be the glue Beware of the tigers! Civility warnings Deletion as revenge Duty to comply Failure Forgive and forget It's not the end of the world Nobody cares Most people who disagree with you on content are not vandals Old-fashioned Wikipedian values Profanity, civility, and discussions Revert notification opt-out Shadowless Fists of Death! Staying cool when the editing gets hot The grey zone The last word There is no Divine Right of Editors Most ideas are bad Nothing is clear Reader The rules of polite discourse There is no common sense Two wrongs don't make a right Wikipedia clich\u00e9s Wikipedia is not about winning Wikipedia should not be a monopoly Writing for the opponent Dos Assume good faith Assume the assumption of good faith Assume no clue Avoid personal remarks Avoid the word \"vandal\" Be excellent to one another Be pragmatic Beyond civility Call a spade a spade Candor Deny recognition Desist Discussing cruft Drop the stick and back slowly away from the horse carcass Encourage full discussions Get over it How to lose Imagine others complexly Just drop it Keep it concise Keep it down to earth Mind your own business Say \"MOBY\" Mutual withdrawal Read before commenting Settle the process first You can search, too Don'ts ALPHABETTISPAGHETTI Wikipedia:Because I can Civil POV pushing Cyberbullying Don't accuse someone of a personal attack for accusing of a personal attack Don't be a fanatic Don't be a jerk Don't be an ostrich Don't be ashamed Don't be a WikiBigot Don't be high-maintenance Don't be inconsiderate Don't be obnoxious Don't be prejudiced Don't be rude Don't be the Fun Police Don't bludgeon the process Don't call a spade a spade Don't call people by their real name Don't call the kettle black Don't call things cruft Don't come down like a ton of bricks Don't cry COI Don't demand that editors solve the problems they identify Don't eat the troll's food Don't fight fire with fire Don't give a fuck Don't help too much Don't ignore community consensus Don't knit beside the guillotine Don't make a smarmy valediction part of your signature Don't remind others of past misdeeds Don't shout Don't spite your face Don't take the bait Don't template the regulars Don't throw your toys out of the pram Do not insult the vandals Griefing Hate is disruptive Nationalist editing No angry mastodons just madmen No Nazis No racists No Confederates No queerphobia No, you can't have a pony Passive aggression POV railroad Superhatting There are no oracles There's no need to guess someone's preferred pronouns You can't squeeze blood from a turnip UPPERCASE WikiRelations WikiBullying WikiCrime WikiHarassment WikiHate WikiLawyering WikiLove WikiPeace Essays on notability Advanced source searching All high schools can be notable Alternative outlets Arguments to avoid in deletion discussions Articles with a single source Avoid template creep Bare notability Big events make key participants notable Businesses with a single location But it's true! Common sourcing mistakes Clones Coatrack Discriminate vs indiscriminate information Don't cite GNG Drafts are not checked for notability or sanity Every snowflake is unique Existence \u2260 Notability Existence does not prove notability Extracting the meaning of significant coverage Google searches and numbers How the presumption of notability works High Schools Historical/Policy/Notability/Arguments Inclusion is not an indicator of notability Independent sources Inherent notability Insignificant Masking the lack of notability Make stubs Minimum coverage News coverage does not decrease notability No amount of editing can overcome a lack of notability No big loss No one cares about your garage band No one really cares Notability and tornadoes Notability cannot be purchased Notability comparison test Notability is not a level playing field Notability is not a matter of opinion Notability is not relevance or reliability Notability means impact Notability points Notability sub-pages Notabilitymandering Not every single thing Donald Trump does deserves an article Obscurity \u2260 Lack of notability Offline sources One hundred words One sentence does not an article make Other stuff exists Overreliance upon Google Perennial websites Popularity \u2260 Notability Read the source Reducing consensus to an algorithm Run-of-the-mill Solutions are mixtures and nothing else Significance is not a formula Sources must be out-of-universe Subjective importance Third-party sources Trivial mentions Video links Vanispamcruftisement What BLP1E is not What is and is not routine coverage What notability is not What to include Why is BFDI not on Wikipedia? Wikipedia is not Crunchbase Wikipedia is not here to tell the world about your noble cause Wikipedia is not the place to post your r\u00e9sum\u00e9 Your favourite thing does not need a Wikipedia article Two prongs of merit Humorous essays Adminitis Ain't no rules says a dog can't play basketball Akin's Laws of Article Writing Alternatives to edit warring ANI flu Anti-Wikipedian Anti-Wikipedianism Articlecountitis Asshole John rule Assume bad faith Assume faith Assume good wraith Assume stupidity Assume that everyone's assuming good faith, assuming that you are assuming good faith Avoid using preview button Avoid using wikilinks Bad Jokes and Other Deleted Nonsense Barnstaritis Before they were notable BOLD, revert, revert, revert Boston Tea Party Butterfly effect CaPiTaLiZaTiOn MuCh? Complete bollocks Counting forks Counting juntas Crap Don't stuff beans up your nose Don't-give-a-fuckism Don't abbreviate \"Wikipedia\" as \"Wiki\"! Don't delete the main page Editcountitis Edits Per Day Editsummarisis Editing under the influence Embrace Stop Signs Emerson Fart Five Fs of Wikipedia Seven Ages of Editor, by Will E. Spear-Shake Go ahead, vandalize How many Wikipedians does it take to change a lightbulb? How to get away with UPE How to put up a straight pole by pushing it at an angle How to vandalize correctly How to win a citation war Ignore all essays Ignore every single rule Is that even an essay? Mess with the templates My local pond Newcomers are delicious, so go ahead and bite them Legal vandalism List of jokes about Wikipedia LTTAUTMAOK No climbing the Reichstag dressed as Spider-Man No episcopal threats No one cares about your garage band No one really cares No, really Notability is not eternal Oops Defense Play the game Please be a giant dick, so we can ban you Please bite the newbies Please do not murder the newcomers Pledge of Tranquility R-e-s-p-e-c-t Requests for medication Requirements for adminship Rouge admin Rouge editor Sarcasm is really helpful Sausages for tasting Template madness The Night Before Wikimas The first rule of Wikipedia The Five Pillars of Untruth Things that should not be surprising The WikiBible Watchlistitis We are deletionist! What's a forint? Wikipedia is an MMORPG Yes legal threats You don't have to be mad to work here, but You should not write meaningless lists About essays About essays Essay guide Value of essays Difference between policies, guidelines and essays Don't cite essays as if they were policy Avoid writing redundant essays Finding an essay Quote your own essay Policies and guidelines About policies and guidelines Policies Guidelines How to contribute to Wikipedia guidance Policy writing is hard v t e Wikipedia help pages Visit the Teahouse or the Help desk for an interactive Q & A forum. FAQs (?) Reference desks (?) Noticeboards (?) Cheatsheet (?) Directories (?) About Wikipedia (?) Administration Purpose Principles Policies and guidelines What Wikipedia is not Disclaimer ( parental advice ) Making requests Who writes Wikipedia? Help for readers (?) FAQ Books Copyright Glossary Mobile access Navigation Other languages Searching Students Viewing media Contributing to Wikipedia (?) Advice for young editors Avoiding common mistakes Etiquette Simplified Manual of Style Simplified rule-set \"Ignore all rules\" \"The rules are principles\" Style-tips Tip of the day Your first article ( article wizard ) Getting started (?) Why create an account? Introductions by topic Graphics tutorials Picture tutorial IRC (live chat) tutorial VisualEditor user guide Dos and don'ts (?) Accessibility Biographies Biographies (living) Categorization Consensus Discussions Disambiguation Images Leads Links Lists References Tables Titles (of articles) How-to pages and information pages (?) Appealing blocks Article deletion Categories Citations/references Referencing for beginners Citation Style 1 Cite errors References and page numbers Convert Diff Editing Minor edit toolbar edit conflict Find sources Files Footnotes Image deletion Infoboxes Linking ( link color ) Logging in Merging New page review Page name Renaming pages Redirect Passwords Email confirmation Reverting Simple vandalism cleanup Talk pages ( archiving simple archiving ) User contributions WP search protocol Coding (?) Wiki markup Barcharts Calculations Characters Columns Elevation Hidden text HTML Lists Magic words Music symbols Sections Sounds Tables Templates Transclusion URL Visual files Directories (?) Abbreviations Contents (Encyclopedia proper) Departments Editor's index Essays FAQs Glossary Guidelines Manual of Style Policies Tasks Tips Tools Missing Manual Ask for help on your talk page (?) v t e Wikipedia referencing Policies and guidelines Verifiability No original research Biographies of living persons Reliable sources Medicine Citing sources Scientific citations General advice Citation needed Combining sources Offline sources Referencing styles Citing sources Citation Style 1 Citation Style 2 Citation Style Vancouver Bluebook Comics Citation templates Inline citations Footnotes Punctuation and footnotes Shortened footnotes Nesting footnotes Help for beginners Reference-tags Citations quick reference Introduction to referencing Referencing with citation templates Referencing without using templates Referencing dos and don'ts Citing Wikipedia Advanced help Cite link labels Cite errors Citation merging (bundling) Cite messages Converting between references formats Reference display customization References and page numbers Guidance on source reviewing at FAC Footnote templates Citation Style documentation Multiple references {{ Reflist }} {{ Refbegin }} Find references How to find sources Bibliographies Wikipedia Library Resource Exchange Reference Desk Book Sources Free newspaper sources Citation tools (External links) Citer Biomedical cite Citation bot MakeRef Refill WayBack OABot v t e Wikipedia technical help Get personal technical help at the Teahouse , help desk , village pump (technical) , talk pages or IRC . General technical help Browser notes Bypass cache Keyboard shortcuts Editing CharInsert Edit conflict Edit toolbar Reverting How to create a page IRC Tutorial Mobile access Multilingual support Page history Page information Page name Help Printing Software notices Editnotice Special Characters Entering User access levels VisualEditor Help Special page -related Special page help AllPages Edit filter Emailing users Logging in Reset passwords Logs Moving a page History merging Non-admin and admin-only page moves Notifications/Echo FAQ Page Curation Page import Pending changes Random pages Recent changes Related changes Searching Linksearch Tags User contributions Watchlist What links here Wikitext Wikitext Cheatsheet Columns Line-break handling Lists Magic words For beginners Conditional expressions Switch parser function Time function Redirects Sections and TOCs Tables Introduction Basics Advanced table formatting Collapsing Conditional tables Sortable tables Using colours Links and diffs Links Interlanguage Interwiki Permanent Diffs Simplest diff guide Simple diff and link guide Complete diff and link guide Colon trick Link color Pipe trick URLs Media files: images, videos and sounds Media help Files Creation and usage Moving files to Commons Images Introduction to images Picture tutorial Preparing images for upload Uploading images Options to hide an image Extended image syntax SVG help Gallery tag Graphics tutorials Basic bitmap image editing How to improve image quality Graphics Lab resources Sound file markup Visual file markup Other graphics Family trees Graphs and charts How to create Barcharts To scale charts Math formulas Math symbols Musical scores Musical symbols Timeline EasyTimeline syntax WikiHiero syntax Templates and Lua modules Templates Advanced template coding Template documentation Template index Template limits Template sandbox and test cases Citation templates Lua help Lua project Resources To do Substitution Purge Job queue Transclusion Labeled section Costs and benefits Guide to Scribbling Data structure Namespaces Main/Article Category Draft File File description page Help Portal Project/Wikipedia Talk Archiving Simple Template User User page design MediaWiki Bug reports and feature requests TimedMediaHandler extension Module Special HTML and CSS Cascading Style Sheets HTML in wikitext Catalogue of CSS classes Common.js and common.css User CSS for mono spaced coding font Classes in microformats Markup validation Span tags Useful styles Customisation and tools Preferences Gadgets Skins Citation tools Cleaning up vandalism tools Customizing watchlists Hide pages IRC Scripts User scripts Guide List Techniques User style Tools Alternative browsing Browser tools Editing tools Navigation shortcuts Optimum tool set Wikimedia Cloud Services Beta Features at MediaWiki Automated editing AfC helper script AntiVandal AutoWikiBrowser Bots Creating history HotCat Huggle Navigation popups RedWarn Twinkle Ultraviolet WPCleaner Inactive igloo STiki See also: Category:Wikipedia how-to Category:Wikipedia information pages Further navigation at: Help pages Administrators Accessibility Accounts Bots Referencing Citation metadata Templates User scripts v t e Wikipedia templates Main namespace General Cleanup Verifiability and sources Disputes Hatnotes Infoboxes Links External link templates Linking country articles Lists Main page Section Sources of articles Quick reference Standard boxes Stub types Translation Other namespaces Compact TOC Category File Talk Template User Userboxes User talk Wikipedia WikiProject banners All namespaces Deletion Speedy Formatting Maintenance Merging Moving Requested Navigation Redirect pages Functional index Language codes Splitting Wikimedia sister projects Navboxes with templates Archive Articles for deletion Birth, death and age Button Category header Citation and verifiability Citation Style 1 Deletion review Editnotice Hatnotes Help desk Inline cleanup Introduction cleanup IPA Math Notice and warnings Organization infoboxes Proposed article mergers Protection Quotation Redirects Search Semantics Speedy deletion Notices String-handling Sup and sub-related Top icon Transwiki maintenance Unicode User talk pages User noticeboard notices Userboxes User rights Userspace linking Userspace Disclaimers Wikibreak WikiLove Inline images Wikipedia icons Discussion icons Comment icons Emoji Help pages Template documentation Examples of templates Maintenance template removal Requested templates Template help Quick guide Template namespace Related topics WikiProject Templates Index Category Search Category v t e Awards, decorations, and medals of Wikipedia Awarded by co-founder Jimmy Wales Order of the Day Wikimedian of the Year also by country Awards by WikiProject WikiCup Editor of the Week W Award Four Award Triple Crown Million Award Impact \u2022 Precious Barnstars and other personal awards Barnstar awards by topic Personal user awards Personal greetings and cheers Awards by country 2.0 Ribbons Awards by number of edits Service awards Incremental service awards Administrative service awards See also WikiLove Thanks! Reward board Contests Merchandise giveaways WikiProject Wikipedia Awards Kindness Campaign v t e Useful links Daily pages CAT:HELP WP:ANI WP:BLPN WP:BOTN WP:COIN WP:DASH WP:FTN WP:HD WP:MCQ WP:NORN WP:NPOVN WP:PUMP ( /A /M /P /R /T ) WP:RD ( /C /E /H /L /M /MA /S ) WP:RFC ( /MEDIA /BIO /ECON /HIST /LANG /NAME /PAG /PHIL /POLY /SCI /SOC /STYLE ) WP:RFF WP:RSN WP:SPI WP:THIRD WP:DRV ) Things to do CAT:BACKLOG CAT:M CAT:WRI WP:AFC WP:ASOF WP:DPWL WP:FAC WP:FAR WP:FPC WP:GAN WP:GAR WP:KIND WP:MISSING WP:PR WP:RD WP:RP WP:SPOKEN WP:UW WP:THQ WP:VITAL WP:WANTED WP:WC BrokenRedirects Disambigs LongPages NewPages OrphanPages TagsPages UncatImages UncatPages Resources CAT:HOWTO WP:AWARD WP:CATEGORY WP:CAT-R WP:CITE WP:CITET WP:COPYEDIT WP:CUTPASTE WP:CUV WP:DEPT WP:DFD WP:DISAMBIG WP:EDIT WP:FOOT WP:FORMULA WP:HAT WP:ICT WP:LAYOUT WP:LIBRARY WP:MAGIC WP:MERGE WP:PIFU WP:PRECEDENT WP:REDIRECT WP:STYLE WP:SUMMARY WP:TEMPLATE CatTree PrefixIndex Policies / Guidelines WP:3RR WP:ADMIN WP:AP WP:APPEAL WP:ATTACK WP:AUTOBIO WP:BAN WP:BLANK WP:BLOCK WP:BOLD WP:BOTPOL WP:BUILD WP:CANVASS WP:CFORK WP:CIVIL WP:CONFLICT WP:CONSENSUS WP:COPYRIGHT WP:DELETE WP:DISCLAIM WP:DISPUTE WP:DISRUPTIVE WP:DONTBITE WP:DPR WP:EP WP:ETIQ WP:EDITWAR WP:FRINGE WP:GAME WP:GOOGLE WP:HARASS WP:HOAX WP:IGNORE WP:IUP WP:LEAD WP:LEGAL WP:LIBEL WP:LINKS WP:LIST WP:LIVING WP:LOGOS WP:M WP:NAME WP:NONFREE WP:NONSENSE WP:NOT WP:NOTE WP:NPOV WP:NPS WP:OFFICE WP:ORIGINAL WP:OVERSIGHT WP:OWN WP:PACL WP:PD WP:PEACOCK WP:PERFORM WP:POINT WP:POLLING WP:PROD WP:PROTECT WP:RELIABLE WP:SELF WP:SOCK WP:SPAM WP:SPEEDY WP:STUB WP:TALK WP:TP WP:TRIVIA WP:USERBOX WP:USERNAME WP:USERPAGE WP:VANDAL WP:VANISH WP:VERIFY WP:WEASEL WP:WHEEL Administrators CAT:AB CAT:CSD CAT:PER CAT:RFU WP:AE WP:AIV WP:AN WP:ANEW WP:ARL WP:CP WP:DGFA WP:NAS WP:OP WP:RFA WP:RFARB WP:RFM WP:RFPP WP:RM WP:RSPAM WP:UAA BlockLog DelLog ProtectLog External links Article statistics Edit Counter FIST ISBNdb Make Reference Wikichecker WikiEN-l archives Retrieved from \" https://en.wikipedia.org/w/index.php?title=Wikipedia:Community_portal&oldid=1271744263 \" Categories : Wikipedia basic information Wikipedia discussion Wikipedia directories Wikipedia news Wikipedia noticeboards Hidden categories: Wikipedia semi-protected project pages Wikipedia move-protected project pages This page was last edited on 25 January 2025, at 13:45 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Wikipedia : Community portal 218 languages Add topic"}, "11": {"url": "https://en.wikipedia.org/wiki/Special:RecentChanges", "text": "Recent changes - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Languages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Recent changes Help English Tools Tools move to sidebar hide Actions General Atom Upload file Printable version Get shortened URL Download QR code In other projects Appearance move to sidebar hide This is a list of recent changes to Wikipedia. Other review tools This page : Discuss this page \u2013 What does this page mean? Recent changes for: Featured articles \u2013 Good articles \u2013 Living people \u2013 Vital articles Utilities: RC patrol \u2013 New pages \u2013 IPs' contribs \u2013 Mobile contribs \u2013 Edit filter log \u2013 Cleanup \u2013 Vandalism \u2013 Deletion \u2013 RfC \u2013 Backlogs About Wikipedia : Introduction / FAQ / Policy \u2013 Stats \u2013 News \u2013 Village pump \u2013 Mailing lists \u2013 Chat \u2013 Wikipedia Signpost Recent changes options Show last 50 | 100 | 250 | 500 changes in last 1 | 3 | 7 | 14 | 30 days Hide registered users | Hide unregistered users | Hide my edits | Show bots | Hide minor edits | Show page categorization | Show Wikidata | Hide probably good edits Show new changes starting from 10:31, 17 May 2025 Namespace: all (Article) Talk User User talk Wikipedia Wikipedia talk File File talk MediaWiki MediaWiki talk Template Template talk Help Help talk Category Category talk Portal Portal talk Draft Draft talk MOS MOS talk TimedText TimedText talk Module Module talk Event Event talk Invert selection Associated namespace Tag filter: 2017 wikitext editor ABBA [1.0] Addition of protection template to non-protected page Advanced mobile edit Android app edit AntiVandal App AI assist App description add App description change App description translate App full source App image add infobox App image add top App rollback App section source App select source App suggested edit App talk reply App talk source App talk topic App undo Automatic insertion of extraneous formatting AWB Barnsworth [1.0] Blanking blanking Bot in trial bup 2 [1.0] campaign-external-machine-translation canned edit summary categories removed CD Change of the mentor list changing height and/or weight changing time or duration Citation bot [1.1.0] Citation Bot [1.2.0] citing a blog or free web host COI template removed Community Configuration condition limit reached content model change content sourced to vanity press contentious topics alert ContentTranslation contenttranslation-needcheck ContentTranslation2 ContentTranslation: High unmodified machine translation usage Contest or editathon copyright violation template removed Correct typos in one click [1.0] Correct typos in one click [1.1] CropTool [1.2] CropTool [1.4] CropTool [1.5] CropTool testing [1.0] DAB Mechanic [1.0] dashboard-testing.wikiedu.org [1.0] dashboard-testing.wikiedu.org [2.0] dashboard.wikiedu.org [1.0] dashboard.wikiedu.org [1.2] dashboard.wikiedu.org [2.0] dashboard.wikiedu.org [2.2] dashboard.wikiedu.org [2.3] dashboard.wikiedu.org account-creation dev [1.0] dashboard.wikiedu.org dev [0.4.11] dashboard.wikiedu.org dev [0.4.2] dashboard.wikiedu.org dev [0.4.3] dashboard.wikiedu.org dev [0.4.4] dashboard.wikiedu.org dev [0.4.5] dashboard.wikiedu.org dev [0.4.7] dashboard.wikiedu.org dev [0.4.8] dashboard.wikiedu.org dev [0.4] dashboard.wikiedu.org dev [2.0] dashboard.wikiedu.org dev [2.1] demo-oauthratelimiter-3 [1.0] Deputy DiBabel [1.2] Disambiguation links added disambiguation template removed discussiontools (hidden tag) discussiontools-added-comment (hidden tag) discussiontools-source (hidden tag) discussiontools-source-enhanced (hidden tag) discussiontools-visual (hidden tag) Dispenser [1.0] Dispenser [2.4] Downstream Pull [0.1] draft or userpage link Edit Check (references) activated Edit Check (references) declined (irrelevant) Edit Check (references) declined (other) Edit Check (references) declined (uncertain) editcheck-newcontent (hidden tag) editcheck-newreference (hidden tag) editcheck-references (hidden tag) editProtectedHelper Education Dashboard development - awight [1.0] English Wikipedia Account Creation Assistance Tool [2.0] excessive whitespace External link added to disambiguation page extraneous markup featured article or good article template added or removed Find link [1.0] Fixed lint errors Fountain [0.1.3] Fountain Test [1.1] gabinaluz+app@gmail.com [1.0] harv or sfn error Huggle IABot [1.0] IABotManagementConsole (Personal Debug Consumer) [1.0] IABotManagementConsole [1.1] IABotManagementConsole [1.2] IABotManagementConsole [1.3] image template modification Incorrectly formatted external link or image Invalid TimedText edit iOS app edit Jonathan_at_WINTR_7 [1.0] JWB KenBurnsEffect tool [1.1] large non-free file large plot addition large unwikified new article LCA Tools [1.0] LCA Tools [1.5] Localhost [1.0] Manual revert Mass pronoun change MassMessage delivery massmove Medha_Bansal_intern_at_WikiEduDashboard [1.0] Mentorship module question Mentorship panel question missing file added missingredirectsproject [1.0] Mobile app edit Mobile edit Mobile web edit Modified by FileImporter moveToDraft MTC-Web for local dev/testing [1.0] MTCWeb-Dev [1.0] New redirect New topic New user adding protection template new user modifying archives new user moving page out of userspace Newcomer task Newcomer task: copyedit Newcomer task: expand Newcomer task: links Newcomer task: references Newcomer task: update Non-autoconfirmed user rapidly reverting edits non-English content nowiki added Nuke OAbot [1.0] OAbot [2.1] OAuth Uploader [1.0] outreachdashboard.wmflabs.org [2.0] outreachdashboard.wmflabs.org [2.1] pageswap GUI PageTriage Parliament edits PAWS [1.2] PAWS [2.1] paws [2.2] possible autobiography or conflict of interest possible birth or death date change possible BLP issue or vandalism possible conflict of interest possible cut and paste move or recreation possible MOS:ETHNICITY violation Possible self promotion in user or draftspace Possible self promotion in userspace possible unreferenced addition to BLP possible userspace spam possible vandalism possible WP:BLPCRIME issue possibly inaccurate edit summary ProveIt edit QuickCategories [1.0] QuickCategories [1.1] rapid date format changes Recreated Redirect target changed reference list removal references removed removal of Category:Living People Removed redirect repeated addition of external links by non-autoconfirmed user Replaced Reply Reverted reverting anti-vandal bot review edit Rollback RW section blanking SectionTranslation self-renaming or bad user talk move Shortdesc helper shouting Snuggle (English Wikipedia) [1.0.0] speedy deletion template removed Suggested: add links Suggestor [0.0.1] SWViewer [1.0] SWViewer [1.2] SWViewer [1.3] SWViewer [1.4] SWViewer [1.6] Takedown Tools [1.0] Talk banner shell conversion talk page blanking Text added at end of page TorProxy [0.1] TorProxy [0.2] Twinkle U.S. Congress edit Ultraviolet Undo Unexpected #REDIRECT markup unsourced AFC submission use of deprecated (unreliable) source use of predatory open access journal User Analysis Tool [1.0] very short new article Visual edit Visual edit: Check Visual edit: Switched Weekipedia v3 [1.0] Weekipedia2 [1.0] wiki-file-transfer [1.0] wikieditor (hidden tag) wikiedu Assignment Wizard 2.0 Teting [1.0] wikiedu Assignment Wizard [1.0] wikiedu.org Assignment Design Wizard (testing) [0.99] wikiedu.org Assignment Design Wizard [1.0.1] wikiedu.org Assignment Design Wizard [1.0.2] WikiEduDashboard NTDB [1.2] WikiEduDashboard NTDB3 [1.0] WikiEduDashboard NTDB4 [1.0] WikiEduDashboard NTDB5 [1.0] WikiEduWizard NTDB [1.2] Wikifile Transfer [1.0] Wikifile Transfer [2.0] Wikifile Transfer [4.0] Wikifile Transfer [5.0] WikiLeaks wikilinks removed WikiLoop Battlefield WikiLoop Battlefield Dev Local [2.0.0] WikiLoop Battlefield on WMF Cloud VPS [1.0] WikiLoop Battlefield Prod [2.2.1-beta] WikiLoop DoubleCheck WMF Cloud VPS (2020-07-13 version) [4.1.0] WikiLove WINTR Wikiedu Dashboard Local Test 3 [1.0] WINTR Wikiedu Wizard Local Test [1.0] WPCleaner yabbr [1.3] Invert selection List of abbreviations ( help ): D Edit made at Wiki d ata r Edit flagged by O R ES N N ew page m M inor edit b B ot edit (\u00b1123) Page byte size change Temporarily watched page 17 May 2025 diff hist User:Stebb/sandbox 10:31 +54 Stebb talk contribs ( \u2192 Personal life ) diff hist Mesocapromys 10:30 +34 Ubs6u!d-pongsakorn talk contribs (Map) Tags : Visual edit Mobile edit Mobile web edit diff hist Category:1998 elections in S\u00f6dermanland County 10:30 +41 Kaffet i halsen talk contribs ({{Sweden by county category navigation}}) diff hist Sydney Airport 10:30 +415 119.18.0.29 talk (added rex and pelican terminal change) Tag : Visual edit diff hist Democratic Party of Albania 10:30 0 Mirditor22 talk contribs Tags : Mobile edit Mobile web edit diff hist Violent Summer 10:30 +22 Cavarrone talk contribs (French title) diff hist Talk:Orazio Alfani 10:30 +65 Simeon talk contribs ( WikiProject tagging) Tag : 2017 wikitext editor diff hist Aix-Marseille University 10:30 \u22125 2a00:23c6:982f:fb01:10db:70db:941:4fe2 talk ( \u2192 Medicine ) Tag : Manual revert diff hist Anbe Vaa (1966 film) 10:30 \u221230 223.178.84.238 talk Tags : Visual edit Mobile edit Mobile web edit diff hist Operation Herof 2.0 10:30 +838 AE182 talk contribs ( \u2192 May 15 ) diff hist m MP Muar F.C. 10:30 \u22124 Aadirulez8 talk contribs (v2.05 - Auto / Fix errors for CW project (Template value ends with break)) Tag : WPCleaner diff hist m La Fortaleza 10:30 +22 Tektonson talk contribs (Added wiki link) Tags : Visual edit Mobile edit Mobile web edit diff hist Imperial Business School 10:30 +129 Mikecurry1 talk contribs Tag : Visual edit diff hist Aselsan SMASH 10:30 +1 Kistara talk contribs ( \u2192 Operators ) User creation log 10:30 User account OKOT PATRICK P'MINONYANG talk contribs was created diff hist Johnny Lockwood 10:30 +186 122.106.2.164 talk diff hist Pat Butcher 10:30 +12 2a00:23c5:733:8001:1cf3:36ea:b6f1:b864 talk Tag : Visual edit diff hist Baring baronets 10:30 +306 Charles Matthews talk contribs ( \u2192 Baring baronets, of Larkbeer (1793) : refs) diff hist 2024\u201325 Hibernian F.C. season 10:30 \u221214 Jmorrison230582 talk contribs (linked 2025/26) diff hist Draft:Kanhoji Angre's War in Konkan 10:30 +70 SolarSyntax talk contribs Tags : Mobile edit Mobile web edit Advanced mobile edit diff hist Shymkent 10:30 0 113.185.46.96 talk Tags : Mobile edit Mobile web edit diff hist Just Add Magic (TV series) 10:30 +50 110.175.34.40 talk ( \u2192 Part 1 ) diff hist User talk:JBW 10:30 +154 JBW talk contribs ( \u2192 User:Abjrk : deleted) diff hist Eden Lake 10:30 +33 OnTheMountainTop talk contribs ( \u2192 Plot : reworded summary to describe plot events in order of occurrence) diff hist Tina Beaudry-Mellor 10:30 +59 172.100.106.166 talk Tags : Mobile edit Mobile web edit diff hist Real Malabar F.C 10:30 \u221223 Editorofindianfootball talk contribs ( \u2190 Changed redirect target from List of football clubs in India#Kerala to Real Malabar FC ) Tags : Redirect target changed Manual revert Mobile edit Mobile web edit diff hist Denpasar 10:30 +2 Indonesianinfo2 talk contribs Tags : Mobile edit Mobile web edit diff hist Commonwealth Offices Building, Melbourne 10:30 +100 Mairremena talk contribs ( \u2192 Further reading : Architecture categories) diff hist Miss Supranational 2025 10:30 +2 84.248.44.184 talk ( \u2192 Contestants ) Tag : Visual edit User creation log 10:30 User account Aagan talk contribs was created Tags : Mobile edit Mobile web edit diff hist Scarlett Bordeaux 10:30 \u221259 Mann Mann talk contribs ( \u2192 External links : Redundant. WWE superstar is a part of Professional wrestling profiles now.) diff hist Ian Rush 10:30 +348 Bakermannn talk contribs ( \u2192 Club ) diff hist Kodigehalli, Bengaluru East 10:30 +124 Vanderwaalforces talk contribs (Requesting copyvio revdel ( cv-revdel )) diff hist Archie Panjabi 10:30 \u2212282 Sciencefish talk contribs (rm WP:FANCRUFT , Removing unsourced content) diff hist m Draft:Hazel Smith (writer) 10:30 \u2212293 Timtrent talk contribs (clean up ( DraftCleaner )) diff hist N Mission South Africa 10:30 +49 Skitash talk contribs ( \u2190 Redirected page to White South African refugee program ) Tags : New redirect Visual edit: Switched diff hist Study Press 10:30 +53 2607:f710:72::13a7:0:1 talk ( Jiang Zemin was then General Secretary of the Chinese Communist Party in 1993.) diff hist Christian Bale filmography 10:30 +35 199.71.169.110 talk (Died) diff hist Hugh Roe O'Donnell 10:30 +53 SkywalkerEccleston talk contribs ( \u2192 Succession ) Tag : Visual edit diff hist Vijay Jacob 10:30 +32 2409:4073:4e1d:3af9::ff88:c214 talk ( \u2192 As an Actor ) Tags : Mobile edit Mobile web edit diff hist Gavin Gordon (rugby league) 10:30 +1 2a02:c7c:6230:b400:f8d6:19d3:38c2:536f talk (Fixed typo) Tags : changing height and/or weight Mobile edit Mobile web edit diff hist Mullane v. Central Hanover Bank & Trust Co. 10:30 +7 DocWatson42 talk contribs ( \u2192 External links : Performed minor cleanup.) Tags : Mobile edit Mobile web edit Advanced mobile edit diff hist Category:Actors from Sk\u00e5ne County 10:30 +41 Kaffet i halsen talk contribs ({{Sweden by county category navigation}}) User rename log 10:30 Malarz pl talk contribs renamed user Feniks Heron (0 edits) to Volleyboy1977 (per request ) diff hist User:Curb Safe Charmer/AfC log 10:30 +99 Curb Safe Charmer talk contribs (Logging decline of Draft:Grigol Kuntchulia ( AFCH )) diff hist N User talk:KGrigol 10:30 +4,038 Curb Safe Charmer talk contribs (Notification: Your Articles for Creation submission has been declined ( AFCH )) diff hist Aviators Square (Szczecin) 10:30 +39 Artemis Andromeda talk contribs diff hist Telephone numbers in Europe 10:30 0 Upset New Bird talk contribs ( \u2192 European Economic Area ) diff hist Draft:Grigol Kuntchulia 10:30 +181 Curb Safe Charmer talk contribs (Declining submission: bio - Submission is about a person not yet shown to meet notability guidelines ( AFCH )) diff hist Isack Hadjar 10:30 \u2212543 M.Bitton talk contribs (Reverted 12 edits by French Thutmose III ( talk ): Not what the source says) Tags : Twinkle Undo Retrieved from \" https://en.wikipedia.org/wiki/Special:RecentChanges \" Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Recent changes Add topic"}, "12": {"url": "https://en.wikipedia.org/wiki/Wikipedia:File_upload_wizard", "text": "Wikipedia:File upload wizard - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Wikipedia : File upload wizard 63 languages \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u0985\u09b8\u09ae\u09c0\u09af\u09bc\u09be Az\u0259rbaycanca \u062a\u06c6\u0631\u06a9\u062c\u0647 \u09ac\u09be\u0982\u09b2\u09be \u95a9\u5357\u8a9e / B\u00e2n-l\u00e2m-g\u00fa \u092d\u094b\u091c\u092a\u0941\u0930\u0940 Bosanski ChiShona Corsu \u0627\u0644\u062f\u0627\u0631\u062c\u0629 \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Esperanto \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais Gagauz \u8d1b\u8a9e \ud55c\uad6d\uc5b4 Hawai\u02bbi \u0939\u093f\u0928\u094d\u0926\u0940 Hrvatski Bahasa Indonesia \u00cdslenska Italiano \u049a\u0430\u0437\u0430\u049b\u0448\u0430 Ladin Magyar \u041c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438 \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02 \u092e\u0930\u093e\u0920\u0940 \u0645\u0627\u0632\u0650\u0631\u0648\u0646\u06cc Bahasa Melayu \u1019\u103c\u1014\u103a\u1019\u102c\u1018\u102c\u101e\u102c Nederlands \u0928\u0947\u092a\u093e\u0932\u0940 \u65e5\u672c\u8a9e \u041d\u043e\u0445\u0447\u0438\u0439\u043d O\u02bbzbekcha / \u045e\u0437\u0431\u0435\u043a\u0447\u0430 \u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40 \u067e\u069a\u062a\u0648 Polski Portugu\u00eas Qaraqalpaqsha Rom\u00e2n\u0103 \u0dc3\u0dd2\u0d82\u0dc4\u0dbd Sloven\u0161\u010dina Soomaaliga \u06a9\u0648\u0631\u062f\u06cc \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Srpskohrvatski / \u0441\u0440\u043f\u0441\u043a\u043e\u0445\u0440\u0432\u0430\u0442\u0441\u043a\u0438 Sunda Tagalog \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41 \u0e44\u0e17\u0e22 \u0422\u043e\u04b7\u0438\u043a\u04e3 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u0627\u0631\u062f\u0648 Ti\u1ebfng Vi\u1ec7t \u7cb5\u8a9e \u4e2d\u6587 \u2d5c\u2d30\u2d4e\u2d30\u2d63\u2d49\u2d56\u2d5c \u2d5c\u2d30\u2d4f\u2d30\u2d61\u2d30\u2d62\u2d5c Edit links Project page Talk English Read View source View history Tools Tools move to sidebar hide Actions Read View source View history General What links here Related changes Upload file Permanent link Page information Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikimedia Commons Wikiversity Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Wikimedia file upload wizard Shortcut WP:FUW Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand copyright and the image use policy before proceeding. Upload your own or a freely licensed file Uploads to Wikimedia Commons Upload a non-free file Uploads locally to the English Wikipedia; must comply with the non-free content criteria You do not have JavaScript enabled Sorry, in order to use this uploading script, JavaScript must be enabled. You can still use the plain Special:Upload page to upload files to the English Wikipedia without JavaScript. You are not currently logged in. Sorry, in order to use this uploading script and to upload files, you need to be logged in with your named account. Please log in and then try again. Your account has not become confirmed yet. Sorry, in order to upload files on the English Wikipedia, you need to have a confirmed account . Normally, your account will become confirmed automatically once you have made 10 edits and four days have passed since you created it. You may already be able to upload files on the Wikimedia Commons , but you can't do it on the English Wikipedia just yet. If the file you want to upload has a free license, please go to Commons and upload it there. Important note: if you don't want to wait until you are autoconfirmed, you may ask somebody else to upload a file for you at Wikipedia:Files for upload . In very rare cases an administrator may make your account confirmed manually through a request at Wikipedia:Requests for permissions/Confirmed . Step 1: Choose your file File: Choose a file from your computer. Maximum file size: 100 MB. Permitted file types: png, gif, jpg, jpeg, xcf, pdf, mid, mp3, mpeg, mpg, ogg, ogv, opus, svg, djvu, tiff, tif, oga, flac, wav, wave, webm, webp. Step 2: Describe your file Please provide a clear, descriptive name by which your file will be known on Wikipedia. This name must be unique across the whole of Wikipedia, so please make it informative and easy to recognize. It's no problem to use a fairly long name. It may also include spaces, commas and most other punctuation marks. Please also note that file names are case sensitive (with the exception of the first letter). Good: \"City of London, skyline from London City Hall, Oct 2008.jpg\". Bad : \"Skyline.jpg\", \"DSC0001234.jpg\". Sorry, a few special characters and character combinations cannot be used in the filename for technical reasons. This goes especially for #\u00a0<\u00a0>\u00a0[\u00a0]\u00a0|\u00a0:\u00a0{\u00a0}\u00a0/ and ~~~ . Your filename has been modified to avoid these. Please check if it is okay now. The filename you chose seems to be very short, or overly generic. Please don't use: Titles that consist only of very generic descriptive words (e.g. \"Sunset.jpg\", \"Townhall.jpg\") Titles that consist only of a person's first or last name, when that name is likely to be shared by many others (e.g. \"John.jpg\", \"Miller.jpg\") Titles that consist of mere numbers, of the kind often produced by digital cameras (\"DSC_001234\", \"IMGP0345\"), or random strings like those sometimes found on the web (\"30996951316264l.jpg\") Existing file . Last uploaded by Example user . A file of this name already exists on Commons! If you upload your file with this name, you will be masking the existing file and make it inaccessible. Your new file will be displayed everywhere the existing file was previously used. This should not be done, except in very rare exceptional cases. Please don't upload your file under this name, unless you seriously know what you are doing. Choose a different name for your new file instead. If you want to replace the existing file with an uncontroversial, improved version of the same work, please go to Commons and upload it there, not here on the English Wikipedia's local wiki. A file of this name already exists. If you upload your file with this name, you will be overwriting the existing file. Your new file will be displayed everywhere the existing file was previously used. Please don't do this, unless you have a good reason to: No , I don't want to overwrite the existing file. I will choose a different name for my new file instead. Yes , I want to overwrite the existing file. My file is merely a new, improved and uncontentious version of the same work. The old description page, including the source and copyright information, will still be correct for the new version and can remain the same. Yes , I want to overwrite the existing file, and I will use this wizard to add a new description and new source information for it. The previous version was my own, or else I have made sure the previous uploader(s) don't object to this replacement. Please provide a brief description of the contents of this file. This will be stored and displayed as part of the file description page. It is important that other editors are able to understand what this file is about. What does this file show? What is it a photo/diagram/recording of? It will be helpful if you also add a wikilink to the article where you want to use it. Step 3: Provide source and copyright information It is very important that you read through the following options and questions, and provide all required information truthfully and carefully. This is a free work. I can demonstrate that it is legally okay for anybody to use, in Wikipedia and elsewhere, for any purpose. Thank you for offering to upload a free work. Wikipedia loves free files. However, we would love it even more if you uploaded them on our sister project, Wikimedia Commons .\nFiles uploaded on Commons can be used immediately here on Wikipedia as well as on all its sister projects. Uploading files on Commons works just the same as here. Your Wikipedia account will automatically work on Commons too. Please consider uploading your file on Commons . However, if you prefer to do it here instead, you may go ahead with this form. You can also first use this form to collect the information about your file and then send it to Commons from here. Copyright status: This file is entirely my own work. I am the copyright holder. I made this myself, from scratch, without copying or incorporating anybody else's creative work, and I am willing to release it under a free license. Please note that by \"entirely self-made\" we really mean just that. Do not use this section for any of the following: a scan or photograph you made of a painting, drawing, printed page or other item originally created by somebody else. The copyright belongs to the original creator, not to you. a screenshot or other kind of capture of a video, computer screen, TV programme or other kind of visual media. a picture you created by modifying or copying some other picture or by combining several preexisting pictures made by somebody else. a picture given to you by somebody else. a picture you found somewhere on the Internet. Editors who falsely declare such items as their \"own work\" will be blocked from editing . Please describe how and when you created this item. How? (e.g.: Where and on what kind of occasion did you take this photo? How did you make this diagram? etc.) Date (please use YYYY-MM-DD format if possible). Publication: Please indicate here if you have previously published this item elsewhere, e.g. on your own website, your Flickr or Facebook account, etc., providing a link. It is important that you place this work under a free license, which will allow everybody else to use it for any purpose, including both commercial and non-commercial purposes, and to modify it. This license will be irrevocable. This file was given to me by its owner. The copyright owner of this file has given it to me for uploading on Wikipedia. I can provide evidence that they have agreed to release it under a free license, for free use by anybody and for any purpose. Please describe who owns this work and how you got it from them. Owner/author: Date of creation: Source: Where did you get the file from? (e.g. available online; was sent to me personally \u2026) Permission: How did you receive the permission? (e.g.: by e-mail, personally, I work for the owner \u2026) The copyright owner has chosen the following license: License: Other: Evidence: The license statement can be found online at: The license agreement has been forwarded to Wikimedia's volunteer response team at \"permissions-en@wikimedia.org\". OTRS ticket received: The license hasn't yet been forwarded, but I will do so shortly or ask the owner to send it. I haven't got the evidence right now, but I will provide some if requested to do so. Note: files without verifiable permissions may be deleted. You may be better off obtaining proof of permission first. This file is from a free published source. I took it from a website or other published source, where its author has explicitly placed it under a free license, allowing free re-use by anybody. Use this only if there is an explicit licensing statement in the source. The website must explicitly say that the image is released under a license that allows free re-use for any purpose, e.g. the Creative Commons Attribution license. You must be able to point exactly to where it says this. If the source website doesn't say so explicitly, please do not upload the file . Author/owner: Date of creation: Source: For web sources: please provide a link to the html web page where the file can be found (\"http://... .html\"), not a direct link to the image file itself (\"http://... .jpg\"). For print sources: provide full bibliographic information (author, title, publisher, year, page, etc.) The copyright owner has chosen the following license for this file: License: Other: Evidence: Provide a link to where the author explicitly says that the file is released under a free license. (if not visible on the source page itself). This work is so old its copyright has expired This is an old photograph, or a photographic reproduction of an old painting, drawing, etc. I can provide enough information about its author and provenance to prove that it is old enough for its copyright to have expired. It is now legally in the Public Domain . Original author: Please name the original author of this work. Lifetime: Specify date of death, if applicable to public domain status. In many cases, we need to be certain the author died before a certain year, in many cases before 1930. Original publication: Provide as much information as possible about the original time and place of publication of this work. For print publications: provide full bibliographic information. Date of publication: Provide date of first publication, and date of creation if different. Immediate source: State exactly where you found this file. For web sources: please provide a link to the html web page where the file can be found (\"http://... .html\"), not a direct link to the image file itself (\"http://... .jpg\"). For print sources: provide full bibliographic information (author, title, publisher, year, page, etc.) Public Domain status: This work is free of all copyrights because: It was created and first published before 1930 and is therefore in the Public Domain in the US. It was first published outside the US , and it was in the Public Domain in its country of origin by the \" URAA date \". For most countries, this means the author died 70 years before 1 January 1996, i.e. before 1926. Please look up the copyright rules for the specific country at [1] . It was first published in the US before 1989 , and its copyright expired because it was published without a copyright notice and/or without the necessary copyright registration. Please look up the exact rules at [2] . Its copyright expired for some other reason. [Please explain below]. Explanation: Please provide any evidence necessary to verify the public domain status. This file is in the Public Domain for some other reason. I can demonstrate that this work is legally in the Public Domain, i.e. nobody owns any copyrights on it. This may be for a variety of reasons, for instance because it was created by the US Federal Government, or because it is too simple to attract any copyright. Public Domain means that nobody owns any copyrights on this work. It does not mean simply that it is freely viewable somewhere on the web or that it has been widely used by others. This is not for images you simply found somewhere on the web. Most images on the web are under copyright and belong to somebody, even if you believe the owner won't care about that copyright. If it is in the public domain, you must be able to point to an actual law that makes it so. If you can't point to such a law but merely found this image somewhere, then please do not upload it. Author: Please name the original author of this work. Source: State exactly where you found this file. For web sources: please provide a link to the html web page where the file can be found (\"http://... .html\"), not a direct link to the image file itself (\"http://... .jpg\"). For print sources: provide full bibliographic information (author, title, publisher, year, page, etc.) Date of creation: Public Domain status: This work is free of all copyrights because: It was created by an agency of the US Federal Government. This does not apply to most US state and local agencies , nor to governments of other countries! It is an official governmental item, such as a flag, state emblem, banknote or postage stamp, from a country where such items are exempt from copyright according to local law. This does not apply to all countries, nor does it usually apply to all publications! (For instance, it usually doesn't apply to simple publicity photographs published on a governmental website.)  Please look up the copyright rules for the specific country in question. If the country does not have an exemption rule that applies to this item, go on with the section for \" non-free copyrighted works \" below. Explanation: Add any explanation or evidence necessary to substantiate your statement above. Cite and provide a link to the relevant law if possible. It is too simple to be eligible for copyright. This typically applies only to graphics that consist solely of simple geometric shapes and/or a few letters or words, or to items such as mathematical or chemical formulae. It may apply to some very simple logos that do not contain complex pictorial elements. It never applies to photographs! It is in the Public Domain for some other reason. Explanation: This is a copyrighted, non-free work, but I believe it is Fair Use. I have read the Wikipedia rules on Non-Free Content , and I am prepared to explain how the use of this file will meet the criteria set out there. Please remember that you will need to demonstrate that: The file will serve an important function in a particular article; ( NFCC8 ) It cannot be replaced by any other, free illustration that might yet be created; ( NFCC1 ) Its use does not negatively affect the commercial interests of its owner ( NFCC2 ) There will not be more non-free material used than necessary. ( NFCC3 ) This file will be used in the following article: Enter the name of exactly one Wikipedia article, without the [[...]] brackets and without the \"http://en.wikipedia.org/...\" URL code. It has to be an actual article, not a talkpage, template, user page, etc. If you plan to use the file in more than one article, please name only one of them here. Then, after uploading, open the image description page for editing and add your separate explanations for each additional article manually. Example \u2013 article okay. This article doesn't exist! The article Example could not be found. Please check the spelling, and make sure you enter the name of an existing article in which you will include this file. If this is an article you are only planning to write, please write it first and upload the file afterwards. This is not an actual encyclopedia article! The page Example is not in the main article namespace. Non-free files can only be used in mainspace article pages, not on a user page, talk page, template, etc. Please upload this file only if it is going to be used in an actual article. If this page is an article draft in your user space, we're sorry, but we must ask you to wait until the page is ready and has been moved into mainspace, and only upload the file after that. This is a disambiguation page! The page Example is not a real article, but a disambiguation page pointing to a number of other pages. Please check and enter the exact title of the actual target article you meant. Non-free use rationale This image is the object of discussion in an article. This is a copyrighted artwork or photograph, and the image itself is the topic of discussion in the article. The discussion is about the photograph or painting as such, as a creative work, not just about the thing or person it shows. Which of these options describes this item best? Who created this work? Author/owner: Date: Source: State exactly where you found this file. For web sources: please provide a link to the html web page where the file can be found (\"http://... .html\"), not a direct link to the image file itself (\"http://... .jpg\"). For print sources: provide full bibliographic information (author, title, publisher, year, page, etc.) Usage: The article as a whole is dedicated specifically to a discussion of this particular photograph/painting. (It is not just about the person or thing shown in the picture.) There is a substantial amount of encyclopedic discussion of this particular photograph/painting ( not just about the person or thing shown in it) in this article. The illustration is specifically needed to support the following point: If neither of these two statements applies, then please do not upload this image. This section is not for images used merely to illustrate an article about a person or thing, showing what that person or thing look like. This is a depiction of a copyrighted three-dimensional work or building , which is the object of discussion in an article. This is a photograph or other kind of depiction of a copyrighted, three-dimensional creative work, such as a statue or work of architecture. The article contains a discussion of that work which requires illustration. The photograph as such is free, or was provided by the creator of the sculpture. Which of these options describes this item best? First describe who created the original work depicted: Creator: Date: Usage: The article as a whole is dedicated specifically to this work. There is a substantial amount of encyclopedic discussion of this work in this article. The illustration is specifically needed to support the following point: Now describe who created the image : Author: Date: Source: State exactly where you found this file. For web sources: please provide a link to the html web page where the file can be found (\"http://... .html\"), not a direct link to the image file itself (\"http://... .jpg\"). For print sources: provide full bibliographic information (author, title, publisher, year, page, etc.) Image status The image was created and published by the same author who also holds the rights to the original object, and no alternative depiction could be suitably created. The author of the image has released the photographic work under a free license, or it is in the public domain: Name the license or describe the public domain status, adding any necessary evidence to make the licensing status verifiable. This is an excerpt from a copyrighted work . This is an excerpt from a copyrighted work, e.g. a screenshot from a movie or TV programme, a panel from a comic, or a sound sample from a song. Its presence is needed to support a piece of explicit critical discussion in an article related to that work or its creator(s). Which of these options describes this item best? Author: (author / copyright owner of the original work) Date of creation: Source: (where exactly did you get this file from?) Please explain what exactly in the article it is that you want to illustrate with this. Typically, the illustration must be used to support some specific issue of discussion in the article. Please be concrete and specific. Don't just copy boilerplate statements from elsewhere. State clearly, in your own words, what this particular file will be doing in this particular article. If necessary, please explain why this purpose cannot be achieved through text alone. This is the official cover art of a work. This is the cover or dustjacket of a book, the cover of a CD or video, the official release poster of a movie, or a comparable item. It will be included as that work's primary means of visual identification, at the top of the article about the book, movie, etc. in question. Note: If you plan to use it for any other purpose than this, please tick the box for \"other non-free work\" below. Which of these options describes this item best? Author: (author / copyright owner of the original work) Date of publication: Source: (where exactly did you get this file from?) This image will be shown as a primary means of visual identification at the top of the article dedicated to the work in question. A standard fair use rationale will be added that matches this type of use. If you plan to use the file for any purpose other than this, please do not use this section, but the one labelled \"other type of non-free work\" below. This is a logo of an organization , company, brand, etc. This is an official logo of an entity that is the subject of a Wikipedia article. It will be included as that entity's primary means of visual identification, at the top of the article in question. Note: If you plan to use it for any other purpose than this, please tick the box for \"other non-free work\" below. Which of these options describes this item best? Source: (where exactly did you get this file from?) This image will be shown as a primary means of visual identification at the top of the article dedicated to the entity in question. A standard fair use rationale will be added that matches this type of use. If you plan to use the file for any purpose other than this, please do not use this section, but the one labelled \"other type of non-free work\" below. This is a historical portrait of a person no longer alive. This is a historical photograph or other depiction of a person who is no longer alive. It will be used as the primary means of visual identification of that person in the article about them. Deceased since: (This type of file can typically only be used with people who are no longer alive.) Author: (Who made this image?) Original publication: (Where, when and how was this image first published?) Date of publication: Source: (Where exactly did you get this file from?) This image will be shown as a primary means of visual identification at the top of the article dedicated to the person in question. A standard fair use rationale will be added that matches this type of use. If you plan to use the file for any purpose other than this, please do not use this section, but the one labelled \"other type of non-free work\" below. Please explain why a free alternative to this image cannot be found. For subjects who died recently: have you made a reasonable effort to find people who might possess photographs of this person and might be willing to release one? For subjects who lived in the early-to-mid 20th century: have you considered if there might be an older photograph that has fallen in the public domain? Please explain why you are confident that our use of the file will not harm any commercial opportunities of its owner. We will typically not use a file if its owner has a potential commercial interest in its use, and if our use of it would compete with its original market role. This is some other kind of non-free work that I believe is legitimate Fair Use. This is a copyrighted work whose use does not fall into any of the classes above. I have read the Wikipedia rules on Non-free content , and I will explain how this file meets all of the criteria set out there. Which of these options describes this item best? Author: (Who made this image?) Original publication: (Where, when and how was this image first published?) Date of publication: Source: (Where exactly did you get this file from?) Please explain what exact purpose this file will serve in the article. Please be concrete and specific. Don't just copy boilerplate statements from elsewhere. State clearly, in your own words, what this particular file will be doing in this particular article. Your explanation must make it clear why the article would be significantly worse off without this file. Please explain why this purpose could not be served by text alone. Please explain why this purpose could not be served with an alternative, free illustration that could yet be found or created. For a file to be deemed replaceable, it is not necessary that a suitable replacement already exists. Even if it is merely possible that a replacement could be created, we will not use the non-free file. Please explain why you are confident that our use of the file will not harm any commercial opportunities of its owner. We will typically not use a file if its owner has a potential commercial interest in its use, and if our use of it would compete with its original market role. Remember that the use of non-free files must be minimal. This can mean several things: Don't use more files per article/topic than necessary Don't use the same file in more articles than necessary Don't use larger excerpts of a single work than necessary Don't use images of higher resolution than necessary In view of this, please explain how the use of this file will be minimal. Special source and license conditions (optional) Any further relevant information about this file? Edit summary: A brief description of why you are overwriting this file. This will not be saved on the description page, but will be shown in its edit history. Yes , I want this file to be immediately available in all Wikimedia projects, in all languages. I will upload it on the Wikimedia Commons . Clicking this button will redirect you to a page on Commons. This will only work if you are already logged in there, which is likely the case if you have a unified account . Check here to see if you are logged in. No , I want to upload this file locally. This way it can be used only on the English Wikipedia. We urge you to upload it to the Commons unless there is a very good reason for it to stay local. Often such local files are copied to the Commons for use elsewhere and deleted locally, requiring extra work for other volunteers. If you do not want your file to be copied to Commons and deleted locally, consider adding the {{ Keep local }} tag. Reset this form and start over. This file doesn't fit either of the categories above. This file doesn't seem to fall into either of the classes above, or I am not certain what its status is. I found this file somewhere, but I don't really know who made it or who owns it. Well, we're very sorry, but if you're not sure about this file's copyright status, or if it doesn't fit into any of the groups above, then: Please don't upload it. Really, please don't. Even if you think it would make for a great addition to an article. We really take these copyright rules very seriously on Wikipedia. Note that media is assumed to be fully-copyrighted unless shown otherwise; the burden is on the uploader. In particular, please don't upload: any file you simply found on some website, without knowing who its author or copyright owner is. any file you found somewhere, even if you have good reason to believe it has a copyright owner who would not mind us using it, but you don't have an explicit licensing statement from them. any file that was released for publicity purposes by its owners but doesn't have a fully-free license for free re-use for all purposes. any file credited to a commercial image agency, such as Reuters, AP or Getty Images. Such files normally cannot be used even under the \"Fair Use\" rules, except in rare cases. any file that is licensed for use exclusively on Wikipedia, or is free except that the free license excludes commercial use. If you are in any doubt, please ask some experienced editors for advice before uploading. People will be happy to assist you at Wikipedia:Media copyright questions . Thank you. Preview (test) This is the data that will be submitted to upload: Filename: Edit summary: Text: Upload in process Your file is being uploaded. This might take a minute or two, depending on the size of the file and the speed of your internet connection. Once uploading is completed, you will find your new file at this link: File:Example.jpg File successfully uploaded Your file has been uploaded successfully and can now be found here: File:Example.jpg Please follow the link and check that the image description page has all the information you meant to include. If you want to change the description, just go to the image page, click the \"edit\" tab at the top of the page and edit just as you would edit any other page. Do not go through this upload form again, unless you want to replace the actual file with a new version. To insert this file into an article, you may want to use code similar to the following: [[File: Example.jpg |thumb|right|insert a caption here]] If you wish to make a link to the file in text, without actually showing the image, for instance when discussing the image on a talk page, you can use the following (mark the \":\" after the initial brackets!): [[:File: Example.jpg ]] See Wikipedia:Picture tutorial for more detailed help on how to insert and position images in pages. Thank you for using the File Upload Wizard. Please leave your feedback, comments, bug reports or suggestions on the talk page . Uploading media files Commons Wikipedia Commons Wizard (recommended for free files) Plain form for Commons (experienced users) Old form Files for upload process (recommended for new users) Plain form for local uploads (experienced users) Old guided form Help and guidelines Ask copyright questions Image use policy Non-free content This wizard Documentation Script Discuss Retrieved from \" https://en.wikipedia.org/w/index.php?title=Wikipedia:File_upload_wizard&oldid=1283861952 \" This page was last edited on 4 April 2025, at 02:30 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Wikipedia : File upload wizard 63 languages Add topic"}, "13": {"url": "https://en.wikipedia.org/wiki/Special:SpecialPages", "text": "Special pages - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Languages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Maintenance reports 2 Lists of pages 3 Account management 4 Users and rights 5 Recent changes and logs 6 Media reports and uploads 7 Data and tools 8 Redirecting special pages 9 High use pages 10 Page tools 11 Spam tools 12 Growth tools 13 Events 14 Pending changes 15 Other special pages Toggle the table of contents Special pages Help English Tools Tools move to sidebar hide Actions General Upload file Printable version Get shortened URL Download QR code In other projects Appearance move to sidebar hide This page contains a list of special pages . Most of the content of these pages is automatically generated and cannot be edited. To suggest a change to the parts that can be edited, find the appropriate text on Special:AllMessages and then request your change on the talk page of the message (using {{ editprotected }} to draw the attention of administrators). You can also see what message names are used on a page by adding ?uselang=qqx to the end of its URL, e.g. https://en.wikipedia.org/wiki/Special:SpecialPages?uselang=qqx will show (specialpages-summary) in place of this message, which allows you to find MediaWiki:Specialpages-summary . For an index of special pages, see Help:SpecialPages . Maintenance reports Broken redirects Dead-end pages Dormant pages Double redirects Lint errors Long pages Orphaned pages Pages not connected to items Pages with the fewest revisions Pages without language links Protected pages Protected titles Short pages Uncategorized categories Uncategorized files Uncategorized pages Uncategorized templates Unused categories Unused files Unused templates Wanted categories Wanted files Wanted pages Wanted templates Lists of pages All pages All pages with prefix Categories Category tree Disambiguation pages Entity usage External links search Pages linking to disambiguation pages Pages on topics near you Pages with a page property Pages with badges Redirects Search Tracking categories Account management Bot passwords Change credentials Change or remove email address Global preferences Global user account rename request Global user account vanish request Log in Log out Login unification status Notifications Preferences Remove credentials Reset password Reset tokens Topic subscriptions Users and rights Active users list Autoblocks Blocked users Create account Email user Global account manager Global accounts list Global group management Grants List OAuth applications List of globally blocked IP addresses and accounts Membership in global groups Password policies Search for global contributions User contributions User group rights User groups management Users Recent changes and logs Edit Recovery Edit filter log Gallery of new files New pages New pages feed Recent changes Related changes User logs Valid change tags Media reports and uploads File list Global file usage List of files with duplicates MIME search Media statistics Orphaned TimedText pages Search for duplicate files VIPS scaling test page Data and tools API feature usage API sandbox Book sources Edit filter configuration Expand templates Gadget usage statistics Gadgets Namespace information Statistics System messages Template sandbox Try hieroglyph markup Version View interwiki data Wiki sets Wikimedia wikis Redirecting special pages Delete a page Differences Edit a page New section Page history Page info Permanent link Protect a page Purge Random article Random page in category Random redirect Random root page Redirect by file, user, page, revision, or log ID High use pages Most linked-to categories Most linked-to files Most linked-to pages Most transcluded pages Pages with the most categories Pages with the most interwikis Pages with the most revisions Page tools Book Cite This Page Compare pages Export pages Mass delete Page assessments QR Code Generator URL Shortener What links here Spam tools Blocked External Domains Growth tools Impact Manage mentors Mentor dashboard Newcomer Tasks Events All events Cancel your registration for an event Delete event registration Edit event registration Event details Generate invitation list Invitation list Register for an event Your events Your invitation lists Pending changes Page review statistics Pages using Pending Changes Pages with edits awaiting review Other special pages Automatic translation Community Configuration Contribute Discussion tools data structure Find comment Global rename progress Math Formula Information ORES model statistics SecurePoll Retrieved from \" https://en.wikipedia.org/wiki/Special:SpecialPages \" Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Special pages Add topic"}, "14": {"url": "https://en.wikipedia.org/wiki/Special:Search", "text": "Search - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Languages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Search Help English Tools Tools move to sidebar hide Actions General Upload file Printable version Get shortened URL Download QR code In other projects Appearance move to sidebar hide Search Content pages Multimedia Everything Advanced Retrieved from \" https://en.wikipedia.org/wiki/Special:Search \" Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Search Add topic"}, "15": {"url": "https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=Web+crawler", "text": "Create account - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Languages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Create account English Tools Tools move to sidebar hide Actions General Upload file Printable version Get shortened URL Download QR code In other projects Appearance move to sidebar hide Username (username policy) Your username is public and cannot be made private later. Password It is recommended to use a unique password that you are not using on any other website. Confirm password Email address (recommended) Email is required to recover your account if you lose your password. CAPTCHA Security check Refresh Can't see the image? Request an account Create your account Wikipedia is made by people like you. 1,286,077,052 edits 6,994,844 articles 118,797 recent contributors Retrieved from \" https://auth.wikimedia.org/enwiki/wiki/Special:CreateAccount \" Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Create account Add topic"}, "16": {"url": "https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Web+crawler", "text": "Log in - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Languages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Log in English Tools Tools move to sidebar hide Actions General Upload file Printable version Get shortened URL Download QR code In other projects Appearance move to sidebar hide As part of recent enhancements, login processing now uses our domain auth.wikimedia.org . If you are using blocking software, you will need to allow access to this domain to log in. ( technical details ) Username Password Keep me logged in (for up to one year) Log in Help with logging in Forgot your password? Don't have an account? Join Wikipedia Retrieved from \" https://auth.wikimedia.org/enwiki/wiki/Special:UserLogin \" Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Log in Add topic"}, "17": {"url": "https://en.wikipedia.org/wiki/Special:MyContributions", "text": "User contributions for 2405:201:500D:211A:DCBB:F91B:FC96:5F8 - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Languages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk User contributions for 2405:201:500D:211A:DCBB:F91B:FC96:5F8 Help English Tools Tools move to sidebar hide Actions General Atom User contributions User logs Upload file Printable version Get shortened URL Download QR code In other projects Appearance move to sidebar hide Results for 2405:201:500D:211A:DCBB:F91B:FC96:5F8 talk block log logs global block log filter log Search for contributions show hide \u29fccontribs-top\u29fd Username, IP address or CIDR range: Namespace: all (Article) Talk User User talk Wikipedia Wikipedia talk File File talk MediaWiki MediaWiki talk Template Template talk Help Help talk Category Category talk Portal Portal talk Draft Draft talk MOS MOS talk TimedText TimedText talk Module Module talk Event Event talk Invert selection Associated namespace Tag filter: 2017 wikitext editor ABBA [1.0] Addition of protection template to non-protected page Advanced mobile edit Android app edit AntiVandal App AI assist App description add App description change App description translate App full source App image add infobox App image add top App rollback App section source App select source App suggested edit App talk reply App talk source App talk topic App undo Automatic insertion of extraneous formatting AWB Barnsworth [1.0] Blanking blanking Bot in trial bup 2 [1.0] campaign-external-machine-translation canned edit summary categories removed CD Change of the mentor list changing height and/or weight changing time or duration Citation bot [1.1.0] Citation Bot [1.2.0] citing a blog or free web host COI template removed Community Configuration condition limit reached content model change content sourced to vanity press contentious topics alert ContentTranslation contenttranslation-needcheck ContentTranslation2 ContentTranslation: High unmodified machine translation usage Contest or editathon copyright violation template removed Correct typos in one click [1.0] Correct typos in one click [1.1] CropTool [1.2] CropTool [1.4] CropTool [1.5] CropTool testing [1.0] DAB Mechanic [1.0] dashboard-testing.wikiedu.org [1.0] dashboard-testing.wikiedu.org [2.0] dashboard.wikiedu.org [1.0] dashboard.wikiedu.org [1.2] dashboard.wikiedu.org [2.0] dashboard.wikiedu.org [2.2] dashboard.wikiedu.org [2.3] dashboard.wikiedu.org account-creation dev [1.0] dashboard.wikiedu.org dev [0.4.11] dashboard.wikiedu.org dev [0.4.2] dashboard.wikiedu.org dev [0.4.3] dashboard.wikiedu.org dev [0.4.4] dashboard.wikiedu.org dev [0.4.5] dashboard.wikiedu.org dev [0.4.7] dashboard.wikiedu.org dev [0.4.8] dashboard.wikiedu.org dev [0.4] dashboard.wikiedu.org dev [2.0] dashboard.wikiedu.org dev [2.1] demo-oauthratelimiter-3 [1.0] Deputy DiBabel [1.2] Disambiguation links added disambiguation template removed discussiontools (hidden tag) discussiontools-added-comment (hidden tag) discussiontools-source (hidden tag) discussiontools-source-enhanced (hidden tag) discussiontools-visual (hidden tag) Dispenser [1.0] Dispenser [2.4] Downstream Pull [0.1] draft or userpage link Edit Check (references) activated Edit Check (references) declined (irrelevant) Edit Check (references) declined (other) Edit Check (references) declined (uncertain) editcheck-newcontent (hidden tag) editcheck-newreference (hidden tag) editcheck-references (hidden tag) editProtectedHelper Education Dashboard development - awight [1.0] English Wikipedia Account Creation Assistance Tool [2.0] excessive whitespace External link added to disambiguation page extraneous markup featured article or good article template added or removed Find link [1.0] Fixed lint errors Fountain [0.1.3] Fountain Test [1.1] gabinaluz+app@gmail.com [1.0] harv or sfn error Huggle IABot [1.0] IABotManagementConsole (Personal Debug Consumer) [1.0] IABotManagementConsole [1.1] IABotManagementConsole [1.2] IABotManagementConsole [1.3] image template modification Incorrectly formatted external link or image Invalid TimedText edit iOS app edit Jonathan_at_WINTR_7 [1.0] JWB KenBurnsEffect tool [1.1] large non-free file large plot addition large unwikified new article LCA Tools [1.0] LCA Tools [1.5] Localhost [1.0] Manual revert Mass pronoun change MassMessage delivery massmove Medha_Bansal_intern_at_WikiEduDashboard [1.0] Mentorship module question Mentorship panel question missing file added missingredirectsproject [1.0] Mobile app edit Mobile edit Mobile web edit Modified by FileImporter moveToDraft MTC-Web for local dev/testing [1.0] MTCWeb-Dev [1.0] New redirect New topic New user adding protection template new user modifying archives new user moving page out of userspace Newcomer task Newcomer task: copyedit Newcomer task: expand Newcomer task: links Newcomer task: references Newcomer task: update Non-autoconfirmed user rapidly reverting edits non-English content nowiki added Nuke OAbot [1.0] OAbot [2.1] OAuth Uploader [1.0] outreachdashboard.wmflabs.org [2.0] outreachdashboard.wmflabs.org [2.1] pageswap GUI PageTriage Parliament edits PAWS [1.2] PAWS [2.1] paws [2.2] possible autobiography or conflict of interest possible birth or death date change possible BLP issue or vandalism possible conflict of interest possible cut and paste move or recreation possible MOS:ETHNICITY violation Possible self promotion in user or draftspace Possible self promotion in userspace possible unreferenced addition to BLP possible userspace spam possible vandalism possible WP:BLPCRIME issue possibly inaccurate edit summary ProveIt edit QuickCategories [1.0] QuickCategories [1.1] rapid date format changes Recreated Redirect target changed reference list removal references removed removal of Category:Living People Removed redirect repeated addition of external links by non-autoconfirmed user Replaced Reply Reverted reverting anti-vandal bot review edit Rollback RW section blanking SectionTranslation self-renaming or bad user talk move Shortdesc helper shouting Snuggle (English Wikipedia) [1.0.0] speedy deletion template removed Suggested: add links Suggestor [0.0.1] SWViewer [1.0] SWViewer [1.2] SWViewer [1.3] SWViewer [1.4] SWViewer [1.6] Takedown Tools [1.0] Talk banner shell conversion talk page blanking Text added at end of page TorProxy [0.1] TorProxy [0.2] Twinkle U.S. Congress edit Ultraviolet Undo Unexpected #REDIRECT markup unsourced AFC submission use of deprecated (unreliable) source use of predatory open access journal User Analysis Tool [1.0] very short new article Visual edit Visual edit: Check Visual edit: Switched Weekipedia v3 [1.0] Weekipedia2 [1.0] wiki-file-transfer [1.0] wikieditor (hidden tag) wikiedu Assignment Wizard 2.0 Teting [1.0] wikiedu Assignment Wizard [1.0] wikiedu.org Assignment Design Wizard (testing) [0.99] wikiedu.org Assignment Design Wizard [1.0.1] wikiedu.org Assignment Design Wizard [1.0.2] WikiEduDashboard NTDB [1.2] WikiEduDashboard NTDB3 [1.0] WikiEduDashboard NTDB4 [1.0] WikiEduDashboard NTDB5 [1.0] WikiEduWizard NTDB [1.2] Wikifile Transfer [1.0] Wikifile Transfer [2.0] Wikifile Transfer [4.0] Wikifile Transfer [5.0] WikiLeaks wikilinks removed WikiLoop Battlefield WikiLoop Battlefield Dev Local [2.0.0] WikiLoop Battlefield on WMF Cloud VPS [1.0] WikiLoop Battlefield Prod [2.2.1-beta] WikiLoop DoubleCheck WMF Cloud VPS (2020-07-13 version) [4.1.0] WikiLove WINTR Wikiedu Dashboard Local Test 3 [1.0] WINTR Wikiedu Wizard Local Test [1.0] WPCleaner yabbr [1.3] Invert selection Only show edits that are latest revisions Only show edits that are page creations Hide minor edits \u29fccontribs-date\u29fd From date: To date: Show only edits pending review Hide probably good edits Search No changes were found matching these criteria. This is the contributions page for an IP user, identified by the user's IP address . Many IP addresses change periodically, and are often shared by several users. If you are an IP user, you may create an account or log in to avoid future confusion with other IP users. Registering also hides your IP address. ( WHOIS ( alt \u2022 old ) Geolocate ( Alternate ) Proxy Checker Current blocks XTools Global contributions ) ( RIRs : Africa America Asia-Pacific Europe Latin America/Caribbean ) Retrieved from \" https://en.wikipedia.org/wiki/Special:Contributions/2405:201:500D:211A:DCBB:F91B:FC96:5F8 \" Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search User contributions for 2405:201:500D:211A:DCBB:F91B:FC96:5F8 Add topic"}, "18": {"url": "https://en.wikipedia.org/wiki/Special:MyTalk", "text": "User talk:2405:201:500D:211A:DCBB:F91B:FC96:5F8 - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk User talk : 2405:201:500D:211A:DCBB:F91B:FC96:5F8 Add languages Page contents not supported in other languages. User page Talk English Create Add topic Tools Tools move to sidebar hide Actions Create Add topic General What links here User contributions User logs Upload file Printable version Page information Get shortened URL Download QR code In other projects Appearance move to sidebar hide From Wikipedia, the free encyclopedia Welcome to this talk page People on Wikipedia can use this talk page to post a public message about edits made from the IP address you are currently using. Many IP addresses change periodically, and are often shared by several people. You may create an account or log in to avoid future confusion with other logged out users. Creating an account also hides your IP address. Retrieved from \" https://en.wikipedia.org/wiki/User_talk:2405:201:500D:211A:DCBB:F91B:FC96:5F8 \" Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search User talk : 2405:201:500D:211A:DCBB:F91B:FC96:5F8 Add topic"}, "19": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Nomenclature", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "20": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Overview", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "21": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Crawling_policy", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "22": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Selection_policy", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "23": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Restricting_followed_links", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "24": {"url": "https://en.wikipedia.org/wiki/Web_crawler#URL_normalization", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "25": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Path-ascending_crawling", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "26": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Focused_crawling", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "27": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Academic_focused_crawler", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "28": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Semantic_focused_crawler", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "29": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Re-visit_policy", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "30": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Politeness_policy", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "31": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Parallelization_policy", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "32": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Architectures", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "33": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Security", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "34": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Crawler_identification", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "35": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Crawling_the_deep_web", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "36": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Visual_vs_programmatic_crawlers", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "37": {"url": "https://en.wikipedia.org/wiki/Web_crawler#List_of_web_crawlers", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "38": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Historical_web_crawlers", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "39": {"url": "https://en.wikipedia.org/wiki/Web_crawler#In-house_web_crawlers", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "40": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Commercial_web_crawlers", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "41": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Open-source_crawlers", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "42": {"url": "https://en.wikipedia.org/wiki/Web_crawler#See_also", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "43": {"url": "https://en.wikipedia.org/wiki/Web_crawler#References", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "44": {"url": "https://en.wikipedia.org/wiki/Web_crawler#Further_reading", "text": "Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Nomenclature 2 Overview 3 Crawling policy Toggle Crawling policy subsection 3.1 Selection policy 3.1.1 Restricting followed links 3.1.2 URL normalization 3.1.3 Path-ascending crawling 3.1.4 Focused crawling 3.1.4.1 Academic focused crawler 3.1.4.2 Semantic focused crawler 3.2 Re-visit policy 3.3 Politeness policy 3.4 Parallelization policy 4 Architectures 5 Security 6 Crawler identification 7 Crawling the deep web 8 Visual vs programmatic crawlers 9 List of web crawlers Toggle List of web crawlers subsection 9.1 Historical web crawlers 9.2 In-house web crawlers 9.3 Commercial web crawlers 9.4 Open-source crawlers 10 See also 11 References 12 Further reading Toggle the table of contents Web crawler 47 languages Afrikaans \u0627\u0644\u0639\u0631\u0628\u064a\u0629 Az\u0259rbaycanca Boarisch Catal\u00e0 \u010ce\u0161tina Cymraeg \u0627\u0644\u062f\u0627\u0631\u062c\u0629 Deutsch Eesti \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac Espa\u00f1ol Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais \ud55c\uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 Hrvatski Bahasa Indonesia Interlingua Italiano \u05e2\u05d1\u05e8\u05d9\u05ea Latvie\u0161u Lietuvi\u0173 Magyar Bahasa Melayu Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l Norsk nynorsk \u041e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439 Polski Portugu\u00eas Rom\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Simple English \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Suomi Svenska \u0ba4\u0bae\u0bbf\u0bb4\u0bcd \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u6587\u8a00 \u4e2d\u6587 Edit links Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Wikidata item Appearance move to sidebar hide From Wikipedia, the free encyclopedia Software which systematically browses the World Wide Web This article is about the internet bot. For the search engine, see WebCrawler . \"Web spider\" redirects here and is not to be confused with Spider web . \"Spiderbot\" redirects here. For the video game, see Arac (video game) . Architecture of a Web crawler Web crawler , sometimes called a spider or spiderbot and often shortened to crawler , is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing ( web spidering ). [ 1 ] Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently. Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all. The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming . Nomenclature [ edit ] A web crawler is also known as a spider , [ 2 ] an ant , an automatic indexer , [ 3 ] or (in the FOAF software context) a Web scutter . [ 4 ] Overview [ edit ] A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds . As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier . URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving ), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'. [ 5 ] The archive is known as the repository and is designed to store and manage the collection of web pages . The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler. [ citation needed ] The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted. The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content . Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content. As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\" [ 6 ] A crawler must carefully choose at each step which pages to visit next. Crawling policy [ edit ] The behavior of a Web crawler is the outcome of a combination of policies: [ 7 ] a selection policy which states the pages to download, a re-visit policy which states when to check for changes to the pages, a politeness policy that states how to avoid overloading websites . a parallelization policy that states how to coordinate distributed web crawlers. Selection policy [ edit ] Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40\u201370% of the indexable Web; [ 8 ] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999. [ 9 ] As a crawler always downloads just a fraction of the Web pages , it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web. This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain , or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling. Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies. [ 10 ] The ordering metrics tested were breadth-first , backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling. [ 11 ] Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering. [ 12 ] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\" Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation). [ 13 ] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web. Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations. [ 14 ] [ 15 ] Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies. [ 16 ] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one. Daneshpajouh et al. designed a community based algorithm for discovering good seeds. [ 17 ] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective. Restricting followed links [ edit ] A crawler may only want to seek out HTML pages and avoid all other MIME types . In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped. Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs. URL normalization [ edit ] Main article: URL normalization Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization , also called URL canonicalization , refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component. [ 18 ] Path-ascending crawling [ edit ] Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl. [ 19 ] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling. Focused crawling [ edit ] Main article: Focused crawler The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers . The concepts of topical and focused crawling were first introduced by Filippo Menczer [ 20 ] [ 21 ] and by Soumen Chakrabarti et al. [ 22 ] The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton [ 23 ] in the first web crawler of the early days of the Web. Diligenti et al. [ 24 ] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points. Academic focused crawler [ edit ] An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot , which is the crawler of CiteSeer X search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix , must be customized to filter out other MIME types , or a middleware is used to extract these documents out and import them to the focused crawl database and repository. [ 25 ] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers. [ 26 ] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads. Semantic focused crawler [ edit ] Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. [ 27 ] In addition, ontologies can be automatically updated in the crawling process. Dong et al. [ 28 ] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages. Re-visit policy [ edit ] The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions. From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age. [ 29 ] Freshness : This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as: F p ( t ) = { 1 i f p i s e q u a l t o t h e l o c a l c o p y a t t i m e t 0 o t h e r w i s e {\\displaystyle F_{p}(t)={\\begin{cases}1&{\\rm {if}}~p~{\\rm {~is~equal~to~the~local~copy~at~time}}~t\\\\0&{\\rm {otherwise}}\\end{cases}}} Age : This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as: A p ( t ) = { 0 i f p i s n o t m o d i f i e d a t t i m e t t \u2212 m o d i f i c a t i o n t i m e o f p o t h e r w i s e {\\displaystyle A_{p}(t)={\\begin{cases}0&{\\rm {if}}~p~{\\rm {~is~not~modified~at~time}}~t\\\\t-{\\rm {modification~time~of}}~p&{\\rm {otherwise}}\\end{cases}}} Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler. [ 30 ] The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are. Evolution of Freshness and Age in a web crawler Two simple re-visiting policies were studied by Cho and Garcia-Molina: [ 31 ] Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change. Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency. In both cases, the repeated crawling order of pages can be done either in a random or a fixed order. Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them. To improve freshness, the crawler should penalize the elements that change too often. [ 32 ] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\". [ 30 ] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes, [ 32 ] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution. [ 33 ] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy. Politeness policy [ edit ] Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers. As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community. [ 34 ] The costs of using Web crawlers include: network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time; server overload, especially if the frequency of accesses to a given server is too high; poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and personal crawlers that, if deployed by too many users, can disrupt networks and Web servers. A partial solution to these problems is the robots exclusion protocol , also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers. [ 35 ] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google , Ask Jeeves , MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests. The first proposed interval between successive pageloads was 60 seconds. [ 36 ] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used. Cho uses 10 seconds as an interval for accesses, [ 31 ] and the WIRE crawler uses 15 seconds as the default. [ 37 ] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10 t seconds before downloading the next page. [ 38 ] Dill et al. use 1 second. [ 39 ] For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl. [ 40 ] Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\" [ 41 ] Parallelization policy [ edit ] Main article: Distributed web crawling A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes. Architectures [ edit ] High-level architecture of a standard Web crawler A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture. Shkapenyuk and Suel noted that: [ 42 ] While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \" search engine spamming \", which prevent major search engines from publishing their ranking algorithms. Security [ edit ] While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines , web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software. Main article: Google hacking Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt ) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.). Crawler identification [ edit ] Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers ' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler. Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine . Crawling the deep web [ edit ] A vast amount of web pages lie in the deep or invisible web . [ 43 ] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai [ 44 ] are intended to allow discovery of these deep-Web resources. Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a\u00a0href=\"URL\"> form. In some cases, such as the Googlebot , Web crawling is done on all text contained inside the hypertext content, tags, or text. Strategic approaches may be taken to target deep Web content. With a technique called screen scraping , specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers. [ 45 ] Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index. [ 46 ] Visual vs programmatic crawlers [ edit ] There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data. The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs [ 47 ] ), there is continued growth and investment in this area by investors and end-users. [ citation needed ] List of web crawlers [ edit ] Further information: List of search engine software The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features: Historical web crawlers [ edit ] WolfBot was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis. World Wide Web Worm was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the grep Unix command. Yahoo! Slurp was the name of the Yahoo! Search crawler until Yahoo! contracted with Microsoft to use Bingbot instead. In-house web crawlers [ edit ] Applebot is Apple 's web crawler. It supports Siri and other products. [ 48 ] Bingbot is the name of Microsoft's Bing webcrawler. It replaced Msnbot . Baiduspider is Baidu 's web crawler. DuckDuckBot is DuckDuckGo 's web crawler. Googlebot is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and Python . The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server. WebCrawler was used to build the first publicly available full-text index of a subset of the Web. It was based on lib-WWW to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query. WebFountain is a distributed, modular crawler similar to Mercator but written in C++. Xenon is a web crawler used by government tax authorities to detect fraud. [ 49 ] [ 50 ] Commercial web crawlers [ edit ] The following web crawlers are available, for a price:: Diffbot - programmatic general web crawler, available as an API SortSite - crawler for analyzing websites, available for Windows and Mac OS Swiftbot - Swiftype 's web crawler, available as software as a service Aleph Search - web crawler allowing massive collection with high scalability Open-source crawlers [ edit ] Apache Nutch is a highly extensible and scalable web crawler written in Java and released under an Apache License . It is based on Apache Hadoop and can be used with Apache Solr or Elasticsearch . Grub was an open source distributed search crawler that Wikia Search used to crawl the web. Heritrix is the Internet Archive 's archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in Java . ht://Dig includes a Web crawler in its indexing engine. HTTrack uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in C and released under the GPL. Norconex Web Crawler is a highly extensible Web Crawler written in Java and released under an Apache License . It can be used with many repositories such as Apache Solr , Elasticsearch , Microsoft Azure Cognitive Search , Amazon CloudSearch and more. mnoGoSearch is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only) Open Search Server is a search engine and web crawler software release under the GPL. Scrapy , an open source webcrawler framework, written in python (licensed under BSD ). Seeks , a free distributed search engine (licensed under AGPL ). StormCrawler , a collection of resources for building low-latency, scalable web crawlers on Apache Storm (Apache License). tkWWW Robot , a crawler based on the tkWWW web browser (licensed under GPL). GNU Wget is a command-line -operated crawler written in C and released under the GPL . It is typically used to mirror Web and FTP sites. YaCy , a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL). See also [ edit ] Automatic indexing Gnutella crawler Web archiving Webgraph Website mirroring software Search Engine Scraping Web scraping References [ edit ] ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 6 December 2021. ^ Spetka, Scott. \"The TkWWW Robot: Beyond Browsing\" . NCSA . Archived from the original on 3 September 2004 . Retrieved 21 November 2010 . ^ Kobayashi, M. & Takeda, K. (2000). \"Information retrieval on the web\". ACM Computing Surveys . 32 (2): 144\u2013 173. CiteSeerX 10.1.1.126.6094 . doi : 10.1145/358923.358934 . S2CID 3710903 . ^ See definition of scutter on FOAF Project's wiki Archived 13 December 2009 at the Wayback Machine ^ Masan\u00e8s, Julien (15 February 2007). Web Archiving . Springer. p.\u00a01. ISBN 978-3-54046332-0 . Retrieved 24 April 2014 . ^ Edwards, J.; McCurley, K. S.; and Tomlin, J. A. (2001). \"An adaptive model for optimizing performance of an incremental web crawler\". Proceedings of the 10th international conference on World Wide Web . pp. 106\u2013 113. CiteSeerX 10.1.1.1018.1506 . doi : 10.1145/371920.371960 . ISBN 978-1581133486 . S2CID 10316730 . Archived from the original on 25 June 2014 . Retrieved 25 January 2007 . {{ cite book }} :  CS1 maint: multiple names: authors list ( link ) ^ Castillo, Carlos (2004). Effective Web Crawling (PhD thesis). University of Chile . Retrieved 3 August 2010 . ^ Gulls, A.; A. Signori (2005). \"The indexable web is more than 11.5 billion pages\". Special interest tracks and posters of the 14th international conference on World Wide Web . ACM Press. pp. 902\u2013 903. doi : 10.1145/1062745.1062789 . ^ Lawrence, Steve; C. Lee Giles (8 July 1999). \"Accessibility of information on the web\" . Nature . 400 (6740): 107\u2013 9. Bibcode : 1999Natur.400..107L . doi : 10.1038/21987 . PMID 10428673 . S2CID 4347646 . ^ Cho, J.; Garcia-Molina, H.; Page, L. (April 1998). \"Efficient Crawling Through URL Ordering\" . Seventh International World-Wide Web Conference . Brisbane, Australia. doi : 10.1142/3725 . ISBN 978-981-02-3400-3 . Retrieved 23 March 2009 . ^ Cho, Junghoo, \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\" , PhD dissertation, Department of Computer Science, Stanford University, November 2001. ^ Najork, Marc and Janet L. Wiener. \"Breadth-first crawling yields high-quality pages\". Archived 24 December 2017 at the Wayback Machine In: Proceedings of the Tenth Conference on World Wide Web , pages 114\u2013118, Hong Kong, May 2001. Elsevier Science. ^ Abiteboul, Serge; Mihai Preda; Gregory Cobena (2003). \"Adaptive on-line page importance computation\" . Proceedings of the 12th international conference on World Wide Web . Budapest, Hungary: ACM. pp. 280\u2013 290. doi : 10.1145/775152.775192 . ISBN 1-58113-680-3 . Retrieved 22 March 2009 . ^ Boldi, Paolo; Bruno Codenotti; Massimo Santini; Sebastiano Vigna (2004). \"UbiCrawler: a scalable fully distributed Web crawler\" (PDF) . Software: Practice and Experience . 34 (8): 711\u2013 726. CiteSeerX 10.1.1.2.5538 . doi : 10.1002/spe.587 . S2CID 325714 . Archived from the original (PDF) on 20 March 2009 . Retrieved 23 March 2009 . ^ Boldi, Paolo; Massimo Santini; Sebastiano Vigna (2004). \"Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations\" (PDF) . Algorithms and Models for the Web-Graph . Lecture Notes in Computer Science. Vol.\u00a03243. pp. 168\u2013 180. doi : 10.1007/978-3-540-30216-2_14 . ISBN 978-3-540-23427-2 . Archived from the original (PDF) on 1 October 2005 . Retrieved 23 March 2009 . ^ Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\" In: Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web , pages 864\u2013872, Chiba, Japan. ACM Press. ^ Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, A Fast Community Based Algorithm for Generating Crawler Seeds Set . In: Proceedings of 4th International Conference on Web Information Systems and Technologies ( Webist -2008), Funchal, Portugal, May 2008. ^ Pant, Gautam; Srinivasan, Padmini; Menczer, Filippo (2004). \"Crawling the Web\" (PDF) . In Levene, Mark; Poulovassilis, Alexandra (eds.). Web Dynamics: Adapting to Change in Content, Size, Topology and Use . Springer. pp. 153\u2013 178. ISBN 978-3-540-40676-1 . Archived from the original (PDF) on 20 March 2009 . Retrieved 9 May 2006 . ^ Cothey, Viv (2004). \"Web-crawling reliability\" (PDF) . Journal of the American Society for Information Science and Technology . 55 (14): 1228\u2013 1238. CiteSeerX 10.1.1.117.185 . doi : 10.1002/asi.20078 . ^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery Archived 21 December 2012 at the Wayback Machine . In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann ^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments Archived 21 December 2012 at the Wayback Machine . In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press ^ Chakrabarti, Soumen; Van Den Berg, Martin; Dom, Byron (1999). \"Focused crawling: A new approach to topic-specific Web resource discovery\" (PDF) . Computer Networks . 31 ( 11\u2013 16): 1623\u2013 1640. doi : 10.1016/s1389-1286(99)00052-3 . Archived from the original (PDF) on 17 March 2004. ^ Pinkerton, B. (1994). Finding what people want: Experiences with the WebCrawler . In Proceedings of the First World Wide Web Conference, Geneva, Switzerland. ^ Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). Focused crawling using context graphs . In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt. ^ Wu, Jian; Teregowda, Pradeep; Khabsa, Madian; Carman, Stephen; Jordan, Douglas; San Pedro Wandelmer, Jose; Lu, Xin; Mitra, Prasenjit; Giles, C. Lee (2012). \"Web crawler middleware for search engine digital libraries\". Proceedings of the twelfth international workshop on Web information and data management - WIDM '12 . p.\u00a057. doi : 10.1145/2389936.2389949 . ISBN 9781450317207 . S2CID 18513666 . ^ Wu, Jian; Teregowda, Pradeep; Ram\u00edrez, Juan Pablo Fern\u00e1ndez; Mitra, Prasenjit; Zheng, Shuyi; Giles, C. Lee (2012). \"The evolution of a crawling strategy for an academic document search engine\". Proceedings of the 3rd Annual ACM Web Science Conference on - Web Sci '12 . pp. 340\u2013 343. doi : 10.1145/2380718.2380762 . ISBN 9781450312288 . S2CID 16718130 . ^ Dong, Hai; Hussain, Farookh Khadeer; Chang, Elizabeth (2009). \"State of the Art in Semantic Focused Crawlers\" . Computational Science and Its Applications \u2013 ICCSA 2009 . Lecture Notes in Computer Science. Vol.\u00a05593. pp. 910\u2013 924. doi : 10.1007/978-3-642-02457-3_74 . hdl : 20.500.11937/48288 . ISBN 978-3-642-02456-6 . ^ Dong, Hai; Hussain, Farookh Khadeer (2013). \"SOF: A semi-supervised ontology-learning-based focused crawler\" . Concurrency and Computation: Practice and Experience . 25 (12): 1755\u2013 1770. doi : 10.1002/cpe.2980 . S2CID 205690364 . ^ Junghoo Cho; Hector Garcia-Molina (2000). \"Synchronizing a database to improve freshness\" (PDF) . Proceedings of the 2000 ACM SIGMOD international conference on Management of data . Dallas, Texas, United States: ACM. pp. 117\u2013 128. doi : 10.1145/342009.335391 . ISBN 1-58113-217-4 . Retrieved 23 March 2009 . ^ a b E. G. Coffman Jr; Zhen Liu; Richard R. Weber (1998). \"Optimal robot scheduling for Web search engines\". Journal of Scheduling . 1 (1): 15\u2013 29. CiteSeerX 10.1.1.36.6087 . doi : 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K . ^ a b Cho, Junghoo; Garcia-Molina, Hector (2003). \"Effective page refresh policies for Web crawlers\". ACM Transactions on Database Systems . 28 (4): 390\u2013 426. doi : 10.1145/958942.958945 . S2CID 147958 . ^ a b Junghoo Cho; Hector Garcia-Molina (2003). \"Estimating frequency of change\". ACM Transactions on Internet Technology . 3 (3): 256\u2013 290. CiteSeerX 10.1.1.59.5877 . doi : 10.1145/857166.857170 . S2CID 9362566 . ^ Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) Modeling and managing content changes in text databases Archived 5 September 2005 at the Wayback Machine . In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo. ^ Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4). ^ Koster, M. (1996). A standard for robot exclusion Archived 7 November 2007 at the Wayback Machine . ^ Koster, M. (1993). Guidelines for robots writers Archived 22 April 2005 at the Wayback Machine . ^ Baeza-Yates, R. and Castillo, C. (2002). Balancing volume, quality and freshness in Web crawling . In Soft Computing Systems\u00a0\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam. ^ Heydon, Allan; Najork, Marc (26 June 1999). \"Mercator: A Scalable, Extensible Web Crawler\" (PDF) . Archived from the original (PDF) on 19 February 2006 . Retrieved 22 March 2009 . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Dill, S.; Kumar, R.; Mccurley, K. S.; Rajagopalan, S.; Sivakumar, D.; Tomkins, A. (2002). \"Self-similarity in the web\" (PDF) . ACM Transactions on Internet Technology . 2 (3): 205\u2013 223. doi : 10.1145/572326.572328 . S2CID 6416041 . ^ M. Thelwall; D. Stuart (2006). \"Web crawling ethics revisited: Cost, privacy and denial of service\" . Journal of the American Society for Information Science and Technology . 57 (13): 1771\u2013 1779. doi : 10.1002/asi.20388 . ^ Brin, Sergey; Page, Lawrence (1998). \"The anatomy of a large-scale hypertextual Web search engine\" . Computer Networks and ISDN Systems . 30 ( 1\u2013 7): 107\u2013 117. doi : 10.1016/s0169-7552(98)00110-x . S2CID 7587743 . ^ Shkapenyuk, V. and Suel, T. (2002). Design and implementation of a high performance distributed web crawler . In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press. ^ Shestakov, Denis (2008). Search Interfaces on the Web: Querying and Characterizing Archived 6 July 2014 at the Wayback Machine . TUCS Doctoral Dissertations 104, University of Turku ^ Michael L Nelson; Herbert Van de Sompel; Xiaoming Liu; Terry L Harrison; Nathan McFarland (24 March 2005). \"mod_oai: An Apache Module for Metadata Harvesting\": cs/0503069. arXiv : cs/0503069 . Bibcode : 2005cs........3069N . {{ cite journal }} : Cite journal requires |journal= ( help ) ^ Shestakov, Denis; Bhowmick, Sourav S.; Lim, Ee-Peng (2005). \"DEQUE: Querying the Deep Web\" (PDF) . Data & Knowledge Engineering . 52 (3): 273\u2013 311. doi : 10.1016/s0169-023x(04)00107-7 . ^ \"AJAX crawling: Guide for webmasters and developers\" . Retrieved 17 March 2013 . ^ ITA Labs \"ITA Labs Acquisition\" Archived 18 March 2014 at the Wayback Machine 20 April 2011 1:28 AM ^ \"About Applebot\" . Apple Inc . Retrieved 18 October 2021 . ^ Norton, Quinn (25 January 2007). \"Tax takers send in the spiders\" . Business. Wired . Archived from the original on 22 December 2016 . Retrieved 13 October 2017 . ^ \"Xenon web crawling initiative: privacy impact assessment (PIA) summary\" . Ottawa: Government of Canada. 11 April 2017. Archived from the original on 25 September 2017 . Retrieved 13 October 2017 . Further reading [ edit ] Cho, Junghoo, \"Web Crawling Project\" , UCLA Computer Science Department. A History of Search Engines , from Wiley WIVET is a benchmarking project by OWASP , which aims to measure if a web crawler can identify all the hyperlinks in a target website. Shestakov, Denis, \"Current Challenges in Web Crawling\" and \"Intelligent Web Crawling\" , slides for tutorials given at ICWE'13 and WI-IAT'13. v t e Internet search Types Web search engine ( List ) Metasearch engine Multimedia search Collaborative search engine Cross-language search Local search Vertical search Social search Image search Audio search Video search engine Enterprise search Semantic search Natural language search engine Voice search Tools Cross-language information retrieval Search by sound Search engine marketing Search engine optimization Evaluation measures Search oriented architecture Selection-based search Document retrieval Text mining Web crawler Multisearch Federated search Search aggregator Index / Web indexing Focused crawler Spider trap Robots exclusion standard Distributed web crawling Web archiving Website mirroring software Web query Web query classification Protocols and standards Z39.50 Search/Retrieve Web Service Search/Retrieve via URL OpenSearch Representational State Transfer Wide area information server See also Search engine Desktop search Online search v t e Web crawlers Internet bots designed for Web crawling and Web indexing Active 80legs bingbot Crawljax Fetcher Googlebot Heritrix HTTrack PowerMapper Wget Discontinued FAST Crawler msnbot RBSE TkWWW robot Twiceler Types Distributed web crawler Focused crawler Authority control databases : National Germany Retrieved from \" https://en.wikipedia.org/w/index.php?title=Web_crawler&oldid=1287674457 \" Categories : Search engine software Web crawlers Internet search algorithms Hidden categories: Webarchive template wayback links CS1 maint: multiple names: authors list CS1 errors: missing periodical Articles with short description Short description is different from Wikidata Use dmy dates from September 2020 All articles with unsourced statements Articles with unsourced statements from February 2023 Articles with unsourced statements from June 2021 This page was last edited on 27 April 2025, at 18:46 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Web crawler 47 languages Add topic"}, "45": {"url": "https://en.wikipedia.org/wiki/Talk:Web_crawler", "text": "Talk:Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Contents move to sidebar hide (Top) 1 Phrasing 2 comments 2 Low quality citation 1 comment Toggle the table of contents Talk : Web crawler Add languages Page contents not supported in other languages. Article Talk English Read Edit Add topic View history Tools Tools move to sidebar hide Actions Read Edit Add topic View history General What links here Related changes Upload file Permanent link Page information Get shortened URL Download QR code Print/export Download as PDF Printable version In other projects Appearance move to sidebar hide From Wikipedia, the free encyclopedia This article is rated B-class on Wikipedia's content assessment scale. It is of interest to the following WikiProjects : Computing Mid\u2011importance This article is within the scope of WikiProject Computing , a collaborative effort to improve the coverage of computers , computing , and information technology on Wikipedia. If you would like to participate, please visit the project page, where you can join the discussion and see a list of open tasks. Computing Wikipedia:WikiProject Computing Template:WikiProject Computing Computing Mid This article has been rated as Mid-importance on the project's importance scale . Internet High\u2011importance Internet portal This article is within the scope of WikiProject Internet , a collaborative effort to improve the coverage of the Internet on Wikipedia. If you would like to participate, please visit the project page, where you can join the discussion and see a list of open tasks. Internet Wikipedia:WikiProject Internet Template:WikiProject Internet Internet High This article has been rated as High-importance on the project's importance scale . This article is prone to spam .  Please monitor the References and External links sections. Archives ( index ) Index 1 This page has archives. Sections older than 730 days may be automatically archived by Lowercase sigmabot III when more than 4 sections are present. Phrasing [ edit ] I would like to bring attention to this paragraph on the article: \"The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly.\" I wanted to get other people's insight on how to rephrase this. Akdulj ( talk ) 20:29, 5 February 2023 (UTC) [ reply ] sher alam 188.54.121.141 ( talk ) 01:33, 23 December 2023 (UTC) [ reply ] Low quality citation [ edit ] First citation [ 1 ] in this Wikipedia entry is a website with just one article, hosted on Wordpress, with no data about the article's author, dead for about a year at this point. Due to all of those facts, it seems that the cited article is low effort, and low quality. Maciej B\u0142\u0119dkowski ( talk ) 15:55, 20 January 2024 (UTC) [ reply ] References ^ \"Web Crawlers: Browsing the Web\" . Archived from the original on 2021-12-06. Retrieved from \" https://en.wikipedia.org/w/index.php?title=Talk:Web_crawler&oldid=1257623751 \" Categories : B-Class Computing articles Mid-importance Computing articles All Computing articles B-Class Internet articles High-importance Internet articles WikiProject Internet articles Hidden category: Articles prone to spam from October 2020 This page was last edited on 15 November 2024, at 22:14 (UTC) . Text is available under the Creative Commons Attribution-ShareAlike 4.0 License ;\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Toggle the table of contents Talk : Web crawler Add topic"}, "46": {"url": "https://en.wikipedia.org/w/index.php?title=Web_crawler&action=edit", "text": "Editing Web crawler - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Editing Web crawler Add languages Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Page information Get shortened URL Download QR code In other projects Wikidata item Appearance move to sidebar hide Page notice You are not logged in. Your IP address will be publicly visible if you make any edits. If you log in or create an account , your edits will be attributed to a username, among other benefits . Content that violates any copyrights will be deleted. Encyclopedic content must be verifiable through citations to reliable sources . Anti-spam check.\nDo not fill this in! {{short description|Software which systematically browses the World Wide Web}}\n{{hatnote group|\n{{about|the internet bot|the search engine|WebCrawler}}\n{{redirect-distinguish|Web spider|Spider web}}\n{{redirect|Spiderbot|the video game|Arac (video game)}}\n}}\n{{Use dmy dates|date=September 2020}}\n[[File:WebCrawlerArchitecture.svg|thumb|Architecture of a Web crawler]]\n\n'''Web crawler''', sometimes called a '''spider''' or '''spiderbot''' and often shortened to '''crawler''', is an [[Internet bot]] that systematically browses the [[World Wide Web]] and that is typically operated by search engines for the purpose of [[Web indexing]] (''web spidering'').<ref>{{Cite web |title=Web Crawlers: Browsing the Web |url=https://webbrowsersintroduction.com/ |url-status=dead |archive-url=https://web.archive.org/web/20211206205907/https://webbrowsersintroduction.com/ |archive-date=2021-12-06}}</ref>\n\nWeb [[search engine]]s and some other [[website]]s use Web crawling or spidering [[software]] to update their [[web content]] or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which [[Index (search engine)|indexes]] the downloaded pages so that users can search more efficiently.\n\nCrawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a <code>[[robots.txt]]</code> file can request [[Software agent|bots]] to index only parts of a website, or nothing at all.\n\nThe number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly.\n\nCrawlers can validate [[hyperlink]]s and [[HTML]] code. They can also be used for [[web scraping]] and [[data-driven programming]].\n\n== Nomenclature ==\nA web crawler is also known as a ''spider'',<ref name=\"spekta\">{{cite web|last=Spetka|first=Scott|title=The TkWWW Robot: Beyond Browsing|url=http://archive.ncsa.uiuc.edu/SDG/IT94/Proceedings/Agents/spetka/spetka.html|publisher=[[National Center for Supercomputing Applications|NCSA]]|access-date=21 November 2010 |archive-url=https://web.archive.org/web/20040903174942/http://archive.ncsa.uiuc.edu/SDG/IT94/Proceedings/Agents/spetka/spetka.html |archive-date=3 September 2004}}</ref> an ''ant'', an ''automatic indexer'',<ref>{{cite journal |author1=Kobayashi, M. |author2=Takeda, K. |name-list-style=amp |title = Information retrieval on the web |journal = ACM Computing Surveys|volume = 32 |issue = 2 |pages = 144\u2013173 |year = 2000 |doi = 10.1145/358923.358934|citeseerx=10.1.1.126.6094 |s2cid=3710903 }}</ref> or (in the [[FOAF (software)|FOAF]] software context) a ''Web scutter''.<ref>See [http://wiki.foaf-project.org/w/Scutter definition of scutter on FOAF Project's wiki] {{Webarchive|url=https://web.archive.org/web/20091213213920/http://wiki.foaf-project.org/w/Scutter |date=13 December 2009 }}</ref>\n\n== Overview ==\nA Web crawler starts with a list of [[Uniform Resource Locator|URLs]] to visit. Those first URLs are called the ''seeds''. As the crawler visits these URLs, by communicating with [[web server]]s that respond to those URLs, it identifies all the [[hyperlink]]s in the retrieved web pages and adds them to the list of URLs to visit, called the ''[[crawl frontier]]''. URLs from the frontier are [[Recursion|recursively]] visited according to a set of policies. If the crawler is performing archiving of [[website]]s (or [[web archiving]]), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'.<ref name=\"GoogleBooks-9237380\">{{cite book |url=https://www.springer.com/gp/book/9783540233381 |title=Web Archiving |isbn=978-3-54046332-0 |date=15 February 2007 |publisher=Springer |access-date=24 April 2014 |page=1 |first=Julien |last=Masan\u00e8s}}</ref>\n\nThe archive is known as the ''repository'' and is designed to store and manage the collection of [[web page]]s. The [[Repository (version control)|repository]] only stores [[HTML]] pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler.{{Cn|date=February 2023}}\n\nThe large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted.\n\nThe number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving [[duplicate content]]. Endless combinations of [[HTTP]] GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of [[thumbnail]] size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This [[mathematical combination]] creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content.\n\nAs Edwards ''et al.'' noted, \"Given that the [[Bandwidth (computing)|bandwidth]] for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\"<ref name=edwards2001>{{Cite book |author = Edwards, J.; McCurley, K. S.; and Tomlin, J. A. |title = Proceedings of the 10th international conference on World Wide Web |chapter = An adaptive model for optimizing performance of an incremental web crawler |pages = 106\u2013113 |year = 2001 |doi = 10.1145/371920.371960 |url = http://www10.org/cdrom/papers/210/index.html |isbn = 978-1581133486 |citeseerx = 10.1.1.1018.1506 |s2cid = 10316730 |access-date = 25 January 2007 |archive-date = 25 June 2014 |archive-url = https://web.archive.org/web/20140625233510/http://www10.org/cdrom/papers/210/index.html |url-status = dead }}</ref> A crawler must carefully choose at each step which pages to visit next.\n\n==Crawling policy==\nThe behavior of a Web crawler is the outcome of a combination of policies:<ref>{{Cite thesis |degree=PhD |title=Effective Web Crawling |url=http://chato.cl/research/crawling_thesis |last=Castillo |first=Carlos |year=2004 |publisher=University of Chile|access-date=2010-08-03}}</ref>\n* a ''selection policy'' which states the pages to download,\n* a ''re-visit policy'' which states when to check for changes to the pages,\n* a ''politeness policy'' that states how to avoid overloading [[websites]].\n* a ''parallelization policy'' that states how to coordinate distributed web crawlers.\n\n===Selection policy===\nGiven the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale [[search engines]] index no more than 40\u201370% of the indexable Web;<ref>{{cite conference |title=The indexable web is more than 11.5 billion pages |author=Gulls, A. |author2=A. Signori |year=2005 |publisher=ACM Press |book-title=Special interest tracks and posters of the 14th international conference on World Wide Web |pages=902\u2013903 |doi=10.1145/1062745.1062789 }}</ref> a previous study by [[Steve Lawrence (computer scientist)|Steve Lawrence]] and [[Lee Giles]] showed that no [[Search engine indexing|search engine indexed]] more than 16% of the Web in 1999.<ref>{{Cite journal | doi = 10.1038/21987 | volume = 400 | issue = 6740 | pages = 107\u20139 | author1= Lawrence, Steve | author2 = C. Lee Giles | title = Accessibility of information on the web | journal = Nature | date = 1999-07-08 | pmid = 10428673 |bibcode = 1999Natur.400..107L | s2cid = 4347646 | doi-access = free }}</ref> As a crawler always downloads just a fraction of the [[Web pages]], it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web.\n\nThis requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its [[Intrinsic and extrinsic properties (philosophy)|intrinsic]] quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of [[vertical search|vertical search engines]] restricted to a single [[top-level domain]], or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling.\n\nJunghoo Cho ''et al.'' made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the <kbd>stanford.edu</kbd> domain, in which a crawling simulation was done with different strategies.<ref>{{cite journal |author=Cho, J. |author2=Garcia-Molina, H. |author3=Page, L. |title = Efficient Crawling Through URL Ordering |url = http://ilpubs.stanford.edu:8090/347/ |journal = Seventh International World-Wide Web Conference |date = April 1998 |doi=10.1142/3725 |isbn=978-981-02-3400-3 |place = Brisbane, Australia |access-date = 2009-03-23}}</ref> The ordering metrics tested were [[Breadth-first search|breadth-first]], [[backlink]] count and partial [[PageRank]] calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling.<ref>Cho, Junghoo, [http://oak.cs.ucla.edu/~cho/papers/cho-thesis.pdf \"Crawling the Web: Discovery and Maintenance of a Large-Scale Web Data\"], PhD dissertation, Department of Computer Science, Stanford University, November 2001.</ref>\n\nNajork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering.<ref>Najork, Marc and Janet L. Wiener. [http://www10.org/cdrom/papers/pdf/p208.pdf \"Breadth-first crawling yields high-quality pages\".] {{Webarchive|url=https://web.archive.org/web/20171224210412/http://www10.org/cdrom/papers/pdf/p208.pdf |date=24 December 2017 }} In: ''Proceedings of the Tenth Conference on World Wide Web'', pages 114\u2013118, Hong Kong, May 2001. Elsevier Science.</ref> They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\"\n\nAbiteboul designed a crawling strategy based on an [[algorithm]] called OPIC (On-line Page Importance Computation).<ref>{{cite conference |publisher = ACM |doi = 10.1145/775152.775192 |isbn = 1-58113-680-3 |pages = 280\u2013290 |author1 = Abiteboul, Serge |author2 = Mihai Preda |author3 = Gregory Cobena |title = Adaptive on-line page importance computation |book-title = Proceedings of the 12th international conference on World Wide Web |location = Budapest, Hungary |access-date = 2009-03-22 |year = 2003 | url = http://www2003.org/cdrom/papers/refereed/p007/p7-abiteboul.html}}</ref> In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web.\n\nBoldi ''et al.'' used simulation on subsets of the Web of 40 million pages from the <kbd>.it</kbd> domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations.<ref>{{cite journal |doi = 10.1002/spe.587 |volume = 34 |issue = 8 |pages = 711\u2013726 |author1 =Boldi, Paolo |author2 = Bruno Codenotti |author3 = Massimo Santini |author4 = Sebastiano Vigna |title = UbiCrawler: a scalable fully distributed Web crawler |journal = Software: Practice and Experience |access-date = 2009-03-23 |year = 2004 |url = http://vigna.dsi.unimi.it/ftp/papers/UbiCrawler.pdf |citeseerx = 10.1.1.2.5538 |s2cid = 325714 |archive-date = 20 March 2009 |archive-url = https://web.archive.org/web/20090320030833/http://vigna.dsi.unimi.it/ftp/papers/UbiCrawler.pdf |url-status = dead }}</ref><ref>{{cite book |pages = 168\u2013180 |author1 = Boldi, Paolo |author2 = Massimo Santini |author3 = Sebastiano Vigna |title = Algorithms and Models for the Web-Graph |volume = 3243 |chapter = Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations |access-date = 2009-03-23 |year = 2004 |chapter-url = http://vigna.dsi.unimi.it/ftp/papers/ParadoxicalPageRank.pdf |doi = 10.1007/978-3-540-30216-2_14 |series = Lecture Notes in Computer Science |isbn = 978-3-540-23427-2 |archive-date = 1 October 2005 |archive-url = https://web.archive.org/web/20051001063017/http://vigna.dsi.unimi.it/ftp/papers/ParadoxicalPageRank.pdf |url-status = dead }}</ref>\n\nBaeza-Yates ''et al.'' used simulation on two subsets of the Web of 3 million pages from the <kbd>.gr</kbd> and <kbd>.cl</kbd> domain, testing several crawling strategies.<ref name=baeza2005>Baeza-Yates, R.; Castillo, C.;  Marin, M. and Rodriguez, A. (2005). [http://chato.cl/papers/baeza05_crawling_country_better_breadth_first_web_page_ordering.pdf \"Crawling a Country: Better Strategies than Breadth-First for Web Page Ordering.\"] In: ''Proceedings of the Industrial and Practical Experience track of the 14th conference on World Wide Web'', pages 864\u2013872, Chiba, Japan. ACM Press.</ref> They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than [[Breadth-first search|breadth-first]] crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one.\n\nDaneshpajouh ''et al.'' designed a community based algorithm for discovering good seeds.<ref>Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, Mohammad Ghodsi, [https://www.researchgate.net/publication/220724572_A_Fast_Community_Based_Algorithm_for_Generating_Web_Crawler_Seeds_Set A Fast Community Based Algorithm for Generating Crawler Seeds Set]. In: ''Proceedings of 4th International Conference on Web Information Systems and Technologies'' (''[[Webist]]''-2008), Funchal, Portugal, May 2008.</ref> Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective.\n\n====Restricting followed links====\nA crawler may only want to seek out HTML pages and avoid all other [[Internet media type|MIME types]]. In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped.\n\nSome crawlers may also avoid requesting any resources that have a [[Query string|\"?\"]] in them (are dynamically produced) in order to avoid [[spider trap]]s that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses [[URL rewriting]] to simplify its URLs.\n\n====URL normalization====\n{{Main|URL normalization}}\n\nCrawlers usually perform some type of [[URL normalization]] in order to avoid crawling the same resource more than once. The term ''URL normalization'', also called ''URL canonicalization'', refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component.<ref>{{cite book |first1 = Gautam |last1 = Pant |first2 = Padmini |last2 = Srinivasan |first3 = Filippo |last3 = Menczer |editor-last = Levene |editor-first = Mark |editor2-last = Poulovassilis |editor2-first = Alexandra |editor2-link = Alexandra Poulovassilis |contribution = Crawling the Web |contribution-url = http://dollar.biz.uiowa.edu/~pant/Papers/crawling.pdf |title = Web Dynamics: Adapting to Change in Content, Size, Topology and Use |year = 2004 |pages = 153\u2013178 |publisher = Springer |isbn = 978-3-540-40676-1 |access-date = 9 May 2006 |archive-date = 20 March 2009 |archive-url = https://web.archive.org/web/20090320030832/http://dollar.biz.uiowa.edu/~pant/Papers/crawling.pdf |url-status = dead }}</ref>\n\n====Path-ascending crawling====\nSome crawlers intend to download/upload as many resources as possible from a particular web site. So ''path-ascending crawler'' was introduced that would ascend to every path in each URL that it intends to crawl.<ref>{{Cite journal | doi = 10.1002/asi.20078 | volume = 55 | issue = 14 | pages = 1228\u20131238 | last = Cothey | first = Viv | title = Web-crawling reliability | journal = Journal of the American Society for Information Science and Technology | year = 2004 | url = http://www.scit.wlv.ac.uk/~in7803/publications/cothey_2004.pdf | citeseerx = 10.1.1.117.185 }}</ref> For example, when given a seed URL of <nowiki>http://llama.org/hamster/monkey/page.html</nowiki>, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling.\n\n====Focused crawling====\n{{Main|Focused crawler}}\n\nThe importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called '''focused crawler''' or '''topical crawlers'''. The concepts of topical and focused crawling were first introduced by [[Filippo Menczer]]<ref>Menczer, F. (1997). [http://informatics.indiana.edu/fil/Papers/ICML.ps ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery] {{Webarchive|url=https://web.archive.org/web/20121221113620/http://informatics.indiana.edu/fil/Papers/ICML.ps |date=21 December 2012 }}. In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann</ref><ref>Menczer, F. and Belew, R.K. (1998). [http://informatics.indiana.edu/fil/Papers/AA98.ps Adaptive Information Agents in Distributed Textual Environments] {{Webarchive|url=https://web.archive.org/web/20121221113630/http://informatics.indiana.edu/fil/Papers/AA98.ps |date=21 December 2012 }}. In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press</ref> and by Soumen Chakrabarti ''et al.''<ref>{{cite journal|url=http://www.fxpal.com/people/vdberg/pubs/www8/www1999f.pdf|archive-url=https://web.archive.org/web/20040317210216/http://www.fxpal.com/people/vdberg/pubs/www8/www1999f.pdf|url-status=dead|archive-date=2004-03-17|doi=10.1016/s1389-1286(99)00052-3|title=Focused crawling: A new approach to topic-specific Web resource discovery|journal=Computer Networks|volume=31|issue=11\u201316|pages=1623\u20131640|year=1999|last1=Chakrabarti|first1=Soumen|last2=Van Den Berg|first2=Martin|last3=Dom|first3=Byron}}</ref>\n\nThe main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton<ref name=pinkerton1994>Pinkerton, B. (1994). [https://web.archive.org/web/20010904075500/http://archive.ncsa.uiuc.edu/SDG/IT94/Proceedings/Searching/pinkerton/WebCrawler.html Finding what people want: Experiences with the WebCrawler]. In Proceedings of the First World Wide Web Conference, Geneva, Switzerland.</ref> in the first web crawler of the early days of the Web. Diligenti ''et al.''<ref>Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., and Gori, M. (2000). [http://clgiles.ist.psu.edu/papers/VLDB-2000-focused-crawling.pdf Focused crawling using context graphs]. In Proceedings of 26th International Conference on Very Large Databases (VLDB), pages 527-534, Cairo, Egypt.</ref> propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points.\n\n=====Academic focused crawler=====\nAn example of the [[focused crawlers]] are academic crawlers, which crawls free-access academic related documents, such as the ''citeseerxbot'', which is the crawler of [[CiteSeer]]<sup>X</sup> search engine. Other academic search engines are [[Google Scholar]] and [[Microsoft Academic Search]] etc. Because most academic papers are published in [[PDF]] formats, such kind of crawler is particularly interested in crawling PDF, [[PostScript]] files, [[Microsoft Word]] including their [[Zipped file|zipped]] formats. Because of this, general open-source crawlers, such as [[Heritrix]], must be customized to filter out other [[MIME types]], or a [[middleware]] is used to extract these documents out and import them to the focused crawl database and repository.<ref>{{Cite book | doi=10.1145/2389936.2389949| chapter=Web crawler middleware for search engine digital libraries| title=Proceedings of the twelfth international workshop on Web information and data management - WIDM '12| pages=57| year=2012| last1=Wu| first1=Jian| last2=Teregowda| first2=Pradeep| last3=Khabsa| first3=Madian| last4=Carman| first4=Stephen| last5=Jordan| first5=Douglas| last6=San Pedro Wandelmer| first6=Jose| last7=Lu| first7=Xin| last8=Mitra| first8=Prasenjit| last9=Giles| first9=C. Lee| isbn=9781450317207| s2cid=18513666}}</ref> Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using [[machine learning]] or [[regular expression]] algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers.<ref>{{Cite book |doi = 10.1145/2380718.2380762|chapter = The evolution of a crawling strategy for an academic document search engine|title = Proceedings of the 3rd Annual ACM Web Science Conference on - Web ''Sci'' '12|pages = 340\u2013343|year = 2012|last1 = Wu|first1 = Jian|last2 = Teregowda|first2 = Pradeep|last3 = Ram\u00edrez|first3 = Juan Pablo Fern\u00e1ndez|last4 = Mitra|first4 = Prasenjit|last5 = Zheng|first5 = Shuyi|last6 = Giles|first6 = C. Lee|isbn = 9781450312288|s2cid = 16718130}}</ref> Other academic crawlers may download plain text and [[HTML]] files, that contains [[metadata]] of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads.\n\n=====Semantic focused crawler=====\nAnother type of focused crawlers is semantic focused crawler, which makes use of [[domain ontology|domain ontologies]] to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes.<ref>{{cite book|chapter-url=https://www.researchgate.net/publication/44241179|doi=10.1007/978-3-642-02457-3_74|chapter=State of the Art in Semantic Focused Crawlers|title=Computational Science and Its Applications \u2013 ICCSA 2009|volume=5593|pages=910\u2013924|series=Lecture Notes in Computer Science|year=2009|last1=Dong|first1=Hai|last2=Hussain|first2=Farookh Khadeer|last3=Chang|first3=Elizabeth|isbn=978-3-642-02456-6|hdl=20.500.11937/48288}}</ref> In addition, ontologies can be automatically updated in the crawling process. Dong et al.<ref>{{cite journal|url=https://www.researchgate.net/publication/264620349|doi=10.1002/cpe.2980|title=SOF: A semi-supervised ontology-learning-based focused crawler|journal=Concurrency and Computation: Practice and Experience|volume=25|issue=12|pages=1755\u20131770|year=2013|last1=Dong|first1=Hai|last2=Hussain|first2=Farookh Khadeer|s2cid=205690364}}</ref> introduced such an ontology-learning-based crawler using a [[support-vector machine]] to update the content of ontological concepts when crawling Web pages.\n\n===Re-visit policy===\nThe Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions.\n\nFrom the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age.<ref>{{Cite conference | publisher = ACM | doi = 10.1145/342009.335391 | isbn = 1-58113-217-4 | pages = 117\u2013128 | author1 = Junghoo Cho | author2 = Hector Garcia-Molina | title = Synchronizing a database to improve freshness | book-title = Proceedings of the 2000 ACM SIGMOD international conference on Management of data | location = Dallas, Texas, United States | access-date = 2009-03-23 | year = 2000 | url = http://www.cs.brown.edu/courses/cs227/2002/cache/Cho.pdf}}</ref>\n\n'''Freshness''': This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page ''p'' in the repository at time ''t'' is defined as:\n\n: <math>\nF_p(t) = \\begin{cases} 1 & {\\rm\nif}~p~{\\rm~is~equal~to~the~local~copy~at~time}~t\\\\ 0 & {\\rm otherwise} \\end{cases}\n</math>\n\n'''Age''': This is a measure that indicates how outdated the local copy is. The age of a page ''p'' in the repository, at time ''t'' is defined as:\n\n: <math>\nA_p(t) = \\begin{cases} 0 & {\\rm if}~p~{\\rm~is~not~modified~at~time}~t\\\\ t - {\\rm modification~time~of}~p &\n{\\rm otherwise} \\end{cases}\n</math>\n\n[[Edward G. Coffman, Jr.|Coffman]] ''et al.'' worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler.<ref name=\"coffman\">{{Cite journal | doi = 10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K | volume = 1 | issue = 1 | pages = 15\u201329 | author1 = E. G. Coffman Jr | author2 = Zhen Liu | author3 = Richard R. Weber | title = Optimal robot scheduling for Web search engines | journal = Journal of Scheduling | year = 1998 | citeseerx = 10.1.1.36.6087 }}</ref>\n\nThe objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are.\n\n[[File:Web Crawling Freshness Age.png|thumb|Evolution of Freshness and Age in a web crawler]]\nTwo simple re-visiting policies were studied by Cho and Garcia-Molina:<ref name=cho2003>{{Cite journal |doi = 10.1145/958942.958945|title = Effective page refresh policies for Web crawlers|journal = ACM Transactions on Database Systems|volume = 28|issue = 4|pages = 390\u2013426|year = 2003|last1 = Cho|first1 = Junghoo|last2 = Garcia-Molina|first2 = Hector|s2cid = 147958}}</ref>\n* Uniform policy: This involves re-visiting all pages in the collection with the same frequency, regardless of their rates of change.\n* Proportional policy: This involves re-visiting more often the pages that change more frequently. The visiting frequency is directly proportional to the (estimated) change frequency.\n\nIn both cases, the repeated crawling order of pages can be done either in a random or a fixed order.\n\nCho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them.\n\nTo improve freshness, the crawler should penalize the elements that change too often.<ref name=\"cho2003a\">{{Cite journal | doi = 10.1145/857166.857170 | volume = 3 | issue = 3 | pages = 256\u2013290 | author1 = Junghoo Cho | author2 = Hector Garcia-Molina | title = Estimating frequency of change | journal =  ACM Transactions on Internet Technology| year = 2003 | citeseerx = 10.1.1.59.5877 | s2cid = 9362566 }}</ref> The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as [[Edward G. Coffman, Jr.|Coffman]] ''et al.'' note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\".<ref name=\"coffman\" /> Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes,<ref name=\"cho2003a\" /> while [[Panos Ipeirotis|Ipeirotis]] ''et al.'' show how to use statistical tools to discover parameters that affect this distribution.<ref>Ipeirotis, P., Ntoulas, A., Cho, J., Gravano, L. (2005) [http://pages.stern.nyu.edu/~panos/publications/icde2005.pdf Modeling and managing content changes in text databases] {{Webarchive|url=https://web.archive.org/web/20050905013119/http://pages.stern.nyu.edu/~panos/publications/icde2005.pdf |date=5 September 2005 }}. In Proceedings of the 21st IEEE International Conference on Data Engineering, pages 606-617, April 2005, Tokyo.</ref> The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy.\n\n===Politeness policy===\nCrawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers.\n\nAs noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community.<ref>Koster, M. (1995). Robots in the web: threat or treat? ConneXions, 9(4).</ref> The costs of using Web crawlers include:\n* network resources, as crawlers require considerable bandwidth and operate with a high degree of parallelism during a long period of time;\n* server overload, especially if the frequency of accesses to a given server is too high;\n* poorly written crawlers, which can crash servers or routers, or which download pages they cannot handle; and\n* personal crawlers that, if deployed by too many users, can disrupt networks and Web servers.\n\nA partial solution to these problems is the [[Robots Exclusion Standard|robots exclusion protocol]], also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers.<ref>Koster, M. (1996). [http://www.robotstxt.org/wc/exclusion.html A standard for robot exclusion] {{Webarchive|url=https://web.archive.org/web/20071107021800/http://www.robotstxt.org/wc/exclusion.html |date=7 November 2007 }}.</ref> This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like [[Google.com|Google]], [[Ask.com|Ask Jeeves]], [[Bing (search engine)|MSN]] and [[Yahoo! Search]] are able to use an extra \"Crawl-delay:\" parameter in the [[robots.txt]] file to indicate the number of seconds to delay between requests.\n\nThe first proposed interval between successive pageloads was 60 seconds.<ref>Koster, M. (1993). [http://www.robotstxt.org/wc/guidelines.html Guidelines for robots writers] {{Webarchive|url=https://web.archive.org/web/20050422045839/http://www.robotstxt.org/wc/guidelines.html |date=22 April 2005 }}.</ref> However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used.\n\nCho uses 10 seconds as an interval for accesses,<ref name=cho2003/> and the WIRE crawler uses 15 seconds as the default.<ref name=baeza2002>Baeza-Yates, R. and Castillo, C. (2002). [http://www.chato.cl/papers/baeza02balancing.pdf Balancing volume, quality and freshness in Web crawling]. In Soft Computing Systems&nbsp;\u2013 Design, Management and Applications, pages 565\u2013572, Santiago, Chile. IOS Press Amsterdam.</ref> The MercatorWeb crawler follows an adaptive politeness policy: if it took ''t'' seconds to download a document from a given server, the crawler waits for 10''t'' seconds before downloading the next page.<ref>{{cite journal |author1=Heydon, Allan |author2=Najork, Marc |title=Mercator: A Scalable, Extensible Web Crawler |date=1999-06-26 |url=http://www.cindoc.csic.es/cybermetrics/pdf/68.pdf |access-date=2009-03-22 |url-status=dead |archive-url=https://web.archive.org/web/20060219085958/http://www.cindoc.csic.es/cybermetrics/pdf/68.pdf |archive-date=19 February 2006}}</ref> Dill ''et al.'' use 1 second.<ref>{{cite journal |last1 = Dill |first1 = S. |last2 = Kumar |first2 = R. |last3 = Mccurley |first3 = K. S. |last4 = Rajagopalan |first4 = S. |last5 = Sivakumar |first5 = D. |last6 = Tomkins |first6 = A. |year = 2002 |title = Self-similarity in the web |url = http://www.mccurley.org/papers/fractal.pdf |journal = ACM Transactions on Internet Technology|volume = 2 |issue = 3 |pages = 205\u2013223 |doi=10.1145/572326.572328|s2cid = 6416041 }}</ref>\n\nFor those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl.<ref>{{Cite journal| author1 = M. Thelwall | author2 = D. Stuart | year = 2006 | url = http://www.scit.wlv.ac.uk/%7Ecm1993/papers/Web_Crawling_Ethics_preprint.doc | title = Web crawling ethics revisited: Cost, privacy and denial of service | volume = 57 | issue = 13 | pages = 1771\u20131779 | journal = Journal of the American Society for Information Science and Technology| doi = 10.1002/asi.20388 }}</ref>\n\nAnecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3\u20134 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. [[Sergey Brin]] and [[Larry Page]] noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\"<ref name=brin1998>{{cite journal|url=http://infolab.stanford.edu/~backrub/google.html|doi=10.1016/s0169-7552(98)00110-x|title=The anatomy of a large-scale hypertextual Web search engine|journal=Computer Networks and ISDN Systems|volume=30|issue=1\u20137|pages=107\u2013117|year=1998|last1=Brin|first1=Sergey|last2=Page|first2=Lawrence|s2cid=7587743 }}</ref>\n\n===Parallelization policy===\n{{Main|Distributed web crawling}}\n\nA [[Parallel computing|parallel]] crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes.\n\n==Architectures==\n[[File:WebCrawlerArchitecture.svg|thumb|300px|High-level architecture of a standard Web crawler]]\nA crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture.\n\nShkapenyuk and Suel noted that:<ref name=shkapenyuk2002>Shkapenyuk, V. and [[Torsten Suel|Suel, T.]] (2002). [http://cis.poly.edu/tr/tr-cis-2001-03.pdf Design and implementation of a high performance distributed web crawler]. In Proceedings of the 18th International Conference on Data Engineering (ICDE), pages 357-368, San Jose, California. IEEE CS Press.</ref>\n\n{{quote|While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability.}}\n\nWeb crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \"[[Spamdexing|search engine spamming]]\", which prevent major search engines from publishing their ranking algorithms.\n\n==Security==\nWhile most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in [[Web search engine|search engines]], web crawling can also have [[unintended consequences]] and lead to a [[Web application security|compromise]] or [[data breach]] if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software.\n\n{{main|Google hacking}}\n\nApart from standard [[web application security]] recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with [[Robots exclusion standard|robots.txt]]) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.).\n\n==Crawler identification==\nWeb crawlers typically identify themselves to a Web server by using the [[user agent|User-agent]] field of an [[HTTP]] request. Web site administrators typically examine their [[Web server]]s' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a [[Uniform Resource Locator|URL]] where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. [[Spambots]] and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler.\n\nWeb site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a [[spider trap|crawler trap]] or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular [[Web search engine|search engine]].\n\n==Crawling the deep web==\nA vast amount of web pages lie in the [[Deep Web (search indexing)|deep or invisible web]].<ref>Shestakov, Denis (2008). [https://oa.doria.fi/handle/10024/38506 ''Search Interfaces on the Web: Querying and Characterizing''] {{Webarchive|url=https://web.archive.org/web/20140706120641/https://oa.doria.fi/handle/10024/38506 |date=6 July 2014 }}. TUCS Doctoral Dissertations 104, University of Turku</ref> These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's [[Sitemaps]] protocol and [[mod oai]]<ref>{{Cite journal|arxiv=cs/0503069 |author1=Michael L Nelson |author2=Herbert Van de Sompel |author3=Xiaoming Liu |author4=Terry L Harrison |author5=Nathan McFarland |title=mod_oai: An Apache Module for Metadata Harvesting |pages=cs/0503069 |date=2005-03-24|bibcode=2005cs........3069N }}</ref> are intended to allow discovery of these [[Deep web|deep-Web]] resources.\n\nDeep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <code>&lt;a&nbsp;href=\"URL\"&gt;</code> form. In some cases, such as the [[Googlebot]], Web crawling is done on all text contained inside the hypertext content, tags, or text.\n\nStrategic approaches may be taken to target deep Web content. With a technique called [[screen scraping]], specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers.<ref>{{cite journal\n | first1 = Denis\n | last1 = Shestakov\n | first2 = Sourav S. | last2 = Bhowmick | first3 = Ee-Peng | last3 = Lim\n | title = DEQUE: Querying the Deep Web\n | journal = Data & Knowledge Engineering |volume=52 |issue=3\n | pages = 273\u2013311\n | year = 2005\n | url = http://www.mendeley.com/download/public/1423991/3893295922/dc0f7d824fd2a8fbbc84f6fdf9e4f337d343987d/dl.pdf\n | doi=10.1016/s0169-023x(04)00107-7}}</ref>\n\nPages built on [[AJAX]] are among those causing problems to web crawlers. [[Google search engine|Google]] has proposed a format of AJAX calls that their bot can recognize and index.<ref>{{cite web | url=https://support.google.com/webmasters/bin/answer.py?hl=en&answer=174992 | title=AJAX crawling: Guide for webmasters and developers | access-date=17 March 2013}}</ref>\n\n== Visual vs programmatic crawlers ==\nThere are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data.\n\nThe visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs<ref>ITA Labs [http://semanticweb.com/more-semantics-for-google-ita-software-acquisition-comes-with-needlebase-too_b19329 \"ITA Labs Acquisition\"] {{Webarchive|url=https://web.archive.org/web/20140318134631/http://semanticweb.com/more-semantics-for-google-ita-software-acquisition-comes-with-needlebase-too_b19329 |date=18 March 2014 }} 20 April 2011 1:28 AM</ref>), there is continued growth and investment in this area by investors and end-users.{{citation needed|date=June 2021}}\n\n== List of web crawlers ==\n{{further|List of search engine software}}\n\nThe following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features:\n\n=== Historical web crawlers ===\n<!-- PLEASE RESPECT ALPHABETICAL ORDER -->\n* [https://web.archive.org/web/20020106130042/http://www.wolfbot.com/ WolfBot] was a massively multi threaded crawler built in 2001 by Mani Singh a Civil Engineering graduate from the University of California at Davis.\n* [[World Wide Web Worm]] was a crawler used to build a simple index of document titles and URLs. The index could be searched by using the <kbd>[[grep]]</kbd> [[Unix]] command.\n* Yahoo! Slurp was the name of the [[Yahoo!]] Search crawler until Yahoo! contracted with [[Microsoft]] to use [[Bingbot]] instead.\n\n=== In-house web crawlers ===\n\n<!-- PLEASE RESPECT ALPHABETICAL ORDER -->\n* Applebot is [[Apple (company)|Apple]]'s web crawler. It supports [[Siri]] and other products.<ref>{{cite web |title=About Applebot |url=https://support.apple.com/en-us/HT204683 |publisher=Apple Inc |access-date=18 October 2021}}</ref>\n* [[Bingbot]] is the name of Microsoft's [[Bing (search engine)|Bing]] webcrawler. It replaced ''[[Msnbot]]''.\n* Baiduspider is [[Baidu]]'s web crawler.\n* DuckDuckBot is [[DuckDuckGo]]'s web crawler.\n* [[Googlebot]] is described in some detail, but the reference is only about an early version of its architecture, which was written in C++ and [[Python (programming language)|Python]]. The crawler was integrated with the indexing process, because text parsing was done for full-text indexing and also for URL extraction. There is a URL server that sends lists of URLs to be fetched by several crawling processes. During parsing, the URLs found were passed to a URL server that checked if the URL have been previously seen. If not, the URL was added to the queue of the URL server.\n* [[WebCrawler]] was used to build the first publicly available full-text index of a subset of the Web. It was based on [[Libwww|lib-WWW]] to download pages, and another program to parse and order URLs for breadth-first exploration of the Web graph. It also included a real-time crawler that followed links based on the similarity of the anchor text with the provided query.\n* [[WebFountain]] is a distributed, modular crawler similar to Mercator but written in C++.\n* [[Xenon (program)|Xenon]] is a web crawler used by government tax authorities to detect fraud.{{r|Norton-2007-01-25}}{{r|Canada-2017-04-11}}\n\n=== Commercial web crawlers ===\nThe following web crawlers are available, for a price::\n* [[Diffbot]] - programmatic general web crawler, available as an [[API]]\n* [[SortSite]] - crawler for analyzing websites, available for [[Microsoft Windows|Windows]] and [[Macintosh operating systems|Mac OS]]\n* Swiftbot - [[Swiftype]]'s web crawler, available as [[software as a service]]\n* Aleph Search - web crawler allowing massive collection with high scalability\n\n=== Open-source crawlers ===\n\n<!-- PLEASE RESPECT ALPHABETICAL ORDER -->\n* [[Apache Nutch]] is a highly extensible and scalable web crawler written in Java and released under an [[Apache License]]. It is based on [[Apache Hadoop]] and can be used with [[Apache Solr]] or [[Elasticsearch]].\n* [[Grub (search engine)|Grub]] was an open source distributed search crawler that [[Wikia Search]] used to crawl the web.\n* [[Heritrix]] is the [[Internet Archive]]'s archival-quality crawler, designed for archiving periodic snapshots of a large portion of the Web. It was written in [[Java (programming language)|Java]].\n* [[Ht-//dig|ht://Dig]] includes a Web crawler in its indexing engine.\n* [[HTTrack]] uses a Web crawler to create a mirror of a web site for off-line viewing. It is written in [[C (programming language)|C]] and released under the GPL.\n* Norconex Web Crawler is a highly extensible Web Crawler written in [[Java (programming language)|Java]] and released under an [[Apache License]]. It can be used with many repositories such as [[Apache Solr]], [[Elasticsearch]], [[Azure Cognitive Search|Microsoft Azure Cognitive Search]], [[Amazon CloudSearch]] and more.\n* [[mnoGoSearch]] is a crawler, indexer and a search engine written in C and licensed under the GPL (*NIX machines only)\n* [[Open Search Server]] is a search engine and web crawler software release under the GPL.\n* [[Scrapy]], an open source webcrawler framework, written in python (licensed under [[BSD License|BSD]]).\n* [[Seeks]], a free distributed search engine (licensed under [[GNU Affero General Public License|AGPL]]).\n* [[StormCrawler]], a collection of resources for building low-latency, scalable web crawlers on [[Storm (event processor)|Apache Storm]] (Apache License).\n* [[tkWWW Robot]], a crawler based on the [[tkWWW]] web browser (licensed under GPL).\n* [[Wget|GNU Wget]] is a [[Command line interface|command-line]]-operated crawler written in [[C (programming language)|C]] and released under the [[GNU General Public License|GPL]]. It is typically used to mirror Web and FTP sites.\n* [[YaCy]], a free distributed search engine, built on principles of peer-to-peer networks (licensed under GPL).\n\n==See also==\n* [[Automatic indexing]]\n* [[Gnutella crawler]]\n* [[Web archiving]]\n* [[Webgraph]]\n* [[Website mirroring software]]\n* [[Search Engine Scraping]]\n* [[Web scraping]]\n\n==References==\n{{Reflist | 30em | refs =\n<ref name        = Canada-2017-04-11>\n{{cite web\n | url           = https://www.canada.ca/en/revenue-agency/services/about-canada-revenue-agency-cra/protecting-your-privacy/privacy-impact-assessment/xenon-web-crawling-initiative-privacy-impact-assessment-summary.html\n | date          = 11 April 2017\n | title         = Xenon web crawling initiative: privacy impact assessment (PIA) summary\n | publisher     = Government of Canada\n | location      = Ottawa\n | access-date   = 2017-10-13\n | archive-url   = https://web.archive.org/web/20170925203155/https://www.canada.ca/en/revenue-agency/services/about-canada-revenue-agency-cra/protecting-your-privacy/privacy-impact-assessment/xenon-web-crawling-initiative-privacy-impact-assessment-summary.html\n | url-status      = live\n | archive-date  = 2017-09-25\n}}</ref>\n<ref name        = Norton-2007-01-25>\n{{cite news\n | url           = https://www.wired.com/2007/01/tax-takers-send-in-the-spiders/\n | last          = Norton\n | first         = Quinn\n | date          = 25 January 2007\n | title         = Tax takers send in the spiders\n | magazine    = [[Wired (magazine)|Wired]]\n | department    = Business\n | access-date   = 2017-10-13\n | archive-url   = https://web.archive.org/web/20161222185208/https://www.wired.com/2007/01/tax-takers-send-in-the-spiders/\n | url-status      = live\n | archive-date  = 2016-12-22\n}}</ref>\n}}\n\n==Further reading==\n* Cho, Junghoo, [http://oak.cs.ucla.edu/~cho/research/crawl.html \"Web Crawling Project\"], UCLA Computer Science Department.\n* [http://www.wiley.com/legacy/compbooks/sonnenreich/history.html A History of Search Engines], from [[John Wiley & Sons|Wiley]]\n* [https://github.com/bedirhan/wivet WIVET] is a benchmarking project by [[OWASP]], which aims to measure if a web crawler can identify all the hyperlinks in a target website.\n* Shestakov, Denis, [http://www.slideshare.net/denshe/icwe13-tutorial-webcrawling \"Current Challenges in Web Crawling\"] and [http://www.slideshare.net/denshe/intelligent-crawling-shestakovwiiat13 \"Intelligent Web Crawling\"], slides for tutorials given at ICWE'13 and WI-IAT'13.\n{{Internet search}}\n{{Web crawlers}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Web crawler}}\n[[Category:Search engine software]]\n[[Category:Web crawlers| ]]\n[[Category:Internet search algorithms]] Edit summary (Briefly describe your changes) By publishing changes, you agree to the Terms of Use , and you irrevocably agree to release your contribution under the CC BY-SA 4.0 License and the GFDL . You agree that a hyperlink or URL is sufficient attribution under the Creative Commons license. Cancel Editing help (opens in new window) Copy and paste: \u2013 \u2014 \u00b0 \u2032 \u2033 \u2248 \u2260 \u2264 \u2265 \u00b1 \u2212 \u00d7 \u00f7 \u2190 \u2192 \u00b7 \u00a7 Cite your sources: <ref></ref> {{}} \u00a0 {{{}}} \u00a0 | \u00a0 [] \u00a0 [[]] \u00a0 [[Category:]] \u00a0 #REDIRECT [[]] \u00a0 &nbsp; \u00a0 <s></s> \u00a0 <sup></sup> \u00a0 <sub></sub> \u00a0 <code></code> \u00a0 <pre></pre> \u00a0 <blockquote></blockquote> \u00a0 <ref></ref> <ref name=\"\" /> \u00a0 {{Reflist}} \u00a0 <references /> \u00a0 <includeonly></includeonly> \u00a0 <noinclude></noinclude> \u00a0 {{DEFAULTSORT:}} \u00a0 <nowiki></nowiki> \u00a0 <!-- --> \u00a0 <span class=\"plainlinks\"></span> Symbols: ~ | \u00a1 \u00bf \u2020 \u2021 \u2194 \u2191 \u2193 \u2022 \u00b6 \u00a0 # \u221e \u00a0 \u2039\u203a \u00ab\u00bb \u00a0 \u00a4 \u20b3 \u0e3f \u20b5 \u00a2 \u20a1 \u20a2 $ \u20ab \u20af \u20ac \u20a0 \u20a3 \u0192 \u20b4 \u20ad \u20a4 \u2133 \u20a5 \u20a6 \u20a7 \u20b0 \u00a3 \u17db \u20a8 \u20aa \u09f3 \u20ae \u20a9 \u00a5 \u00a0 \u2660 \u2663 \u2665 \u2666 \u00a0 \ud834\udd2b \u266d \u266e \u266f \ud834\udd2a \u00a0 \u00a9 \u00bc \u00bd \u00be Latin: A a \u00c1 \u00e1 \u00c0 \u00e0 \u00c2 \u00e2 \u00c4 \u00e4 \u01cd \u01ce \u0102 \u0103 \u0100 \u0101 \u00c3 \u00e3 \u00c5 \u00e5 \u0104 \u0105 \u00c6 \u00e6 \u01e2 \u01e3 \u00a0 B b \u00a0 C c \u0106 \u0107 \u010a \u010b \u0108 \u0109 \u010c \u010d \u00c7 \u00e7 \u00a0 D d \u010e \u010f \u0110 \u0111 \u1e0c \u1e0d \u00d0 \u00f0 \u00a0 E e \u00c9 \u00e9 \u00c8 \u00e8 \u0116 \u0117 \u00ca \u00ea \u00cb \u00eb \u011a \u011b \u0114 \u0115 \u0112 \u0113 \u1ebc \u1ebd \u0118 \u0119 \u1eb8 \u1eb9 \u0190 \u025b \u018e \u01dd \u018f \u0259 \u00a0 F f \u00a0 G g \u0120 \u0121 \u011c \u011d \u011e \u011f \u0122 \u0123 \u00a0 H h \u0124 \u0125 \u0126 \u0127 \u1e24 \u1e25 \u00a0 I i \u0130 \u0131 \u00cd \u00ed \u00cc \u00ec \u00ce \u00ee \u00cf \u00ef \u01cf \u01d0 \u012c \u012d \u012a \u012b \u0128 \u0129 \u012e \u012f \u1eca\u2009\u1ecb \u00a0 J j \u0134 \u0135 \u00a0 K k \u0136 \u0137 \u00a0 L l \u0139 \u013a \u013f \u0140 \u013d \u013e \u013b \u013c \u0141 \u0142 \u1e36 \u1e37 \u1e38 \u1e39 \u00a0 M m \u1e42 \u1e43 \u00a0 N n \u0143 \u0144 \u0147 \u0148 \u00d1 \u00f1 \u0145 \u0146 \u1e46 \u1e47 \u014a \u014b \u00a0 O o \u00d3 \u00f3 \u00d2 \u00f2 \u00d4 \u00f4 \u00d6 \u00f6 \u01d1 \u01d2 \u014e \u014f \u014c \u014d \u00d5 \u00f5 \u01ea \u01eb \u1ecc \u1ecd \u0150 \u0151 \u00d8 \u00f8 \u0152 \u0153 \u00a0 \u0186 \u0254 \u00a0 P p \u00a0 Q q \u00a0 R r \u0154 \u0155 \u0158 \u0159 \u0156 \u0157 \u1e5a \u1e5b \u1e5c \u1e5d \u00a0 S s \u015a \u015b \u015c \u015d \u0160 \u0161 \u015e \u015f \u0218 \u0219 \u1e62 \u1e63 \u00df \u00a0 T t \u0164 \u0165 \u0162 \u0163 \u021a \u021b \u1e6c \u1e6d \u00de \u00fe \u00a0 U u \u00da \u00fa \u00d9 \u00f9 \u00db \u00fb \u00dc \u00fc \u01d3 \u01d4 \u016c \u016d \u016a \u016b \u0168 \u0169 \u016e \u016f \u0172 \u0173 \u1ee4 \u1ee5 \u0170 \u0171 \u01d7 \u01d8 \u01db \u01dc \u01d9 \u01da \u01d5 \u01d6 \u00a0 V v \u00a0 W w \u0174 \u0175 \u00a0 X x \u00a0 Y y \u00dd \u00fd \u0176 \u0177 \u0178 \u00ff \u1ef8 \u1ef9 \u0232 \u0233 \u00a0 Z z \u0179 \u017a \u017b \u017c \u017d \u017e \u00a0 \u00df \u00d0 \u00f0 \u00de \u00fe \u014a \u014b \u018f \u0259 Greek: \u0386 \u03ac \u0388 \u03ad \u0389 \u03ae \u038a \u03af \u038c \u03cc \u038e \u03cd \u038f \u03ce \u00a0 \u0391 \u03b1 \u0392 \u03b2 \u0393 \u03b3 \u0394 \u03b4 \u00a0 \u0395 \u03b5 \u0396 \u03b6 \u0397 \u03b7 \u0398 \u03b8 \u00a0 \u0399 \u03b9 \u039a \u03ba \u039b \u03bb \u039c \u03bc \u00a0 \u039d \u03bd \u039e \u03be \u039f \u03bf \u03a0 \u03c0 \u00a0 \u03a1 \u03c1 \u03a3 \u03c3 \u03c2 \u03a4 \u03c4 \u03a5 \u03c5 \u00a0 \u03a6 \u03c6 \u03a7 \u03c7 \u03a8 \u03c8 \u03a9 \u03c9 \u00a0 {{Polytonic|}} Cyrillic: \u0410 \u0430 \u0411 \u0431 \u0412 \u0432 \u0413 \u0433 \u00a0 \u0490 \u0491 \u0403 \u0453 \u0414 \u0434 \u0402 \u0452 \u00a0 \u0415 \u0435 \u0401 \u0451 \u0404 \u0454 \u0416 \u0436 \u00a0 \u0417 \u0437 \u0405 \u0455 \u0418 \u0438 \u0406 \u0456 \u00a0 \u0407 \u0457 \u0419 \u0439 \u0408 \u0458 \u041a \u043a \u00a0 \u040c \u045c \u041b \u043b \u0409 \u0459 \u041c \u043c \u00a0 \u041d \u043d \u040a \u045a \u041e \u043e \u041f \u043f \u00a0 \u0420 \u0440 \u0421 \u0441 \u0422 \u0442 \u040b \u045b \u00a0 \u0423 \u0443 \u040e \u045e \u0424 \u0444 \u0425 \u0445 \u00a0 \u0426 \u0446 \u0427 \u0447 \u040f \u045f \u0428 \u0448 \u00a0 \u0429 \u0449 \u042a \u044a \u042b \u044b \u042c \u044c \u00a0 \u042d \u044d \u042e \u044e \u042f \u044f \u00a0 \u0301 IPA: t\u032a d\u032a \u0288 \u0256 \u025f \u0261 \u0262 \u02a1 \u0294 \u00a0 \u0278 \u03b2 \u03b8 \u00f0 \u0283 \u0292 \u0255 \u0291 \u0282 \u0290 \u00e7 \u029d \u0263 \u03c7 \u0281 \u0127 \u0295 \u029c \u02a2 \u0266 \u00a0 \u0271 \u0273 \u0272 \u014b \u0274 \u00a0 \u028b \u0279 \u027b \u0270 \u00a0 \u0299 \u2c71 \u0280 \u027e \u027d \u00a0 \u026b \u026c \u026e \u027a \u026d \u028e \u029f \u00a0 \u0265 \u028d \u0267 \u00a0 \u02bc \u00a0 \u0253 \u0257 \u0284 \u0260 \u029b \u00a0 \u0298 \u01c0 \u01c3 \u01c2 \u01c1 \u00a0 \u0268 \u0289 \u026f \u00a0 \u026a \u028f \u028a \u00a0 \u00f8 \u0258 \u0275 \u0264 \u00a0 \u0259 \u025a \u00a0 \u025b \u0153 \u025c \u025d \u025e \u028c \u0254 \u00a0 \u00e6 \u00a0 \u0250 \u0276 \u0251 \u0252 \u00a0 \u02b0 \u02b1 \u02b7 \u02b2 \u02e0 \u02e4 \u207f \u02e1 \u00a0 \u02c8 \u02cc \u02d0 \u02d1 \u032a \u00a0 {{IPA|}} Wikidata entities used in this page web crawler : Title, Sitelink, Description: en, Some statements, Miscellaneous (e.g. aliases, entity existence) Pages transcluded onto the current version of this page ( help ) : Spiderbot ( edit ) Template:About ( view source ) (template editor protected) Template:Authority control ( view source ) (template editor protected) Template:Blockquote ( view source ) (template editor protected) Template:Blockquote/styles.css ( view source ) (template editor protected) Template:Category handler ( view source ) (protected) Template:Citation needed ( view source ) (protected) Template:Cite book ( view source ) (protected) Template:Cite conference ( view source ) (protected) Template:Cite journal ( view source ) (protected) Template:Cite news ( view source ) (protected) Template:Cite thesis ( view source ) (template editor protected) Template:Cite web ( view source ) (protected) Template:Cn ( view source ) (template editor protected) Template:DMCA ( view source ) (template editor protected) Template:Dated maintenance category ( view source ) (template editor protected) Template:Dated maintenance category (articles) ( view source ) (template editor protected) Template:Delink ( view source ) (protected) Template:FULLROOTPAGENAME ( view source ) (template editor protected) Template:Fix ( view source ) (protected) Template:Fix/category ( view source ) (protected) Template:Further ( view source ) (template editor protected) Template:Hatnote group ( view source ) (semi-protected) Template:Hlist/styles.css ( view source ) (protected) Template:Internet search ( edit ) Template:Main ( view source ) (template editor protected) Template:Main other ( view source ) (protected) Template:Navbox ( view source ) (template editor protected) Template:Ns has subpages ( view source ) (protected) Template:Pagetype ( view source ) (protected) Template:Quote ( view source ) (template editor protected) Template:R ( view source ) (template editor protected) Template:R/ref ( view source ) (template editor protected) Template:R/superscript ( view source ) (template editor protected) Template:R/where ( view source ) (template editor protected) Template:Redirect ( view source ) (template editor protected) Template:Redirect-distinguish ( view source ) (extended confirmed protected) Template:Reflist ( view source ) (protected) Template:Reflist/styles.css ( view source ) (protected) Template:SDcat ( view source ) (protected) Template:Short description ( view source ) (protected) Template:Short description/lowercasecheck ( view source ) (protected) Template:Use dmy dates ( view source ) (template editor protected) Template:Web crawlers ( edit ) Template:Webarchive ( view source ) (template editor protected) Module:About ( view source ) (template editor protected) Module:Arguments ( view source ) (protected) Module:Authority control ( view source ) (template editor protected) Module:Authority control/config ( view source ) (template editor protected) Module:Category handler ( view source ) (protected) Module:Category handler/blacklist ( view source ) (protected) Module:Category handler/config ( view source ) (protected) Module:Category handler/data ( view source ) (protected) Module:Category handler/shared ( view source ) (protected) Module:Check for unknown parameters ( view source ) (protected) Module:Citation/CS1 ( view source ) (protected) Module:Citation/CS1/COinS ( view source ) (protected) Module:Citation/CS1/Configuration ( view source ) (protected) Module:Citation/CS1/Date validation ( view source ) (protected) Module:Citation/CS1/Identifiers ( view source ) (protected) Module:Citation/CS1/Utilities ( view source ) (protected) Module:Citation/CS1/Whitelist ( view source ) (protected) Module:Citation/CS1/styles.css ( view source ) (protected) Module:Delink ( view source ) (protected) Module:Disambiguation/templates ( view source ) (protected) Module:EditAtWikidata ( view source ) (protected) Module:Format link ( view source ) (template editor protected) Module:GetParameters ( view source ) (protected) Module:Hatnote ( view source ) (template editor protected) Module:Hatnote/styles.css ( view source ) (template editor protected) Module:Hatnote group ( view source ) (semi-protected) Module:Hatnote list ( view source ) (template editor protected) Module:Labelled list hatnote ( view source ) (template editor protected) Module:Namespace detect/config ( view source ) (protected) Module:Namespace detect/data ( view source ) (protected) Module:Navbar ( view source ) (protected) Module:Navbar/configuration ( view source ) (protected) Module:Navbar/styles.css ( view source ) (protected) Module:Navbox ( view source ) (template editor protected) Module:Navbox/configuration ( view source ) (template editor protected) Module:Navbox/styles.css ( view source ) (template editor protected) Module:Ns has subpages ( view source ) (protected) Module:Pagetype ( view source ) (protected) Module:Pagetype/config ( view source ) (protected) Module:Pagetype/disambiguation ( view source ) (protected) Module:Pagetype/rfd ( view source ) (protected) Module:Pagetype/setindex ( view source ) (protected) Module:Pagetype/softredirect ( view source ) (protected) Module:Redirect-distinguish ( view source ) (extended confirmed protected) Module:Redirect hatnote ( view source ) (template editor protected) Module:SDcat ( view source ) (protected) Module:String ( view source ) (protected) Module:String2 ( view source ) (template editor protected) Module:TableTools ( view source ) (protected) Module:Unsubst ( view source ) (protected) Module:Webarchive ( view source ) (template editor protected) Module:Webarchive/data ( view source ) (template editor protected) Module:Wikitext Parsing ( view source ) (protected) Module:Yesno ( view source ) (protected) This page is a member of 9 hidden categories ( help ) : Category:All articles with unsourced statements Category:Articles with short description Category:Articles with unsourced statements from February 2023 Category:Articles with unsourced statements from June 2021 Category:CS1 errors: missing periodical Category:CS1 maint: multiple names: authors list Category:Short description is different from Wikidata Category:Use dmy dates from September 2020 Category:Webarchive template wayback links Retrieved from \" https://en.wikipedia.org/wiki/Web_crawler \" Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Editing Web crawler Add languages Add topic"}, "47": {"url": "https://en.wikipedia.org/w/index.php?title=Web_crawler&action=history", "text": "Web crawler: Revision history - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Languages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Web crawler: Revision history Help Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Atom Upload file Page information Get shortened URL Download QR code In other projects Wikidata item Appearance move to sidebar hide View logs for this page ( view filter log ) Filter revisions show hide To date: Tag filter: 2017 wikitext editor ABBA [1.0] Addition of protection template to non-protected page Advanced mobile edit Android app edit AntiVandal App AI assist App description add App description change App description translate App full source App image add infobox App image add top App rollback App section source App select source App suggested edit App talk reply App talk source App talk topic App undo Automatic insertion of extraneous formatting AWB Barnsworth [1.0] Blanking blanking Bot in trial bup 2 [1.0] campaign-external-machine-translation canned edit summary categories removed CD Change of the mentor list changing height and/or weight changing time or duration Citation bot [1.1.0] Citation Bot [1.2.0] citing a blog or free web host COI template removed Community Configuration condition limit reached content model change content sourced to vanity press contentious topics alert ContentTranslation contenttranslation-needcheck ContentTranslation2 ContentTranslation: High unmodified machine translation usage Contest or editathon copyright violation template removed Correct typos in one click [1.0] Correct typos in one click [1.1] CropTool [1.2] CropTool [1.4] CropTool [1.5] CropTool testing [1.0] DAB Mechanic [1.0] dashboard-testing.wikiedu.org [1.0] dashboard-testing.wikiedu.org [2.0] dashboard.wikiedu.org [1.0] dashboard.wikiedu.org [1.2] dashboard.wikiedu.org [2.0] dashboard.wikiedu.org [2.2] dashboard.wikiedu.org [2.3] dashboard.wikiedu.org account-creation dev [1.0] dashboard.wikiedu.org dev [0.4.11] dashboard.wikiedu.org dev [0.4.2] dashboard.wikiedu.org dev [0.4.3] dashboard.wikiedu.org dev [0.4.4] dashboard.wikiedu.org dev [0.4.5] dashboard.wikiedu.org dev [0.4.7] dashboard.wikiedu.org dev [0.4.8] dashboard.wikiedu.org dev [0.4] dashboard.wikiedu.org dev [2.0] dashboard.wikiedu.org dev [2.1] demo-oauthratelimiter-3 [1.0] Deputy DiBabel [1.2] Disambiguation links added disambiguation template removed discussiontools (hidden tag) discussiontools-added-comment (hidden tag) discussiontools-source (hidden tag) discussiontools-source-enhanced (hidden tag) discussiontools-visual (hidden tag) Dispenser [1.0] Dispenser [2.4] Downstream Pull [0.1] draft or userpage link Edit Check (references) activated Edit Check (references) declined (irrelevant) Edit Check (references) declined (other) Edit Check (references) declined (uncertain) editcheck-newcontent (hidden tag) editcheck-newreference (hidden tag) editcheck-references (hidden tag) editProtectedHelper Education Dashboard development - awight [1.0] English Wikipedia Account Creation Assistance Tool [2.0] excessive whitespace External link added to disambiguation page extraneous markup featured article or good article template added or removed Find link [1.0] Fixed lint errors Fountain [0.1.3] Fountain Test [1.1] gabinaluz+app@gmail.com [1.0] harv or sfn error Huggle IABot [1.0] IABotManagementConsole (Personal Debug Consumer) [1.0] IABotManagementConsole [1.1] IABotManagementConsole [1.2] IABotManagementConsole [1.3] image template modification Incorrectly formatted external link or image Invalid TimedText edit iOS app edit Jonathan_at_WINTR_7 [1.0] JWB KenBurnsEffect tool [1.1] large non-free file large plot addition large unwikified new article LCA Tools [1.0] LCA Tools [1.5] Localhost [1.0] Manual revert Mass pronoun change MassMessage delivery massmove Medha_Bansal_intern_at_WikiEduDashboard [1.0] Mentorship module question Mentorship panel question missing file added missingredirectsproject [1.0] Mobile app edit Mobile edit Mobile web edit Modified by FileImporter moveToDraft MTC-Web for local dev/testing [1.0] MTCWeb-Dev [1.0] New redirect New topic New user adding protection template new user modifying archives new user moving page out of userspace Newcomer task Newcomer task: copyedit Newcomer task: expand Newcomer task: links Newcomer task: references Newcomer task: update Non-autoconfirmed user rapidly reverting edits non-English content nowiki added Nuke OAbot [1.0] OAbot [2.1] OAuth Uploader [1.0] outreachdashboard.wmflabs.org [2.0] outreachdashboard.wmflabs.org [2.1] pageswap GUI PageTriage Parliament edits PAWS [1.2] PAWS [2.1] paws [2.2] possible autobiography or conflict of interest possible birth or death date change possible BLP issue or vandalism possible conflict of interest possible cut and paste move or recreation possible MOS:ETHNICITY violation Possible self promotion in user or draftspace Possible self promotion in userspace possible unreferenced addition to BLP possible userspace spam possible vandalism possible WP:BLPCRIME issue possibly inaccurate edit summary ProveIt edit QuickCategories [1.0] QuickCategories [1.1] rapid date format changes Recreated Redirect target changed reference list removal references removed removal of Category:Living People Removed redirect repeated addition of external links by non-autoconfirmed user Replaced Reply Reverted reverting anti-vandal bot review edit Rollback RW section blanking SectionTranslation self-renaming or bad user talk move Shortdesc helper shouting Snuggle (English Wikipedia) [1.0.0] speedy deletion template removed Suggested: add links Suggestor [0.0.1] SWViewer [1.0] SWViewer [1.2] SWViewer [1.3] SWViewer [1.4] SWViewer [1.6] Takedown Tools [1.0] Talk banner shell conversion talk page blanking Text added at end of page TorProxy [0.1] TorProxy [0.2] Twinkle U.S. Congress edit Ultraviolet Undo Unexpected #REDIRECT markup unsourced AFC submission use of deprecated (unreliable) source use of predatory open access journal User Analysis Tool [1.0] very short new article Visual edit Visual edit: Check Visual edit: Switched Weekipedia v3 [1.0] Weekipedia2 [1.0] wiki-file-transfer [1.0] wikieditor (hidden tag) wikiedu Assignment Wizard 2.0 Teting [1.0] wikiedu Assignment Wizard [1.0] wikiedu.org Assignment Design Wizard (testing) [0.99] wikiedu.org Assignment Design Wizard [1.0.1] wikiedu.org Assignment Design Wizard [1.0.2] WikiEduDashboard NTDB [1.2] WikiEduDashboard NTDB3 [1.0] WikiEduDashboard NTDB4 [1.0] WikiEduDashboard NTDB5 [1.0] WikiEduWizard NTDB [1.2] Wikifile Transfer [1.0] Wikifile Transfer [2.0] Wikifile Transfer [4.0] Wikifile Transfer [5.0] WikiLeaks wikilinks removed WikiLoop Battlefield WikiLoop Battlefield Dev Local [2.0.0] WikiLoop Battlefield on WMF Cloud VPS [1.0] WikiLoop Battlefield Prod [2.2.1-beta] WikiLoop DoubleCheck WMF Cloud VPS (2020-07-13 version) [4.1.0] WikiLove WINTR Wikiedu Dashboard Local Test 3 [1.0] WINTR Wikiedu Wizard Local Test [1.0] WPCleaner yabbr [1.3] Invert selection Show revisions External tools: Find addition/removal ( Alternate ) Find edits by user ( Alternate ) Page statistics Pageviews Fix dead links For any version listed below, click on its date to view it. For more help, see Help:Page history and Help:Edit summary . (cur) = difference from current version, (prev) = difference from preceding version, m = minor edit , \u2192 = section edit , \u2190 = automatic edit summary ( newest | oldest ) View ( newer 50 | older 50 ) ( 20 | 50 | 100 | 250 | 500 ) 27 April 2025 cur prev 18:46 18:46, 27 April 2025 Squeakachu talk contribs m 54,630 bytes +42 Rollback edit(s) by 2A02:A470:69A0:0:AC48:18DB:2DC9:CD01 ( talk ): non-constructive (RW 16.1) undo Tags : RW Rollback cur prev 18:37 18:37, 27 April 2025 2a02:a470:69a0:0:ac48:18db:2dc9:cd01 talk 54,588 bytes +14 No edit summary undo Tag : Reverted cur prev 18:36 18:36, 27 April 2025 2a02:a470:69a0:0:ac48:18db:2dc9:cd01 talk 54,574 bytes \u221256 No edit summary undo Tag : Reverted 17 April 2025 cur prev 02:57 02:57, 17 April 2025 45.251.104.36 talk 54,630 bytes 0 No edit summary undo 11 April 2025 cur prev 16:17 16:17, 11 April 2025 129.7.0.226 talk 54,630 bytes \u22122 \u2192 Nomenclature undo Tag : Visual edit 19 March 2025 cur prev 14:06 14:06, 19 March 2025 JeffSpaceman talk contribs m 54,632 bytes \u22127 Reverted edit by 77.121.11.10 ( talk ) to last version by WhiteTailedEagle undo Tag : Rollback cur prev 14:06 14:06, 19 March 2025 77.121.11.10 talk 54,639 bytes +7 No edit summary undo Tag : Reverted 12 March 2025 cur prev 19:56 19:56, 12 March 2025 WhiteTailedEagle talk contribs m 54,632 bytes 0 Cleaned up using AutoEd undo 4 March 2025 cur prev 21:59 21:59, 4 March 2025 Viewmont Viking talk contribs 54,632 bytes \u221236 \u2192 List of web crawlers : External link in body of article undo cur prev 21:57 21:57, 4 March 2025 Uis246 talk contribs 54,668 bytes \u221255 Xapian is not a crawler undo 17 February 2025 cur prev 10:29 10:29, 17 February 2025 Fragga talk contribs 54,723 bytes +115 \u2192 Commercial web crawlers undo 20 December 2024 cur prev 05:47 05:47, 20 December 2024 Augmented Seventh talk contribs 54,608 bytes \u221256 Reverted 1 edit by 2806:102E:22:2CD:C88E:9912:B1CF:E7C3 ( talk ): Promotional undo Tags : Twinkle Undo Mobile edit Mobile web edit Advanced mobile edit cur prev 05:44 05:44, 20 December 2024 2806:102e:22:2cd:c88e:9912:b1cf:e7c3 talk 54,664 bytes +56 https://www.facebook.com/profile.php?id=100078106500760 undo Tags : Reverted Mobile edit Mobile web edit 17 November 2024 cur prev 12:37 12:37, 17 November 2024 2601:206:8480:17f0:81f0:78ce:c49f:28bb talk 54,608 bytes +219 \u2192 Historical web crawlers undo 6 October 2024 cur prev 22:55 22:55, 6 October 2024 AntiDionysius talk contribs m 54,389 bytes \u2212129 Reverted edit by J\u00e1n Rege\u0161 ( talk ) to last version by Squeakachu undo Tag : Rollback cur prev 21:47 21:47, 6 October 2024 J\u00e1n Rege\u0161 talk contribs 54,518 bytes +129 Added SiteOne Crawler undo Tags : Reverted Visual edit 21 August 2024 cur prev 05:08 05:08, 21 August 2024 Squeakachu talk contribs m 54,389 bytes \u221245 Rollback edit(s) by Bomboclatmonkey69 ( talk ): Vandalism (RW 16.1) undo Tags : RW Rollback cur prev 05:06 05:06, 21 August 2024 Bomboclatmonkey69 talk contribs 54,434 bytes +45 No edit summary undo Tags : Reverted possible vandalism cur prev 05:04 05:04, 21 August 2024 107.146.237.135 talk 54,389 bytes \u221212 istoptrolls undo Tag : Manual revert cur prev 05:03 05:03, 21 August 2024 Bomboclatmonkey69 talk contribs m 54,401 bytes +12 No edit summary undo Tag : Reverted cur prev 04:58 04:58, 21 August 2024 107.146.237.135 talk 54,389 bytes \u221219 sorry for trolling sirs undo Tag : Manual revert cur prev 04:56 04:56, 21 August 2024 107.146.237.135 talk 54,408 bytes +19 No edit summary undo Tag : Reverted 10 August 2024 cur prev 20:37 20:37, 10 August 2024 2a02:587:3235:7000:f564:c010:a605:e232 talk 54,389 bytes 0 \u2192 Open-source crawlers undo 15 July 2024 cur prev 21:34 21:34, 15 July 2024 72.174.134.220 talk 54,389 bytes \u22121 \u2192 Crawling policy : No need to capitalize there. In fact, you may want to emphasize what it means to 'overload' a site/page. undo 5 July 2024 cur prev 11:48 11:48, 5 July 2024 CanonNi talk contribs m 54,390 bytes +371 Reverted edits by 2600:1006:B172:D3E3:3BFE:E05E:3C90:244B ( talk ) to last version by Me Da Wikipedian: unexplained content removal undo Tags : Rollback SWViewer [1.6] cur prev 11:48 11:48, 5 July 2024 2600:1006:b172:d3e3:3bfe:e05e:3c90:244b talk 54,019 bytes \u2212371 No edit summary undo Tags : Reverted Mobile edit Mobile web edit 2 July 2024 cur prev 14:07 14:07, 2 July 2024 Me Da Wikipedian talk contribs m 54,390 bytes \u221248 Undid edits by 90.167.50.105 ( talk ) to last version by Alokhnathps: not providing a reliable source ( WP:CITE , WP:RS ) undo Tags : Undo SWViewer [1.6] cur prev 14:06 14:06, 2 July 2024 90.167.50.105 talk 54,438 bytes +48 \u2192 In-house web crawlers undo Tags : Reverted Mobile edit Mobile web edit 1 June 2024 cur prev 15:50 15:50, 1 June 2024 Alokhnathps talk contribs m 54,390 bytes +38 Links added undo Tag : Visual edit 23 May 2024 cur prev 12:06 12:06, 23 May 2024 MrOllie talk contribs m 54,352 bytes +110 Reverted 1 edit by 2600:1006:B15B:BC02:C4BE:22C2:48CD:CDEB ( talk ) to last revision by Liz undo Tags : Twinkle Undo cur prev 07:41 07:41, 23 May 2024 2600:1006:b15b:bc02:c4be:22c2:48cd:cdeb talk 54,242 bytes \u2212110 Fixed typo undo Tags : Reverted possibly inaccurate edit summary Mobile edit Mobile web edit 21 May 2024 cur prev 21:15 21:15, 21 May 2024 Liz talk contribs 54,352 bytes \u22124 Unlinking circular redirects: Wikipedia:Articles for deletion/Norconex Web Crawler closed as redirect ( XFDcloser ) undo 10 May 2024 cur prev 07:52 07:52, 10 May 2024 Pahunkat talk contribs m 54,356 bytes \u221250 Rollback edit(s) by Spiderindexer ( talk ): Addition of unnecessary/inappropriate external links (RW 16.1) undo Tags : RW Rollback cur prev 07:49 07:49, 10 May 2024 Spiderindexer talk contribs 54,406 bytes +50 No edit summary undo Tags : Reverted possible conflict of interest 6 May 2024 cur prev 08:21 08:21, 6 May 2024 MathXplore talk contribs m 54,356 bytes +768 Reverted edit by 174.212.227.151 ( talk ) to last version by HeyElliott undo Tag : Rollback cur prev 08:21 08:21, 6 May 2024 174.212.227.151 talk 53,588 bytes \u2212768 No edit summary undo Tags : Reverted Visual edit Mobile edit Mobile web edit 5 April 2024 cur prev 19:15 19:15, 5 April 2024 HeyElliott talk contribs m 54,356 bytes +1 WP:N'T undo Tag : 2017 wikitext editor 14 March 2024 cur prev 03:37 03:37, 14 March 2024 Johnnie Bob talk contribs 54,355 bytes \u221239 Reverted 1 edit by Webitfy ( talk ): wp:linkspam undo Tags : Twinkle Undo cur prev 03:31 03:31, 14 March 2024 Webitfy talk contribs m 54,394 bytes +39 Add example link undo Tags : Reverted possible conflict of interest Visual edit 9 March 2024 cur prev 21:13 21:13, 9 March 2024 InfiniteNexus talk contribs 54,355 bytes \u22123 No edit summary undo Tag : Manual revert 8 March 2024 cur prev 09:43 09:43, 8 March 2024 Onel5969 talk contribs m 54,358 bytes +3 Disambiguating links to Google.com (link changed to Google Search ) using DisamAssist . undo Tags : Manual revert Reverted 5 March 2024 cur prev 22:54 22:54, 5 March 2024 InfiniteNexus talk contribs 54,355 bytes \u22123 No edit summary undo Tags : Manual revert Reverted 2 March 2024 cur prev 20:44 20:44, 2 March 2024 Onel5969 talk contribs m 54,358 bytes +3 Disambiguating links to Google.com (link changed to Google Search ) using DisamAssist . undo Tag : Reverted 11 February 2024 cur prev 19:09 19:09, 11 February 2024 Count Count talk contribs 54,355 bytes \u221230 Reverted 1 edit by 114.10.98.110 ( talk ): Rv spam undo Tags : Twinkle Undo cur prev 19:08 19:08, 11 February 2024 114.10.98.110 talk 54,385 bytes +30 Cs dana berlari undo Tags : Reverted Visual edit Mobile edit Mobile web edit 27 January 2024 cur prev 09:23 09:23, 27 January 2024 USSR-Slav talk contribs m 54,355 bytes +1,617 Undid edits by 128.61.100.107 ( talk ) to last revision by Alexeyevitch undo Tags : Undo SWViewer [1.6] cur prev 09:23 09:23, 27 January 2024 128.61.100.107 talk 52,738 bytes \u22121,617 No edit summary undo Tags : Reverted possible vandalism references removed shouting cur prev 09:21 09:21, 27 January 2024 Alexeyevitch talk contribs 54,355 bytes \u221214 rv test undo Tag : Undo cur prev 09:18 09:18, 27 January 2024 128.61.100.107 talk 54,369 bytes +14 No edit summary undo Tag : Reverted cur prev 09:16 09:16, 27 January 2024 ClueBot NG talk contribs m 54,355 bytes \u221220 Reverting possible vandalism by 128.61.100.107 to version by Broc. Report False Positive? Thanks, ClueBot NG . (4296893) (Bot) undo Tag : Rollback ( newest | oldest ) View ( newer 50 | older 50 ) ( 20 | 50 | 100 | 250 | 500 ) Retrieved from \" https://en.wikipedia.org/wiki/Web_crawler \" Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Web crawler: Revision history Add topic"}, "48": {"url": "https://en.wikipedia.org/wiki/Special:WhatLinksHere/Web_crawler", "text": "Pages that link to \"Web crawler\" - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Languages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Pages that link to \"Web crawler\" Help Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General Upload file Printable version Get shortened URL Download QR code In other projects Appearance move to sidebar hide \u2190 Web crawler What links here \u29fcwhatlinkshere-whatlinkshere-target\u29fd Page: \u29fcwhatlinkshere-whatlinkshere-ns\u29fd Namespace: all (Article) Talk User User talk Wikipedia Wikipedia talk File File talk MediaWiki MediaWiki talk Template Template talk Help Help talk Category Category talk Portal Portal talk Draft Draft talk MOS MOS talk TimedText TimedText talk Module Module talk Event Event talk Invert selection \u29fcwhatlinkshere-whatlinkshere-filter\u29fd Hide transclusions Hide links Hide redirects Go The following pages link to Web crawler External tools ( link count transclusion count sorted list ) \u00b7 See help page for transcluding these entries Showing 50 items. View ( previous 50 | next 50 ) ( 20 | 50 | 100 | 250 | 500 ) HTML ( links | edit ) HTTP ( links | edit ) HTTPS ( links | edit ) Knowbot ( links | edit ) Meta element ( links | edit ) Search engine (computing) ( links | edit ) Semantic Web ( links | edit ) World Wide Web ( links | edit ) Web server ( links | edit ) Web indexing ( links | edit ) 1990s ( links | edit ) Wide area information server ( links | edit ) Spamdexing ( links | edit ) Hyperlink ( links | edit ) WebCrawler ( links | edit ) Larry Page ( links | edit ) Sergey Brin ( links | edit ) Robots.txt ( links | edit ) Web spider (redirect page) ( links | edit ) Netscape ( links | edit ) Web crawler ( links | edit ) Googlebot ( links | edit ) Server hog ( links | edit ) Apache Lucene ( links | edit ) Media monitoring service ( links | edit ) Email-address harvesting ( links | edit ) Spider trap ( links | edit ) Talk:PageRank ( links | edit ) Talk:Hyperlink/Archive 1 ( links | edit ) User:Uncle G/Wikipedia triage ( links | edit ) User:Tubezone/Dead Parrot Analogy ( links | edit ) User:Samuel2252/sandbox ( links | edit ) User talk:PocklingtonDan/Archive/1 ( links | edit ) Wikipedia:Help desk/Archive 11 ( links | edit ) Wikipedia:Reference desk/Archives/Miscellaneous/May 2006 ( links | edit ) Wikipedia:Reference desk/Archives/Computing/2007 October 5 ( links | edit ) Wikipedia:Reference desk/Archives/Computing/2008 January 4 ( links | edit ) Wikipedia:Articles for deletion/Log/2008 October 22 ( links | edit ) Wikipedia:Articles for deletion/Spider Loop ( links | edit ) Wikipedia:Reference desk/Archives/Computing/2009 July 3 ( links | edit ) Wikipedia:Reference desk/Archives/Computing/2010 July 16 ( links | edit ) Wikipedia:Redirects for discussion/Log/2016 May 31 ( links | edit ) Wikipedia talk:Disambiguation/Archive 44 ( links | edit ) Web directory ( links | edit ) Internet Archive ( links | edit ) Search engine optimization ( links | edit ) Search engine spider (redirect page) ( links | edit ) Cloaking ( links | edit ) Internet bot ( links | edit ) Schema.org ( links | edit ) Web shell ( links | edit ) Talk:Male bra ( links | edit ) User talk:ZimZalaBim/Archive 4 ( links | edit ) User talk:Magioladitis/Archive 5 ( links | edit ) Wikipedia:Village pump (technical)/Archive 155 ( links | edit ) Wikipedia:Redirects for discussion/Log/2020 June 8 ( links | edit ) Wikipedia talk:WikiProject Medicine/Archive 119 ( links | edit ) Search engine spiders (redirect page) ( links | edit ) User talk:Devin004 ( links | edit ) OPIC (disambiguation) ( links | edit ) Deep linking ( links | edit ) Trespass ( links | edit ) Unsupervised learning ( links | edit ) Distributed web crawling ( links | edit ) Extract, transform, load ( links | edit ) Alexa Internet ( links | edit ) HTML element ( links | edit ) Spider (disambiguation) ( links | edit ) Googlebot ( links | edit ) Archive ( links | edit ) Image retrieval ( links | edit ) Crawler ( links | edit ) AltaVista ( links | edit ) Wget ( links | edit ) 1996 in science ( links | edit ) Apache Nutch ( links | edit ) Google Shopping ( links | edit ) Internet research ( links | edit ) Deep web ( links | edit ) User-Agent header ( links | edit ) Search/Retrieve Web Service ( links | edit ) Crawl ( links | edit ) CURL ( links | edit ) Metasearch engine ( links | edit ) Doorway page ( links | edit ) View ( previous 50 | next 50 ) ( 20 | 50 | 100 | 250 | 500 ) Retrieved from \" https://en.wikipedia.org/wiki/Special:WhatLinksHere/Web_crawler \" Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Pages that link to \"Web crawler\" Add topic"}, "49": {"url": "https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Web_crawler", "text": "Related changes - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main page Contents Current events Random article About Wikipedia Contact us Contribute Help Learn to edit Community portal Recent changes Upload file Special pages Languages Search Search Appearance Donate Create account Log in Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk Related changes Help Article Talk English Read Edit View history Tools Tools move to sidebar hide Actions Read Edit View history General Atom Upload file Printable version Get shortened URL Download QR code In other projects Appearance move to sidebar hide \u2190 Web crawler Enter a page name to see changes on pages linked to or from that page. (To see members of a category, enter Category:Name of category). Changes to pages on your Watchlist are shown in bold with a green bullet . See more at Help:Related changes . Recent changes options Show last 50 | 100 | 250 | 500 changes in last 1 | 3 | 7 | 14 | 30 days Hide registered users | Hide unregistered users | Hide my edits | Show bots | Hide minor edits | Show page categorization | Show Wikidata | Hide probably good edits Show new changes starting from 10:32, 17 May 2025 Namespace: all (Article) Talk User User talk Wikipedia Wikipedia talk File File talk MediaWiki MediaWiki talk Template Template talk Help Help talk Category Category talk Portal Portal talk Draft Draft talk MOS MOS talk TimedText TimedText talk Module Module talk Event Event talk Invert selection Associated namespace Tag filter: 2017 wikitext editor ABBA [1.0] Addition of protection template to non-protected page Advanced mobile edit Android app edit AntiVandal App AI assist App description add App description change App description translate App full source App image add infobox App image add top App rollback App section source App select source App suggested edit App talk reply App talk source App talk topic App undo Automatic insertion of extraneous formatting AWB Barnsworth [1.0] Blanking blanking Bot in trial bup 2 [1.0] campaign-external-machine-translation canned edit summary categories removed CD Change of the mentor list changing height and/or weight changing time or duration Citation bot [1.1.0] Citation Bot [1.2.0] citing a blog or free web host COI template removed Community Configuration condition limit reached content model change content sourced to vanity press contentious topics alert ContentTranslation contenttranslation-needcheck ContentTranslation2 ContentTranslation: High unmodified machine translation usage Contest or editathon copyright violation template removed Correct typos in one click [1.0] Correct typos in one click [1.1] CropTool [1.2] CropTool [1.4] CropTool [1.5] CropTool testing [1.0] DAB Mechanic [1.0] dashboard-testing.wikiedu.org [1.0] dashboard-testing.wikiedu.org [2.0] dashboard.wikiedu.org [1.0] dashboard.wikiedu.org [1.2] dashboard.wikiedu.org [2.0] dashboard.wikiedu.org [2.2] dashboard.wikiedu.org [2.3] dashboard.wikiedu.org account-creation dev [1.0] dashboard.wikiedu.org dev [0.4.11] dashboard.wikiedu.org dev [0.4.2] dashboard.wikiedu.org dev [0.4.3] dashboard.wikiedu.org dev [0.4.4] dashboard.wikiedu.org dev [0.4.5] dashboard.wikiedu.org dev [0.4.7] dashboard.wikiedu.org dev [0.4.8] dashboard.wikiedu.org dev [0.4] dashboard.wikiedu.org dev [2.0] dashboard.wikiedu.org dev [2.1] demo-oauthratelimiter-3 [1.0] Deputy DiBabel [1.2] Disambiguation links added disambiguation template removed discussiontools (hidden tag) discussiontools-added-comment (hidden tag) discussiontools-source (hidden tag) discussiontools-source-enhanced (hidden tag) discussiontools-visual (hidden tag) Dispenser [1.0] Dispenser [2.4] Downstream Pull [0.1] draft or userpage link Edit Check (references) activated Edit Check (references) declined (irrelevant) Edit Check (references) declined (other) Edit Check (references) declined (uncertain) editcheck-newcontent (hidden tag) editcheck-newreference (hidden tag) editcheck-references (hidden tag) editProtectedHelper Education Dashboard development - awight [1.0] English Wikipedia Account Creation Assistance Tool [2.0] excessive whitespace External link added to disambiguation page extraneous markup featured article or good article template added or removed Find link [1.0] Fixed lint errors Fountain [0.1.3] Fountain Test [1.1] gabinaluz+app@gmail.com [1.0] harv or sfn error Huggle IABot [1.0] IABotManagementConsole (Personal Debug Consumer) [1.0] IABotManagementConsole [1.1] IABotManagementConsole [1.2] IABotManagementConsole [1.3] image template modification Incorrectly formatted external link or image Invalid TimedText edit iOS app edit Jonathan_at_WINTR_7 [1.0] JWB KenBurnsEffect tool [1.1] large non-free file large plot addition large unwikified new article LCA Tools [1.0] LCA Tools [1.5] Localhost [1.0] Manual revert Mass pronoun change MassMessage delivery massmove Medha_Bansal_intern_at_WikiEduDashboard [1.0] Mentorship module question Mentorship panel question missing file added missingredirectsproject [1.0] Mobile app edit Mobile edit Mobile web edit Modified by FileImporter moveToDraft MTC-Web for local dev/testing [1.0] MTCWeb-Dev [1.0] New redirect New topic New user adding protection template new user modifying archives new user moving page out of userspace Newcomer task Newcomer task: copyedit Newcomer task: expand Newcomer task: links Newcomer task: references Newcomer task: update Non-autoconfirmed user rapidly reverting edits non-English content nowiki added Nuke OAbot [1.0] OAbot [2.1] OAuth Uploader [1.0] outreachdashboard.wmflabs.org [2.0] outreachdashboard.wmflabs.org [2.1] pageswap GUI PageTriage Parliament edits PAWS [1.2] PAWS [2.1] paws [2.2] possible autobiography or conflict of interest possible birth or death date change possible BLP issue or vandalism possible conflict of interest possible cut and paste move or recreation possible MOS:ETHNICITY violation Possible self promotion in user or draftspace Possible self promotion in userspace possible unreferenced addition to BLP possible userspace spam possible vandalism possible WP:BLPCRIME issue possibly inaccurate edit summary ProveIt edit QuickCategories [1.0] QuickCategories [1.1] rapid date format changes Recreated Redirect target changed reference list removal references removed removal of Category:Living People Removed redirect repeated addition of external links by non-autoconfirmed user Replaced Reply Reverted reverting anti-vandal bot review edit Rollback RW section blanking SectionTranslation self-renaming or bad user talk move Shortdesc helper shouting Snuggle (English Wikipedia) [1.0.0] speedy deletion template removed Suggested: add links Suggestor [0.0.1] SWViewer [1.0] SWViewer [1.2] SWViewer [1.3] SWViewer [1.4] SWViewer [1.6] Takedown Tools [1.0] Talk banner shell conversion talk page blanking Text added at end of page TorProxy [0.1] TorProxy [0.2] Twinkle U.S. Congress edit Ultraviolet Undo Unexpected #REDIRECT markup unsourced AFC submission use of deprecated (unreliable) source use of predatory open access journal User Analysis Tool [1.0] very short new article Visual edit Visual edit: Check Visual edit: Switched Weekipedia v3 [1.0] Weekipedia2 [1.0] wiki-file-transfer [1.0] wikieditor (hidden tag) wikiedu Assignment Wizard 2.0 Teting [1.0] wikiedu Assignment Wizard [1.0] wikiedu.org Assignment Design Wizard (testing) [0.99] wikiedu.org Assignment Design Wizard [1.0.1] wikiedu.org Assignment Design Wizard [1.0.2] WikiEduDashboard NTDB [1.2] WikiEduDashboard NTDB3 [1.0] WikiEduDashboard NTDB4 [1.0] WikiEduDashboard NTDB5 [1.0] WikiEduWizard NTDB [1.2] Wikifile Transfer [1.0] Wikifile Transfer [2.0] Wikifile Transfer [4.0] Wikifile Transfer [5.0] WikiLeaks wikilinks removed WikiLoop Battlefield WikiLoop Battlefield Dev Local [2.0.0] WikiLoop Battlefield on WMF Cloud VPS [1.0] WikiLoop Battlefield Prod [2.2.1-beta] WikiLoop DoubleCheck WMF Cloud VPS (2020-07-13 version) [4.1.0] WikiLove WINTR Wikiedu Dashboard Local Test 3 [1.0] WINTR Wikiedu Wizard Local Test [1.0] WPCleaner yabbr [1.3] Invert selection Page name: Show changes to pages linked to the given page instead List of abbreviations ( help ): D Edit made at Wiki d ata r Edit flagged by O R ES N N ew page m M inor edit b B ot edit (\u00b1123) Page byte size change Temporarily watched page 17 May 2025 diff hist Microsoft 06:30 +264 Noxoug1 talk contribs Tags : Visual edit Mobile edit Mobile web edit Advanced mobile edit diff hist Microsoft 06:29 +143 Noxoug1 talk contribs (Layoffs) Tags : Visual edit Mobile edit Mobile web edit Advanced mobile edit diff hist Sergey Brin 06:25 +2 NoWikiNoLife talk contribs diff hist Larry Page 06:23 0 NoWikiNoLife talk contribs Tag : Visual edit diff hist API 03:22 0 HueMan1 talk contribs (not a proper noun) Tags : Mobile edit Mobile web edit diff hist m World Wide Web 02:26 +1,203 Joyous! talk contribs (Reverted 1 edit by 2601:5CF:4681:E060:49E3:ABF:884C:2BBE ( talk ) to last revision by Nahida) Tags : Twinkle Undo diff hist World Wide Web 02:23 \u22121,203 2601:5cf:4681:e060:49e3:abf:884c:2bbe talk (content://media/external/downloads/1000005159) Tags : Reverted references removed Mobile edit Mobile web edit 16 May 2025 diff hist Microsoft Windows 16:32 \u221260 HeyElliott talk contribs (Added info to refs, redundant, ce) Tag : 2017 wikitext editor diff hist m World Wide Web 14:16 +15,389 Nahida talk contribs (Reverted edit by 83.11.253.58 ( talk ) to last version by Macaddct1984) Tag : Rollback diff hist World Wide Web 14:16 \u221215,389 83.11.253.58 talk ( \u2192 History ) Tags : Reverted section blanking blanking diff hist m C (programming language) 10:31 \u22127 Materialscientist talk contribs (Reverted edits by 2409:408A:383:B00:9D93:4323:C907:C33E ( talk ) ( HG ) (3.4.13)) Tags : Huggle Rollback diff hist C (programming language) 10:28 +7 2409:408a:383:b00:9d93:4323:c907:c33e talk (Developer full name instead name) Tags : Reverted Visual edit Mobile edit Mobile web edit diff hist m Voice search 06:28 +241 ChristyRobinson21 talk contribs ( \u2192 How it works ) Tag : Visual edit 15 May 2025 diff hist m World Wide Web 18:23 0 Macaddct1984 talk contribs (Reverted edits by 12.46.211.251 ( talk ) ( HG ) (3.4.13)) Tags : Huggle Rollback diff hist World Wide Web 18:21 0 12.46.211.251 talk Tags : Reverted Visual edit: Switched diff hist m Microsoft Word 17:33 +3,906 Bruce1ee talk contribs (Reverted edit by Amos magiri ( talk ) to last version by Ivebeenhacked) Tag : Rollback diff hist Microsoft Word 17:32 \u22123,906 Amos magiri talk contribs Tags : Reverted Visual edit diff hist m PDF 11:00 \u221230 Annh07 talk contribs (Reverted edit by Reshamyogi ( talk ) to last version by Toast1454) Tag : Rollback diff hist m PDF 10:58 +30 Reshamyogi talk contribs (Ok) Tags : Reverted Visual edit Mobile edit Mobile web edit diff hist Siri 10:56 +14 AK4393 talk contribs ( \u2192 Privacy controversy ) diff hist Siri 10:49 +497 AK4393 talk contribs ( \u2192 Privacy controversy ) diff hist Microsoft Word 01:36 +89 Ivebeenhacked talk contribs (Reverting edit(s) by 2409:4085:E11:CBCC:0:0:56C8:5A07 ( talk ) to rev. 1290035556 by Bruce1ee: Vandalism ( UV 0.1.6 )) Tags : Ultraviolet Undo diff hist Microsoft Word 01:35 \u221289 2409:4085:e11:cbcc::56c8:5a07 talk Tags : Reverted missing file added Visual edit Mobile edit Mobile web edit 14 May 2025 diff hist m Web page 20:13 \u221226 Meters talk contribs (Reverted edit by 117.55.251.164 ( talk ) to last version by Meters) Tag : Rollback diff hist Web page 20:10 +26 117.55.251.164 talk (DawnawHsihsi(dawnawhsihsi)) Tags : Reverted Mobile edit Mobile web edit diff hist m HTTP 16:25 +11 BriDash9000 talk contribs (Added {{pp-pc}} template so the white lock displays after semi-protection expires) diff hist Search engine optimization 11:43 \u221296 ScottishFinnishRadish talk contribs (Reverting edit(s) by Naidushiva ( talk ) to rev. 1288407611 by Alokhnathps: Non-constructive edit ( UV 0.1.6 )) Tags : Ultraviolet Undo diff hist m Search engine optimization 10:35 +18 Naidushiva talk contribs ( \u2192 Legal precedents ) Tag : Reverted diff hist m Search engine optimization 10:33 +25 Naidushiva talk contribs ( \u2192 International markets and SEO ) Tag : Reverted diff hist m Search engine optimization 10:21 +1 Naidushiva talk contribs ( \u2192 Relationship with Google ) Tag : Reverted diff hist m Search engine optimization 10:20 +28 Naidushiva talk contribs ( \u2192 Relationship with Google ) Tag : Reverted diff hist m Search engine optimization 10:18 \u22124 Naidushiva talk contribs ( \u2192 Relationship with Google ) Tag : Reverted diff hist m Search engine optimization 10:17 +28 Naidushiva talk contribs ( \u2192 Relationship with Google ) Tag : Reverted diff hist m World Wide Web 07:46 \u22121 Remsense talk contribs (Reverted 1 edit by 122.107.47.87 ( talk ) to last revision by 187.191.8.183) Tags : Twinkle Undo diff hist World Wide Web 07:44 +1 122.107.47.87 talk Tags : Reverted Mobile edit Mobile web edit diff hist m Microsoft 06:06 \u2212566 Remsense talk contribs (Reverted 1 edit by AK4393 ( talk ) to last revision by Faster than Thunder) Tags : Twinkle Undo diff hist Microsoft 04:59 +566 AK4393 talk contribs ( \u2192 Layoffs ) Tag : Reverted 13 May 2025 diff hist m API 11:32 \u2212175 Mindmatrix talk contribs (Reverted edit by 118.99.84.43 ( talk ) to last version by Isaidnoway) Tag : Rollback diff hist API 10:09 +175 118.99.84.43 talk Tags : Reverted use of deprecated (unreliable) source Visual edit Mobile edit Mobile web edit 12 May 2025 diff hist m Search engine 23:19 \u2212783 Kuru talk contribs (Reverted edit by Muneebahmed1 ( talk ) to last version by Kuru) Tag : Rollback diff hist m Search engine 22:48 +783 Muneebahmed1 talk contribs (Added a summary about the evolution of SEO with a citation link at the end, without changing the original content.) Tags : Reverted Visual edit diff hist Search engine 22:25 \u2212803 Kuru talk contribs (rmv promotional addition) Tag : Manual revert diff hist m Search engine 22:18 +803 Muneebahmed1 talk contribs (Added a summary about the evolution of SEO with a citation link at the end, without changing the original content.) Tags : Reverted Visual edit diff hist m Internet Archive 18:15 +19,757 The4lines talk contribs (Reverted edit by 37.39.231.70 ( talk ) to last version by Annh07) Tag : Rollback diff hist Internet Archive 18:14 \u221219,757 37.39.231.70 talk Tags : Reverted section blanking Visual edit Mobile edit Mobile web edit diff hist Internet Archive 18:06 +3,482 Annh07 talk contribs (Reverted 7 edits by 37.39.231.70 ( talk ): Unexplained content removal) Tags : Twinkle Undo diff hist Internet Archive 18:06 \u22121,035 37.39.231.70 talk Tags : Reverted references removed Visual edit Mobile edit Mobile web edit diff hist Internet Archive 18:05 \u2212546 37.39.231.70 talk Tags : Reverted Visual edit Mobile edit Mobile web edit diff hist Internet Archive 18:04 \u2212426 37.39.231.70 talk Tags : Reverted Visual edit Mobile edit Mobile web edit diff hist Internet Archive 18:03 \u221224 37.39.231.70 talk Tags : Reverted Visual edit Mobile edit Mobile web edit Retrieved from \" https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Web_crawler \" Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Search Search Related changes Add topic"}}